{"meta":{"title":"Jiacheng Pan's Wiki","subtitle":null,"description":null,"author":"Jiacheng Pan","url":"https://jiacheng-pan.github.io/wiki"},"pages":[{"title":"Jiacheng Pan","date":"2019-01-21T07:17:28.804Z","updated":"2019-01-21T07:17:28.804Z","comments":true,"path":"about/index.html","permalink":"https://jiacheng-pan.github.io/wiki/about/index.html","excerpt":"","text":"I'm a post graduate student in Zhejiang University, interested in visualization and visual analytics. Find my blog in Jackie Anxis. Say Hi to me through my github: JackieAnxis' GitHub or mail to me: jackieanxis@gmail.com"},{"title":"Categories","date":"2019-01-17T01:48:32.936Z","updated":"2019-01-17T01:48:32.936Z","comments":true,"path":"categories/index.html","permalink":"https://jiacheng-pan.github.io/wiki/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-01-17T01:48:32.937Z","updated":"2019-01-17T01:48:32.937Z","comments":true,"path":"tags/index.html","permalink":"https://jiacheng-pan.github.io/wiki/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"WebGL学习笔记","slug":"webGL/WebGL学习笔记","date":"2019-06-06T09:39:11.000Z","updated":"2019-06-06T09:39:47.296Z","comments":true,"path":"2019/06/06/webGL/WebGL学习笔记/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/06/06/webGL/WebGL学习笔记/","excerpt":"","text":"WebGL学习笔记 入门 1234567891011121314151617181920212223242526var canvas = document.getElementById('webgl');var webgl = canvas.getContext('webgl'); // 获取webGL的context// 清除背景并填充颜色webgl.clearColor(0, 0, 0, 0.6);webgl.clear(webgl.COLOR_BUFFER_BIT);// 顶点着色器，主要用于描述顶点属性（大小，位置） GLSL ES语言var VSHADER_SOURCE = `void main() &#123; gl_Position = vec4(0.5, 0.5, 0, 1); gl_PointSize = 10.0;&#125;`;// 片元着色器，决定片元的颜色等，GLSL ES语言var FSHADER_SOURCE = `void main() &#123; gl_FragColor = vec4(1, 0, 0, 1);&#125;`;//初始化着色器，initShaders函数并非自带函数，需要调用createShader，compileShader等函数// initShaders详见：https://github.com/MrZJD/webgl/blob/master/lib/base.jsinitShaders( webgl, VSHADER_SOURCE, FSHADER_SOURCE)//画点，0表示从第0个点开始绘制，1表示绘制1个点webgl.drawArrays(webgl.POINTS, 0, 1); 使用attribute 123456789101112131415161718192021222324252627282930313233343536// 顶点着色器程序var VSHADER_SOURCE = `attribute vec4 a_Position; //声明attribute变量，存储限定符storage qualifier，必须声明成全局变量，从外部传入attribute float a_PointSize;// 限定符 数据类型 变量名void main() &#123; gl_Position = a_Position; gl_PointSize = a_PointSize;&#125;`;// 片元着色器程序var FSHADER_SOURCE = `void main() &#123; gl_FragColor = vec4(1, 0, 0, 1);&#125;`;var canvas = document.getElementById('webgl');var webgl = canvas.getContext('webgl');initShaders( webgl, VSHADER_SOURCE, FSHADER_SOURCE)// 获取attribute变量的地址var a_Position = webgl.getAttribLocation(webgl.program, 'a_Position');var a_PointSize = webgl.getAttribLocation(webgl.program, 'a_PointSize');// 根据地址给attribute变量赋值webgl.vertexAttrib3f(a_Position, 0.5, 0.5, 0.0); // 省略第四个参数，会默认赋值1.0webgl.vertexAttrib1f(a_PointSize, 10.0);// 还有vertexAttrib2f, vertexAttrib4f等函数// 也可以用矢量版本：// var position = new Float32Array([1.0, 2.0, 3.0, 1.0]);// gl.vertextAttrib4fv(a_Position, position)// 清空颜色缓冲区webgl.clearColor(0, 0, 0, 0.6);webgl.clear(webgl.COLOR_BUFFER_BIT);// 画点webgl.drawArrays(webgl.POINTS, 0, 1); 使用Uniform 只有顶点着色器才能用attribute变量，在片元着色器中，需要用uniform变量（当然也可以用varying）。 uniform变量用来从JavaScript程序向顶点着色器和片元着色器传输“一致的”（不变的）数据。 12345678910111213// 片元着色器程序var FSHADER_SOURCE = `precision mediump float;// 精度限定uniform vec4 u_FragColor;void main() &#123; gl_FragColor = u_FragColor;&#125;`;// 获取uniform变量的地址var u_FragColor = webgl.getUniformLocation(webgl.program, 'u_FragColor');// 根据地址给uniform变量赋值webgl.uniform4f(u_FragColor, 1.0, 1.0, 1.0, 1.0); 三角形 缓冲区 利用缓冲区对象可以向顶点着色器传入多个顶点。 分为五步： 创建缓冲区对象（gl.createBuffer()）。 绑定缓冲区对象（gl.bindBuffer()）。 将数据写入缓冲区（gl.bufferData()）。 将缓冲区对象分配给一个attribute变量（gl.vertextAttribPointer()）。 开启attribute变量（gl.enableVertextAttribArray()）。 1557306698055 1234567891011121314151617181920212223242526272829303132// 顶点数据，类型化数组，处理效率更高，但不支持push,pop；// 可以通过一个数组，或者一个数（也就是数组长度）来初始化var points = new Float32Array([0.0, 0.5, -0.5, -0.5, 0.5, -0.5]);var n = 3;// 创建缓冲区对象，利用deleteBuffer进行删除var vertexBuffer = webgl.createBuffer();// 绑定对象到缓冲区指针(ARRAY_BUFFER)上webgl.bindBuffer(webgl.ARRAY_BUFFER, vertexBuffer);/** gl.bufferData(target, data, usage); * 写入数据到缓冲区 * @param target gl.ARRAY_BUFFER 或 gl.ELEMENT_ARRAY_BUFFER * @param data * @param usage gl.STATIC_DRAW（只会向缓冲区对象写入一次数据，但是要绘制很多次）；gl.STREAM_DRAW（只会向缓冲区对象写入一次数据，然后绘制若干次）；gl.DYNAMIC_DRAW（多次写入数据，多次绘制） */webgl.bufferData(webgl.ARRAY_BUFFER, points, webgl.STATIC_DRAW);/** gl.vertexAttribPointer(location, size, type, normalized, stride, offset); * 指定attribute变量解析规则 * @param location 指定待分配的attribute变量 * @param size 指定每个顶点的数据数量（1-4个） * @param type 数据类型（gl.SHORT, gl.INT, gl.FLOAT等等） * @param normalized 是否需要归一化 * @param stride 指定相邻两个顶点间的字节数，默认为0 * @param offset 指定缓冲区对象中的偏移量（以字节为单位），即attribute变量从缓冲区中的何处开始读取。如果是从起始位置开始的，offset设为0 */webgl.vertexAttribPointer(a_Position, 2, webgl.FLOAT, false, 0, 0);// 启用attribute变量 =&gt; 即链接缓冲区到attribute变量上webgl.enableVertexAttribArray(a_Position); drawArrays的第一个参数 基本图形 参数mode 描述 点 gl.POINTS 一系列点 线段 gl.LINES 一系列单独的线段（(v0,v1),(v2,v3),(v4,v5),...） 线条 gl.LINE_STRIP 一系列连接的线段（(v0,v1),(v1,v2),(v2,v3),...） 回路 gl.LINE_LOOP 首尾相连的一系列线段（(v0,v1),(v1,v2),...,(vn,v0)） 三角形 gl.TRIANGLES 一系列单独的三角形（(v0,v1,v2),(v3,v4,v5),...） 三角带 gl.TRIANGLE_STRIP 一系列条带状的三角形（(v0,v1,v2),(v2,v1,v3),(v2,v3,v4),...） 三角扇 gl.TRIANGLE_FAN 一系列扇状的三角形（(v0,v1,v2),(v0,v2,v3),(v0,v3,v4),...） 1557313632070 平移 利用uniform变量来对图形进行平移： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 顶点着色器程序var VSHADER_SOURCE = `attribute vec4 a_Position;uniform vec4 u_Translation; //平移变换矢量void main() &#123; gl_Position = a_Position + u_Translation;&#125;`;// 片元着色器程序var FSHADER_SOURCE = `void main() &#123; gl_FragColor = vec4(1.0, 0, 0, 1.0);&#125;`;var canvas = document.getElementById('webgl');var webgl = canvas.getContext('webgl');initShaders( webgl, VSHADER_SOURCE, FSHADER_SOURCE)// 获取attribute变量的地址var a_Position = webgl.getAttribLocation(webgl.program, 'a_Position');// 获取uniform变量的地址var u_Translation = webgl.getUniformLocation(webgl.program, 'u_Translation');// 平移矢量值// 为什么平移矢量最后一个值是0：因为点（x,y,z,w）的w值只能为1.0，而原来点的w坐标已经是1.0了var tVal = [0.5, 0.5, 0, 0];// 设置uniform变量的值webgl.uniform4f(u_Translation, tVal[0], tVal[1], tVal[2], tVal[3]);// 顶点数据var points = new Float32Array([0, 0.5, -0.5, -0.5, 0.5, -0.5]);var n = points.length/2;// buffer对象var vertexBuffer = webgl.createBuffer();if ( !vertexBuffer ) &#123; console.error('Failed to create buffer!'); return -1;&#125;// 绑定对象到缓冲区指针(顶点数据)上webgl.bindBuffer(webgl.ARRAY_BUFFER, vertexBuffer);// 写入数据到缓冲区webgl.bufferData(webgl.ARRAY_BUFFER, points, webgl.STATIC_DRAW);// 指定attribute变量解析规则webgl.vertexAttribPointer(a_Position, 2, webgl.FLOAT, false, 0, 0);// 启用attribute变量 =&gt; 即链接缓冲区到attribute变量上webgl.enableVertexAttribArray(a_Position);// 清空颜色缓冲区webgl.clearColor(0, 0, 0, 0.6);webgl.clear(webgl.COLOR_BUFFER_BIT);// 绘制webgl.drawArrays(webgl.TRIANGLES, 0, n); 旋转 右手螺旋法则（right-hand-rule rotation）。 右手握拳，大拇指伸直并使其指向旋转轴的正方向，那么右手的其余几个手指就指明了正旋转（positive rotation）的方向。 1557316884620 假如点p经过正旋转\\(\\beta\\)角度，变成p'，其计算公式： \\[ x = r \\cos \\alpha \\\\\\ y = r \\sin \\alpha \\\\\\ x&#39; = r \\cos (\\alpha + \\beta) = r(\\cos \\alpha \\cos \\beta - \\sin \\alpha \\sin \\beta ) = x\\cos\\beta - y \\sin \\beta \\\\\\ y&#39; = r \\sin (\\alpha + \\beta) = r(\\sin \\alpha \\cos \\beta + \\cos \\alpha \\sin \\beta ) = y \\cos \\beta + x \\sin \\beta \\\\\\ z&#39; = z \\] 利用顶点着色器来进行旋转： 1234567891011121314151617181920212223// 顶点着色器程序var VSHADER_SOURCE = `attribute vec4 a_Position;uniform float u_CosB, u_SinB;//变换量void main() &#123; //旋转变换方式 gl_Position.x = a_Position.x * u_CosB - a_Position.y * u_SinB; gl_Position.y = a_Position.x * u_SinB + a_Position.y * u_CosB; gl_Position.z = a_Position.z; gl_Position.w = 1.0;&#125;`;//计算变换值var tAngle = 90;//角度制var tRadian = tAngle * Math.PI / 180;//弧度制// 获取uniform变量的地址var u_CosB = webgl.getUniformLocation(webgl.program, 'u_CosB');var u_SinB = webgl.getUniformLocation(webgl.program, 'u_SinB');// 设置uniform变量的值webgl.uniform1f(u_SinB, Math.sin(tRadian));webgl.uniform1f(u_CosB, Math.cos(tRadian)); 变换矩阵 Transformation Matrix 上面的旋转过程可以表示成： \\[ \\left[ \\begin{array}{c}{x^{\\prime}} \\\\ {y^{\\prime}} \\\\ {z^{\\prime}}\\end{array}\\right]=\\left[ \\begin{array}{ccc}{\\cos \\beta} &amp; {-\\sin \\beta} &amp; {0} \\\\ {\\sin \\beta} &amp; {\\cos \\beta} &amp; {0} \\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\times \\left[ \\begin{array}{l}{x} \\\\ {y} \\\\ {z}\\end{array}\\right] \\] 在webGL里面点的坐标是四维的，所以最终表示为\\(4 \\times 4\\)的旋转矩阵。 平移 \\[ \\left[ \\begin{array}{c}{x^{\\prime}} \\\\ {y^{\\prime}} \\\\ {z^{\\prime}} \\\\ {1}\\end{array}\\right]=\\left[ \\begin{array}{cccc}{1} &amp; {0} &amp; {0} &amp; {T x} \\\\ {0} &amp; {1} &amp; {0} &amp; {T y} \\\\ {0} &amp; {0} &amp; {1} &amp; {T z} \\\\ {0} &amp; {0} &amp; {0} &amp; {1}\\end{array}\\right] \\times \\left[ \\begin{array}{l}{x} \\\\ {y} \\\\ {z} \\\\ {1}\\end{array}\\right] \\] 旋转 \\[ \\left[ \\begin{array}{c}{x^{\\prime}} \\\\ {y^{\\prime}} \\\\ {z^{\\prime}} \\\\ {1}\\end{array}\\right]=\\left[ \\begin{array}{cccc}{\\cos \\beta} &amp; {-\\sin \\beta} &amp; {0} &amp; {0} \\\\ {\\sin \\beta} &amp; {\\cos \\beta} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {1} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {1}\\end{array}\\right] \\times \\left[ \\begin{array}{l}{x} \\\\ {y} \\\\ {z} \\\\ {1}\\end{array}\\right] \\] 缩放 \\[ \\left[ \\begin{array}{c}{x^{\\prime}} \\\\ {y^{\\prime}} \\\\ {z^{\\prime}} \\\\ {1}\\end{array}\\right]=\\left[ \\begin{array}{cccc}{S x} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {S y} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {S z} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {1}\\end{array}\\right] \\times \\left[ \\begin{array}{l}{x} \\\\ {y} \\\\ {z} \\\\ {1}\\end{array}\\right] \\] 相关代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 顶点着色器程序var VSHADER_SOURCE = `attribute vec4 a_Position;uniform mat4 u_xformMatrix; //变化矩阵void main() &#123; // 注意矩阵乘法的先后顺序 gl_Position = u_xformMatrix * a_Position;&#125;`;// 旋转变换矩阵var tAngle = 90;//角度制var tRadian = (tAngle * Math.PI) / 180;//弧度制var cosB = Math.cos(tRadian);var sinB = Math.sin(tRadian);var rotateMatrix = new Float32Array([// webgl中矩阵是列主序 cosB, sinB, 0.0, 0.0, -sinB, cosB, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]);// 平移变换矩阵var tx=0.5, ty=0.5, tz=0.0;var translateMatrix = new Float32Array([// webgl中矩阵是列主序 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, tx, ty, tz, 1.0]);// 缩放变换矩阵var sx=1.5, sy=1.5, sz=1.0;var scaleMatrix = new Float32Array([// webgl中矩阵是列主序 sx, 0.0, 0.0, 0.0, 0.0, sy, 0.0, 0.0, 0.0, 0.0, sz, 0.0, 0.0, 0.0, 0.0, 1.0]);var xformMatrix = scaleMatrix;// var xformMatrix = translateMatrix// var xformMatrix = rotateMatrix// 获取uniform变量的地址var u_xformMatrix = webgl.getUniformLocation( webgl.program, 'u_xformMatrix');webgl.uniformMatrix4fv(u_xformMatrix, false, xformMatrix); Matrix4对象 自定义对象，用来处理\\(4 \\times 4\\)矩阵。 详见：https://github.com/MrZJD/webgl/blob/master/lib/base.js 方法与属性名称 描述 Matrix4.elements 类型化数组（Float32Array）了Matrix4实例的矩阵元素 Matrix4.setIdentity() 将Matrix4实例初始化为单位阵 Matrix4.setTranslate(x,y,z) 将Matrix4实例设置为平移变换矩阵，在x轴上平移的距离为x，在y轴上平移的距离为y，在z轴上平移的距离为 z Matrix4.setRotate(angle,x,y,z) 将Matrix4实例设置为旋转变换矩阵，旋转的角度为angle，旋转轴为（x,y,z）。旋转轴（x,y,z）无须归一化。 Matrix4.setScale(x,y,z) 将Matrix4实例设置为缩放变换矩阵，在三个轴上的缩放因子分别为x、y和z Matrix4.translate(x,y,z) 将Matrix4实例乘以一个平移变换矩阵（该平移矩阵在x 轴上平移的距离为x，在y轴上平移的距离是y，在z轴上平移的距离是z），所得的结果还存储在Matrix4中 Matrix4.rotate(angle,x,y,z) 将Matrix4实例乘以一个旋转变换矩阵（该旋转矩阵旋转的角度为angle，旋转轴为（x,y,z）。旋转轴（x,y,z）无须归一化），所得的结果还存储AMatrix4中 Matrix4.scale(x,y,z) 将Matrix4实例乘以一个缩放变换矩阵（该缩放矩阵在三个轴上的缩放因子分别为x, y和z），所得的结果还存储在Matrix4中 Matrix4.set(m) 将Matrix4实例设置为m，m必须也是一个Matrix4实例 每秒钟自动旋转 需要引入：https://github.com/MrZJD/webgl/blob/master/lib/base.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263var VSHADER = `attribute vec4 a_Position;uniform mat4 u_xformMatrix;void main() &#123; gl_Position = u_xformMatrix * a_Position;&#125;`var FSHADER = `void main() &#123; gl_FragColor = vec4(1.0, 0, 0, 1.0);&#125;`var canvas = document.getElementById('webgl')var gl = getWebGLContext(canvas)initShaders(gl, VSHADER, FSHADER)var a_Position = gl.getAttribLocation(gl.program, 'a_Position')var u_xformMatrix = gl.getUniformLocation(gl.program, 'u_xformMatrix') // 变换矩阵// 顶点数据var points = new Float32Array([0, 0.3, -0.3, -0.3, 0.3, -0.3])// 创建buffer对象var vBuffer = gl.createBuffer()// 绑定buffer对象到指针上gl.bindBuffer(gl.ARRAY_BUFFER, vBuffer)// 将数据写入buffergl.bufferData(gl.ARRAY_BUFFER, points, gl.STATIC_DRAW)// 指定attribute变量解析规则gl.vertexAttribPointer(a_Position, 2, gl.FLOAT, false, 0, 0)// 将buffer传入到attribute变量上gl.enableVertexAttribArray(a_Position)// 绘制gl.clearColor(0, 0, 0, 0.6)var beginTime = new Date()var currentAngle = 0const anglePerSec = 30 // 每秒钟旋转30度var tick = function () &#123; var time = new Date() currentAngle = anglePerSec * (time - beginTime) / 1000 var rotateMatrix = new Matrix4() rotateMatrix.setRotate(currentAngle, 0, 0, 1) gl.uniformMatrix4fv(u_xformMatrix, false, rotateMatrix.elements) gl.clear(gl.COLOR_BUFFER_BIT) gl.drawArrays(gl.TRIANGLES, 0, points.length / 2) requestAnimationFrame(tick)&#125;tick() 颜色与纹理 非坐标数据传入顶点着色器 加入节点size 1234567891011121314151617181920212223242526272829303132333435363738// 顶点着色器程序var VSHADER_SOURCE = `attribute vec4 a_Position;attribute float a_PointSize;void main() &#123; gl_Position = a_Position; gl_PointSize = a_PointSize;&#125;`;// 获取attribute变量的地址var a_Position = webgl.getAttribLocation(webgl.program, 'a_Position');var a_PointSize = webgl.getAttribLocation(webgl.program, 'a_PointSize');// 顶点数据（x,y,size)var points = new Float32Array([0, 0.5, 2.0, -0.5, -0.5, 5.0, 0.5, -0.5, 10.0]);var n = points.length / 3;var FSIZE = points.BYTES_PER_ELEMENT;// buffer对象var vertexBuffer = webgl.createBuffer();if ( !vertexBuffer ) &#123; console.error('Failed to create buffer!'); return -1;&#125;// 绑定对象到缓冲区指针(顶点数据)上webgl.bindBuffer(webgl.ARRAY_BUFFER, vertexBuffer);// 写入数据到缓冲区webgl.bufferData(webgl.ARRAY_BUFFER, points, webgl.STATIC_DRAW);// 指定attribute变量解析规则// 这里的stride参数设置为了 FSIZE*3，因为一个节点现在有三个参数：x,y,sizewebgl.vertexAttribPointer(a_Position, 2, webgl.FLOAT, false, FSIZE*3, 0);webgl.vertexAttribPointer(a_PointSize, 1, webgl.FLOAT, false, FSIZE*3, FSIZE*2);// 启用attribute变量 =&gt; 即链接缓冲区到attribute变量上webgl.enableVertexAttribArray(a_Position);webgl.enableVertexAttribArray(a_PointSize); varying变量 之前利用uniform变量来将颜色信息传入片元着色器；利用varying变量可以从顶点着色器中向片元着色器中传输颜色； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 顶点着色器程序var VSHADER_SOURCE = `attribute vec4 a_Position;attribute vec4 a_Color;varying vec4 v_Color;void main() &#123; gl_Position = a_Position; v_Color = a_Color;&#125;`;// 片元着色器程序var FSHADER_SOURCE = `precision mediump float;varying vec4 v_Color;void main() &#123; gl_FragColor = v_Color;&#125;`;// 顶点数据: (x,y,r,g,b)var points = new Float32Array([0, 0.5, 1.0, 0.0, 0.0, -0.5, -0.5, 0.0, 1.0, 0.0, 0.5, -0.5, 0.0, 0.0, 1.0]);var n = points.length / 5;var FSIZE = points.BYTES_PER_ELEMENT;// buffer对象var vertexBuffer = webgl.createBuffer();if ( !vertexBuffer ) &#123; console.error('Failed to create buffer!'); return -1;&#125;// 绑定对象到缓冲区指针(顶点数据)上webgl.bindBuffer(webgl.ARRAY_BUFFER, vertexBuffer);// 写入数据到缓冲区webgl.bufferData(webgl.ARRAY_BUFFER, points, webgl.STATIC_DRAW);// 指定attribute变量解析规则// 坐标信息占两位webgl.vertexAttribPointer(a_Position, 2, webgl.FLOAT, false, FSIZE*5, 0);// 颜色信息占三位webgl.vertexAttribPointer(a_Color, 3, webgl.FLOAT, false, FSIZE*5, FSIZE*2);// 启用attribute变量 =&gt; 即链接缓冲区到attribute变量上webgl.enableVertexAttribArray(a_Position);webgl.enableVertexAttribArray(a_Color); 1557388439171 为什么是彩色三角形？ 顶点坐标，图形装配，光栅化，执行片元着色器 图形装配指的是：将孤立的顶点坐标装配成几何图形（几何图形的类型由drawArrays的第一个参数决定） 光栅化过程：将创配好的几何图形转化成片元 纹理 纹理坐标系统和webGL坐标系统 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125// 顶点着色器程序var VSHADER_SOURCE = ` attribute vec4 a_Position; attribute vec2 a_TextCoord; // 纹理坐标 varying vec2 v_TextCoord; void main() &#123; gl_Position = a_Position; v_TextCoord = a_TextCoord; &#125;`;// 片元着色器程序var FSHADER_SOURCE = ` precision mediump float; uniform sampler2D u_Sampler; // varying vec2 v_TextCoord; // void main() &#123; // 利用shader对图片进行反转 // gl_FragColor = texture2D(u_Sampler, vec2(v_TextCoord.x, -v_TextCoord.y)); gl_FragColor = texture2D(u_Sampler, v_TextCoord); &#125;`;var canvas = document.getElementById('webgl');var webgl = canvas.getContext('webgl');initShaders( webgl, VSHADER_SOURCE, FSHADER_SOURCE);webgl.clearColor(0, 0, 0, 0.6);// 初始化buffervar attrData = new Float32Array([ //顶点坐标 纹理坐标 -0.5, 0.5, 0.0, 1.0, -0.5, -0.5, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.5, -0.5, 1.0, 0.0]);var n = attrData.length / 4;var FSIZE = attrData.BYTES_PER_ELEMENT;var buffer = webgl.createBuffer();webgl.bindBuffer(webgl.ARRAY_BUFFER, buffer);webgl.bufferData(webgl.ARRAY_BUFFER, attrData, webgl.STATIC_DRAW);var a_Position = webgl.getAttribLocation(webgl.program, 'a_Position');var a_TextCoord = webgl.getAttribLocation(webgl.program, 'a_TextCoord');webgl.vertexAttribPointer(a_Position, 2, webgl.FLOAT, false, FSIZE*4, 0);webgl.vertexAttribPointer(a_TextCoord, 2, webgl.FLOAT, false, FSIZE*4, FSIZE*2);webgl.enableVertexAttribArray(a_Position);webgl.enableVertexAttribArray(a_TextCoord);//图片加载成功后 =&gt; 进行纹理加载loadImage('http://localhost:8080/static/sky.jpg') .then(function(img)&#123; loadTexture(webgl, n, img); &#125;);// 加载图片function loadImage (url) &#123; return new Promise(function(resolve, reject)&#123; var img = new Image(); img.onload = () =&gt; &#123; resolve(img) &#125;; img.onerror = (e) =&gt; &#123; console.error('Failed to load image!'); reject(e)&#125;; img.src = url; &#125;);&#125;// 绘制纹理function loadTexture (webgl, n, image) &#123; // 创建纹理对象 var texture = webgl.createTexture(); // 获取纹理的存储位置 var u_Sampler = webgl.getUniformLocation(webgl.program, 'u_Sampler'); // 对纹理进行Y轴反转，第一个参数表示对Y轴进行反转，第二个参数1表示true，0表示false // 因为纹理坐标系统中的t轴方向与PNG，BMP，JPG等格式图片的坐标系统Y轴方向相反 // 也可以在着色器中手动反转 webgl.pixelStorei(webgl.UNPACK_FLIP_Y_WEBGL, 1); //开启0号纹理单元，一共有0-7八个纹理单元 webgl.activeTexture(webgl.TEXTURE0); //向target绑定纹理对象 webgl.bindTexture(webgl.TEXTURE_2D, texture); // 配置纹理参数 // 第一个参数为纹理地址 // 第二个参数为 映射方法： // * gl.TEXTURE_MAG_FILTER(纹理绘制范围超过纹理本身时，进行放大) // * gl.TEXTURE_MIN_FILTER(纹理绘制范围小于纹理本身时，进行缩小) // * gl.TEXTURE_WRAP_S(如何对纹理左侧或者右侧的区域进行填充) // * gl.TEXTURE_WRAP_T(如何对纹理上方或者下方的区域进行填充) // 第三个参数是对第二个参数的补充： // * 如果选择（TEXTURE_MAG_FILTER/TEXTURE_MIN_FILTER），第三个参数可以选择： // * gl.NEAREST：使用原纹理上距离映射后像素（新像素）中心最近的那个像素的颜色值，作为新像素的值（使用曼哈顿距离） // * gl.LINEAR：使用距离新像素中心最近的四个像素的颜色值的加权平均，作为新像素的值（与gl.NEAREST相比，该方法图像质量更好，但是会有较大的开销。） // * 如果选择（TEXTURE_WRAP_S/TEXTURE_WRAP_T），第三个参数可以选择： // * gl.REPEAT：平铺式的重复纹理 // * gl.MIRRORED_REPEAT：镜像对称式的重复纹理 // * gl.CLAMP_TO_EDGE：使用纹理图像边缘值 webgl.texParameteri(webgl.TEXTURE_2D, webgl.TEXTURE_MIN_FILTER, webgl.LINEAR); /** gl.texImage2D(target, level, internalformat, format, type, image); * 配置纹理图像 * @param target 纹理对象的目标地址 * @param level 为金字塔纹理准备的，平时传入0 * @param internalformat 图像的内部格式（RGB,RBGA,ALPHA,LUMINANCE,LUMUNANCE_ALPHA） * @param format 纹理数据的格式，必须使用与internalformat相同的值 * @param type 纹理数据的类型（UNISGNED_BYTE，每个颜色分量占一个字节；UNSIGNED_SHORT_5_6_5，RGB每个分量分别占5/6/5比特；UNSIGNED_SHORT_4_4_4_4，RGBA每个分量分别占4/4/4/4比特；UNSIGNED_SHORT_5_5_5_1：RGBA每个分量占5/5/5/1比特） * @param image 纹理图像的image对象 */ webgl.texImage2D(webgl.TEXTURE_2D, 0, webgl.RGB, webgl.RGB, webgl.UNSIGNED_BYTE, image); // 将0号纹理传递给着色器 webgl.uniform1i(u_Sampler, 0); // 绘图 webgl.clear(webgl.COLOR_BUFFER_BIT); webgl.drawArrays(webgl.TRIANGLE_STRIP, 0, n);&#125; 当修改纹理区域： 12345678// 初始化buffervar attrData = new Float32Array([ //顶点坐标 纹理坐标 -0.5, 0.5, -0.3, 1.7, -0.5, -0.5, -0.3, -0.2, 0.5, 0.5, 1.7, 1.7, 0.5, -0.5, 1.7, -0.2]); 结果就会变成： 1557454940388 因为TEXTURE_WRAP_S和TEXTURE_WRAP_T的默认值都是REPEAT。 当修改： 123webgl.texParameteri(webgl.TEXTURE_2D, webgl.TEXTURE_MIN_FILTER, webgl.LINEAR);webgl.texParameteri(webgl.TEXTURE_2D, webgl.TEXTURE_WRAP_S, webgl.CLAMP_TO_EDGE);webgl.texParameteri(webgl.TEXTURE_2D, webgl.TEXTURE_WRAP_T, webgl.MIRRORED_REPEAT); 1557455085332 左右变成了边缘值的重复；上下则是镜像； 多幅纹理 详见：https://github.com/MrZJD/webgl/blob/master/05/lesson05.js","categories":[{"name":"webgl","slug":"webgl","permalink":"https://jiacheng-pan.github.io/wiki/categories/webgl/"}],"tags":[{"name":"webgl","slug":"webgl","permalink":"https://jiacheng-pan.github.io/wiki/tags/webgl/"}]},{"title":"WebGL-BatchDraw代码阅读+理解","slug":"webGL/WebGL-BatchDraw代码阅读+理解","date":"2019-06-06T09:38:00.000Z","updated":"2019-06-06T09:38:55.168Z","comments":true,"path":"2019/06/06/webGL/WebGL-BatchDraw代码阅读+理解/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/06/06/webGL/WebGL-BatchDraw代码阅读+理解/","excerpt":"","text":"WebGL-BatchDraw代码阅读+理解 WebGL2和WebGL1的区别 推荐相关文章：WebGL2系列之从WebGL1迁移到WebGL2 代码中的shader部分几乎没什么区别，只是用了一些300 es版本的新特性： 用了in/out代替varing 片元着色器中不再使用gl_FragColor 关于WebGL-BatchDraw的渲染系统（pixels坐标系统） 点渲染 首先，我们知道webGL的坐标系统是右手坐标系（z轴垂直于屏幕向外，如下图左）： 1557915075222 而为我们所熟悉的坐标系往往像上图右边这样的（也就是源码中对应的pixels坐标系统）。 在webGL-BatchDraw里面，用菱形来拟合一个点，这里的一个菱形是用两个三角形拼起来的： 12345678[-0.5, 0.0, 1.0,0.0, -0.5, 1.0,0.0, 0.5, 1.0,0.5, 0.0, 1.0]// TRIANGLE_STRIP的意思是：// 第一个第二个第三个坐标用来画第一个三角形；// 第二个第三个第四个坐标用来画第二个三角形this.GL.drawArraysInstanced(this.GL.TRIANGLE_STRIP, 0, 4, this.numDots); 1557915075222 而文章使用了实例化(Instancing)的技术，用来加速绘制。 实例化是一种只调用一次渲染函数却能绘制出很多物体的技术，它节省渲染物体时从CPU到GPU的通信时间，而且只需做一次即可。 每当你需要绘制一个新的点，你就需要对已经实例化好的对象进行变换。 对现有的实例进行缩放；我们知道 现有的点太小了（长度和高度都只有1）；假设我们设置的size为\\(s\\)，那么我们就需要对现有的实例放大\\(s\\)倍（z轴不放大）；其缩放矩阵为： \\[ \\left[ \\begin{array}{ccc}{s} &amp; {0} &amp; {0} \\\\ {0} &amp; {s} &amp; {0}\\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\] 位移；要把实例对象移动到我们想要的位置：\\((x, y)\\)，那么平移矩阵是： \\[ \\left[ \\begin{array}{ccc}{1} &amp; {0} &amp; {x} \\\\ {0} &amp; {1} &amp; {y}\\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\] 我们把这两个矩阵相乘合并，也就得到了在原始空间内的变换矩阵： \\[ \\left[ \\begin{array}{ccc}{s} &amp; {0} &amp; {x} \\\\ {0} &amp; {s} &amp; {y}\\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\] 也就是源代码的translate矩阵（577行，779行，857行，947行）： 12345// 因为glsl中是列主序的，相当于做了一个转置mat3 translate = mat3( dotSize, 0, 0, 0, dotSize, 0, dotPos.x, dotPos.y, 1); projection；将原始空间的坐标，映射到webGL中。 假如一个点的在原始空间的坐标是\\((x, y)\\)，其对应的webGL坐标应该是： \\[ (\\frac{x-\\frac{w}{2}}{w / 2}, -\\frac{y-\\frac{h}{2}}{h / 2}) = \\\\\\ (x\\times\\frac{2}{w}-1,-y\\times\\frac{2}{h}+1) \\] 相当于进行了一次缩放和一次平移，那么我们将它写成矩阵： \\[ \\left[ \\begin{array}{ccc}{2/w} &amp; {0} &amp; {-1} \\\\ {0} &amp; {-2/h} &amp; {1}\\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\] 这个矩阵也就是原来的projection矩阵（221行）： 123let projection = new Float32Array([2 / this.canvas.width, 0, 0, 0, -2 / this.canvas.height, 0, -1, 1, 1]); 最后整合一下： 就变成了如何把实例对象的坐标（vertexPos）变成需要绘制的对象的坐标： 1gl_Position = vec4(projection * translate * vertexPos, 1.0); 边渲染 首先，BatchDraw中，同样也是用两个三角形组成的正方形来拟合一条线。 其实例化对象： 1557915075222 缩放；要将实例化对象缩放到目标的长宽：\\((ll, lw)\\)（分别指代线的长度和线的宽度），其缩放矩阵（scale矩阵）： \\[ \\left[ \\begin{array}{ccc}{ll} &amp; {0} &amp; {0} \\\\ {0} &amp; {lw} &amp; {0}\\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\] 旋转；假设 边连接的两个点的坐标分别是：\\((start.x, start.y), (end.x, end.y)\\)，那么需要旋转的夹角是： \\[ \\phi = atan (\\frac{end.y-start.y}{end.x-start.x})=atan(\\frac{delta.y}{delta.x}) \\] 旋转矩阵（rotate矩阵）就可以写作： \\[ \\left[ \\begin{array}{ccc}{\\cos\\phi} &amp; {-\\sin\\phi} &amp; {0} \\\\{\\sin\\phi} &amp; {\\cos\\phi} &amp; {0} \\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\] 平移；经过以上两步，实例的中心位置仍然停留在\\((0,0)\\)上，需要平移到目标的中心点\\(((start.x+end.x)/2, (start.y+end.y)/2)\\)。平移矩阵（translate矩阵）写作： \\[ \\left[ \\begin{array}{ccc}{1} &amp; {0} &amp; {(start.x+end.x)/2} \\\\ {0} &amp; {1} &amp; {(start.y+end.y)/2)}\\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\] 映射；映射到webGL坐标系中，原理同点渲染一致 \\[ \\left[ \\begin{array}{ccc}{2/w} &amp; {0} &amp; {-1} \\\\ {0} &amp; {-2/h} &amp; {1}\\\\ {0} &amp; {0} &amp; {1}\\end{array}\\right] \\]","categories":[{"name":"webgl","slug":"webgl","permalink":"https://jiacheng-pan.github.io/wiki/categories/webgl/"}],"tags":[{"name":"webgk","slug":"webgk","permalink":"https://jiacheng-pan.github.io/wiki/tags/webgk/"}]},{"title":"矩阵相关的概念","slug":"GraphKnowledge/矩阵相关的概念","date":"2019-01-18T05:56:00.000Z","updated":"2019-01-21T07:56:33.441Z","comments":true,"path":"2019/01/18/GraphKnowledge/矩阵相关的概念/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/矩阵相关的概念/","excerpt":"","text":"adjacency matrix 邻接矩阵 一个图的邻接矩阵的定义是：\\(A=\\lbrace a_{ij} \\rbrace\\)，其中\\(a_{ij}=e_{ij}\\)，第\\(ij\\)个元素指的是边\\(e_{ij}\\)的值。 incidence matrix 关联矩阵 一个图的关联矩阵定义为：\\(M=\\lbrace m_{ij} \\rbrace\\)，其中\\(m_{ij}\\)为1，如果节点\\(v_i\\)和边\\(e_j\\)相关，举例而言： 的关联矩阵为： e1 e2 e3 e4 e5 e6 v1 1 1 0 0 0 0 v2 0 0 1 1 0 1 v3 0 0 0 0 1 1 v4 1 0 1 0 0 0 v5 0 1 0 1 1 0","categories":[{"name":"GraphKnowledge","slug":"GraphKnowledge","permalink":"https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"}],"tags":[{"name":"图","slug":"图","permalink":"https://jiacheng-pan.github.io/wiki/tags/图/"},{"name":"graph","slug":"graph","permalink":"https://jiacheng-pan.github.io/wiki/tags/graph/"}]},{"title":"链接相关的概念","slug":"GraphKnowledge/链接相关的概念","date":"2019-01-18T05:55:00.000Z","updated":"2019-01-21T12:45:10.953Z","comments":true,"path":"2019/01/18/GraphKnowledge/链接相关的概念/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/链接相关的概念/","excerpt":"","text":"bridge/cut edge/isthmus 桥边/割边/峡谷 Bridge，也被称为cut edge或者isthmus，指的是某一条边，如果在图中删掉该边，这个图中的连通子图的数量会增加，那么这条边就会被称为bridge。","categories":[{"name":"GraphKnowledge","slug":"GraphKnowledge","permalink":"https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"}],"tags":[{"name":"图","slug":"图","permalink":"https://jiacheng-pan.github.io/wiki/tags/图/"},{"name":"graph","slug":"graph","permalink":"https://jiacheng-pan.github.io/wiki/tags/graph/"}]},{"title":"结构相关的概念","slug":"GraphKnowledge/结构相关的概念","date":"2019-01-18T05:55:00.000Z","updated":"2019-01-21T13:07:59.255Z","comments":true,"path":"2019/01/18/GraphKnowledge/结构相关的概念/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/结构相关的概念/","excerpt":"","text":"bipartite 二部 将图的顶点分成两个非空集合\\(V_1\\)和\\(V_2\\)，如果这个图的每一条边的两个端点都分别属于\\(V_1\\)和\\(V_2\\)，那么称这个图是二部图（bipartite）。 在这个概念的基础上，还有完全二部图（complete bipartite graph）的概念，指的是在bipartite的基础上，\\(V_1\\)中的每个点，都与\\(V_2\\)中的每个点相连。 graphlets 图元 图元，指的是一系列异构的（isomorphism）导出（induced）子图。 下面列举所有的3节点、4节点、5节点的图元： 3节点、4节点、5节点的所有图元 induced 导出 给定一个图\\(G\\)，以及这个图的子图（subgraph）\\(G&#39;=(V&#39;, E&#39;)\\)，其中\\(V&#39;\\subseteq V\\)并且\\(E&#39; \\subseteq E\\)。如果\\(E&#39; = \\{(v_i,v_j)|(v_i,v_j) \\in E \\text{ and }v_i, v_j \\in V&#39;\\}\\)，也就是对于所有属于\\(V&#39;\\)的节点，他们在原图\\(G\\)的中出现的所有边，也都出现在\\(G&#39;\\)的\\(E&#39;\\)中，那么，这个子图\\(G&#39;\\)称为\\(G\\)的导出（induced）子图。 下面是导出子图的一个示例： 导出子图 isomorphism 同构 如果两个图\\(G = (V,E)\\) 和 \\(G&#39; = (V&#39;, E&#39;)\\) 之间存在一个双射（bijection）函数 \\(f: V \\rightarrow V&#39;\\)，使得$ (v_i, v_j) E (f(v_i),f(v_j)) E' v_i,v_j V$，那么这两个图互为同构图（isomorphism） subgraph 子图 给定一个图\\(G\\)，那么定义这个图的子图\\(G&#39;=(V&#39;, E&#39;)\\)，其中\\(V&#39;\\subseteq V\\)并且\\(E&#39; \\subseteq E\\)。","categories":[{"name":"GraphKnowledge","slug":"GraphKnowledge","permalink":"https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"}],"tags":[{"name":"图","slug":"图","permalink":"https://jiacheng-pan.github.io/wiki/tags/图/"},{"name":"graph","slug":"graph","permalink":"https://jiacheng-pan.github.io/wiki/tags/graph/"}]},{"title":"节点相关的概念","slug":"GraphKnowledge/节点相关的概念","date":"2019-01-18T05:54:00.000Z","updated":"2019-03-21T07:45:43.747Z","comments":true,"path":"2019/01/18/GraphKnowledge/节点相关的概念/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/节点相关的概念/","excerpt":"","text":"centrality 中心度 centrality，中心度，可以用于回答“重要节点的特征是什么？”之类的问题。一般而言，有这三种度量方式比较常用： degree centrality[/#degree-度] betweenness centrality[/#betweenness-介数] closeness centrality[#closeness-亲密度] betweenness 介数 节点的betweenness，可以定义为： \\[ betweenness(v) = \\sum_{s \\neq v \\neq t \\in V}\\frac{\\sigma_{st}(v)}{\\sigma_{st}} \\] 其中\\(\\sigma_{st}\\)指的是节点\\(s, t\\)之间的最短路的数量，而\\(\\sigma_{st}(v)\\)则是其中经过节点\\(v\\)的路径数量。 上述定义可以阐述为： 计算网络中任意两个节点之间的所有最短路径，那么某个节点的bewteenness可以被定义为路过这个节点的最短路数量比例。 closeness 亲密度 节点的closeness centrality，可以被定义为： \\[ closeness(v) = \\frac{1}{\\sum_u d(v,u)} \\] 其中，\\(d\\)指两个节点之间的距离，也即它们之间的最短路长度。如果一个节点距离整个图的所有节点都很接近，那么它的closeness也就会很高。 degree 度 一个顶点的度（degree）指与该顶点关联的边的数目。当边有权重时，就是所有边的权重和。记做\\(deg(v)\\)。 在有向图中，还有出度（out-degree）和入度（in-degree）的概念。 出度（out-degree）指以该顶点为起点的边的权重和，入度（in-degree）指的是以该顶点为终点的边的权重和。","categories":[{"name":"GraphKnowledge","slug":"GraphKnowledge","permalink":"https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"}],"tags":[{"name":"图","slug":"图","permalink":"https://jiacheng-pan.github.io/wiki/tags/图/"},{"name":"graph","slug":"graph","permalink":"https://jiacheng-pan.github.io/wiki/tags/graph/"}]},{"title":"模型选择和特征选择","slug":"吴恩达·机器学习/16-模型选择和特征选择","date":"2019-01-18T05:53:00.000Z","updated":"2019-01-20T14:21:01.748Z","comments":true,"path":"2019/01/18/吴恩达·机器学习/16-模型选择和特征选择/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/18/吴恩达·机器学习/16-模型选择和特征选择/","excerpt":"","text":"首先来说几种验证训练结果的方法。 保留交叉验证法 hold out cross validation 给出一个训练集，随机分成两个，一部分当做训练子集（一般占70%）用于训练，另一部分当做保留交叉验证子集（30%）用于测试。 k重交叉验证法 k-fold cross validation 将整个数据集分成k部分，拿出k-1部分进行训练，将剩下一个用于测试。重复上述过程k次（每次都不一样），求出均值即为验证结果。一般k取10。 优点：增加了训练数据；缺点：计算代价高； 留一交叉验证法 leave one out cross validation 当k=m（训练数据量）时，称之为留一交叉验证法。一般用于训练数据过少的情况。 特征选择 在进行学习时，过多的特征容易带来过拟合，需要选出一个特征子集，其中的特征与学习算法最相关。 前向搜索法 forward search 过程如下： 初始化特征集\\(\\cal F\\) 对于不属于\\(\\cal F\\)的每一个特征，计算添加该特征后模型精度的提升； 选择提升最大的特征加入\\(\\cal F\\) 重复2和3，直到精度不在上升。 当然可以设置阈值k，当\\(\\cal F\\)包含了k个特征后即停止。 后向搜索法 backward search 基本步骤与上述类似，只是过程相反：首先从满的特征集开始，之后每次删除表现最差的特征。 上述的方法都被称为\"wrapping\" feature selection（“封装”特征选择），\"封装\"这个词，意味着： 当你进行选择时（前向搜索或者后向搜索）你需要重复使用学习算法去训练模型，根据结果来选择特征子集。 这种方法主要缺点是计算量大，但是它是一种比较准确的选择方法。 另外有一种算法，可能它的泛化误差不会太低，但是计算代价较小。 特征过滤 filter method 主要方法是，计算每一个特征的一些度量，来衡量对y（label）的影响有多大，一般使用互信息（Mutual Information）来度量： \\[ \\begin{align} MI(x_i, y) &amp;= \\sum_{x_i \\in \\lbrace 0,1 \\rbrace} \\sum_{y \\in \\lbrace 0,1 \\rbrace} p(x_i, y) \\log \\frac{p(x_i, y)}{p(x_i)p(y)} \\\\\\ &amp;= KL(p(x, y) || p(x)p(y)) \\text{, KL divergence} \\end{align} \\] 然后去选择最好的k个特征。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"模型选择","slug":"模型选择","permalink":"https://jiacheng-pan.github.io/wiki/tags/模型选择/"},{"name":"特征选择","slug":"特征选择","permalink":"https://jiacheng-pan.github.io/wiki/tags/特征选择/"}]},{"title":"Introduction","slug":"GraphKnowledge/Home","date":"2019-01-18T05:53:00.000Z","updated":"2019-01-21T07:54:50.910Z","comments":true,"path":"2019/01/18/GraphKnowledge/Home/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/Home/","excerpt":"","text":"一些图论、图挖掘等图相关领域的知识收集、汇总。","categories":[{"name":"GraphKnowledge","slug":"GraphKnowledge","permalink":"https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"}],"tags":[{"name":"图","slug":"图","permalink":"https://jiacheng-pan.github.io/wiki/tags/图/"},{"name":"graph","slug":"graph","permalink":"https://jiacheng-pan.github.io/wiki/tags/graph/"}]},{"title":"Vapnik–Chervonenkis dimension","slug":"吴恩达·机器学习/15-Vapnik-Chervonenkis Dimension","date":"2019-01-10T05:53:00.000Z","updated":"2019-01-20T14:21:01.717Z","comments":true,"path":"2019/01/10/吴恩达·机器学习/15-Vapnik-Chervonenkis Dimension/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/10/吴恩达·机器学习/15-Vapnik-Chervonenkis Dimension/","excerpt":"","text":"延续上节课的内容。 给定\\(|\\cal H| = k\\)，给定\\(\\delta, \\gamma\\)，为了保证： \\[ \\varepsilon({\\hat h}) \\leq \\varepsilon(h) + 2\\gamma \\] 的概率不小于\\(1-\\delta\\)，那么\\(m\\)需要满足： \\[ m \\geq \\frac{1}{2\\gamma^2}\\log \\frac{2k}{\\delta} = O(\\frac{1}{\\gamma^2}\\log \\frac{k}{\\delta}) \\] 假如\\(\\cal H\\)是以\\(d\\)个实数为参数的(比如为了解决n个特征的分类问题，d就等于n+1)，而在计算机中，实数多以64位浮点数保存，d个实数就需要64d位来存储，那么\\(\\cal H\\)的整个假设空间大小就为\\(2^{64d}\\)，也即\\(k=2^{64d}\\)，那么： \\[ m \\geq O(\\frac{1}{\\gamma^2}\\log \\frac{k}{\\delta}) = O(\\frac{d}{\\gamma^2}\\log \\frac{1}{\\delta}) \\] 最直观的解释就是\\(m\\)与假设类的参数数量几乎是成正比的。 定义Shatter（分散）：给定一个由\\(d\\)个点构成的集合：\\(S=\\lbrace x^{(1)}, \\ldots, x^{(d)} \\rbrace\\)，我们说一个假设类\\(\\cal H\\)能够分散(shatter)一个集合\\(S\\)，如果\\(\\cal H\\)能够实现对\\(S\\)的任意一种标记方式，也即，对\\(S\\)的任意一种标记方式，我们都可以从\\(\\cal H\\)中找到对应的假设来进行分割。 举例而言，如果\\({\\cal H} = \\lbrace \\text{linear classification in 2D} \\rbrace\\)(二维线性分类器的集合)，对于二维平面上的三个点，有8种标记方式： image-20190111094049430 那么，蓝线所代表的线性分类器，都能完成对它们的标记，所以我们称\\(\\cal H\\)能够分散平面上三个点所构成的集合。但是对于平面上四个点，就有存在以下这种情况，没有任何的线性分类器能够实现这种标记： image-20190111094649618 定义Vapnik-Chervonenkis dimension（VC维）：假设集\\(\\cal H\\)的VC维，写成\\(VC({\\cal H})\\)，指的是能够被\\(\\cal H\\)分散的最大集合的大小。 举例而言，如果\\(\\cal H\\)是所有二维线性分类器构成的集合，那么\\(VC(\\cal H) = 3\\)。当然并不是说\\(\\cal H\\)要能分散所有三个点构成的集合，只要有某个三个点构成的集合能被\\(\\cal H\\)分散即可，比如下面这种标记方式，\\(\\cal H\\)就无法实现，但是我们还是称\\(VC(\\cal H) = 3\\)。 image-20190111095306079 有一个推论： \\(VC({\\text{linear classification of n D}}) = n + 1\\) 定理：给定假设集合\\(\\cal H\\)，令\\(VC({\\cal H})=d\\)，那么，对于任意的\\(h \\in {\\cal H}\\)： \\[ |\\varepsilon(h)-{\\hat \\varepsilon}(h)| \\leq O(\\sqrt{\\frac{d}{m} \\log \\frac{m}{d} + \\frac{1}{m} \\log \\frac{1}{\\delta}}) \\] 的概率不小于\\(1 - \\delta\\)，以及 \\[ \\varepsilon({\\hat h}) \\leq \\varepsilon(h^\\ast) + 2 \\gamma, \\gamma = O(\\sqrt{\\frac{d}{m} \\log \\frac{m}{d} + \\frac{1}{m} \\log \\frac{1}{\\delta}}) \\] 的概率不小于\\(1-\\delta​\\)。 引理：为了保证\\(\\varepsilon({\\hat h}) \\leq \\varepsilon(h ^ \\ast) + 2 \\gamma\\)至少在\\(1 - \\delta\\)的概率下成立，应该满足： \\[ m = O_{\\gamma, \\delta}(d) \\] \\(O_{\\gamma, \\delta}(d)\\)指的是，在固定\\(\\gamma, \\delta\\)的情况下，与\\(d\\)线性相关。 也即，\\(m\\)必须与\\(\\cal H\\)的VC维保持一致，也可以这么理解，为了使泛化误差和训练误差近似，训练样本数目必须和模型的参数数量成正比。 在SVM中，给定数据集，如果我们只考虑半径R以内的点，以及间隔至少为\\(\\gamma\\)的线性分类器构成的假设类，那么： \\[ VC({\\cal H}) \\leq \\lceil \\frac{R^2}{4\\gamma^2} \\rceil + 1 \\] 也就说明，\\(\\cal H​\\) 的VC维上限，并不依赖于数据集中点\\(x​\\)的维度，换句话说，虽然点可能位于无限维的空间中，但是如果只考虑那些具有较大函数间隔的分类器所组成的假设类，那么VC维就存在上界。 所以SVM会自动尝试找到一个具有较小VC维的假设类，所以它不会过拟合（模型参数不会过大）","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"特征选择","slug":"特征选择","permalink":"https://jiacheng-pan.github.io/wiki/tags/特征选择/"},{"name":"VC维","slug":"VC维","permalink":"https://jiacheng-pan.github.io/wiki/tags/VC维/"}]},{"title":"经验风险最小化","slug":"吴恩达·机器学习/14-经验风险最小化","date":"2019-01-05T05:53:00.000Z","updated":"2019-01-20T14:21:01.717Z","comments":true,"path":"2019/01/05/吴恩达·机器学习/14-经验风险最小化/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/05/吴恩达·机器学习/14-经验风险最小化/","excerpt":"","text":"就线性分类模型而言，可以将其表示为： \\[ h_\\theta(x)=g(\\theta^Tx), \\\\\\ g(z) = 1\\lbrace z \\geq 0 \\rbrace \\] 其中，训练集表示为： \\[ S=\\lbrace (x^{(i)}, y^{(i)}) \\rbrace _ {i = 1} ^ m, (x^{(i)}, y^{(i)}) \\sim {\\cal D} \\] 这里假设了训练数据都是独立同分布的。 那么，我们认为，这个线性分类器的训练误差就可以表示为它分类错误的样本比例： \\[ {\\hat{\\varepsilon}}(h_\\theta) = {\\hat{\\varepsilon}}_s(h_\\theta) = \\frac{1}{m}\\sum_{i=1}^m1\\lbrace h_\\theta (x^{(i)}) \\neq y^{(i)} \\rbrace \\] 在这里，我们把训练误差也称为风险（risk），由此我们导出了经验风险最小化。 经验风险最小化 Empirical Risk Minimization，ERM 经验风险最小化，最终导出一组参数，能够使得训练误差最小： \\[ {\\hat{\\theta}} = \\arg \\min {\\hat{\\varepsilon}}_s(h_\\theta) \\] 我们再定义一个假设类\\({\\cal{H}} = \\lbrace h_\\theta, \\theta \\in {\\Bbb R}^{n+1} \\rbrace\\)，它是所有假设的集合。在线性分类中，也就是所有线性分类器的集合。 那么，我们可以重新定义一次ERM： \\[ {\\hat h} = \\mathop{\\arg \\min}_{h \\in {\\cal H}} {\\hat \\varepsilon}(h) \\] 对上述公式的直观理解就是：从假设类中选取一个假设，使得训练误差最小。我们这里用了\\(\\hat{h}\\)表示估计，因为毕竟不可能得到最好的假设，只能得到对这个最好的假设的估计。 但这仍然不是目标，我们的目标是使得泛化误差 Generalization Error最小化，也即新的数据集上分类错误的概率： \\[ \\varepsilon(h)=P_{(x,y) \\sim {\\cal D}}(h(x) \\neq y) \\] 接下去，为了证明： （1）\\({\\hat \\varepsilon} \\approx \\varepsilon\\)，训练误差近似于泛化误差（理解为，泛化误差和训练误差之间的差异存在上界） （2）ERM输出的泛化误差\\(\\varepsilon({\\hat h})\\)存在上界； 我们引出两个引理： 联合界引理（Union Bound） \\(A_1, A_2, \\ldots , A_k\\)是k个事件，他们之间并不一定是独立分布的，有： \\[ P(A_1 \\cup \\ldots \\cup A_k) \\leq P(A_1) + \\dots + P(A_k) \\] Hoeffding不等式（Hoeffding Inequality） \\(z_1, \\ldots z_m\\)是m个iid（independent and identically distribution，独立同分布），他们都服从伯努利分布，\\(P(z_i=1) = \\phi\\)，那么对\\(\\phi\\)的估计： \\[ {\\hat \\phi} = \\frac{1}{m}\\sum_{i=1}^m z_i \\] 于是，给定\\(\\gamma &gt; 0\\)，有： \\[ P(|{\\hat{\\phi}} - \\phi| &gt; \\gamma) \\leq 2 exp(-2\\gamma^2m) \\] Hoeffding不等式的直观解释就是，下图中的阴影面积，会有上界。 image-20190106143941030 一致收敛 Uniform Conversions 对于某个\\(h_j \\in \\cal{H}\\)，我们定义\\(z_i = 1 \\lbrace h_j(x^{(i)}) \\neq y^{(i)}\\rbrace \\in \\lbrace{}\\)为第i个样本被分类错误的指示函数的值，对于logistic而言，它服从伯努利分布。 那么： 泛化误差：\\(P(z_i=1) = \\varepsilon(h_j)\\) 训练误差：\\({\\hat{\\varepsilon}}(h_j) = \\frac{1}{m}\\sum_{i=1}^m z_i\\) 根据Hoeffding不等式，我们能够得到： \\[ P(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma) \\leq 2e^{-2\\gamma^2m} \\] 接着，我们定义训练误差和泛化误差之间的差大于\\(\\gamma\\)（\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma\\)）为事件\\(A_j\\)，根据以上结论，我们可知： \\[ P(A_j) \\leq 2e^{-2\\gamma^2m} \\] 那么根据联合界引理： \\[ \\begin{array}{l} &amp; P(\\exists h_j \\in H, |{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma) \\\\\\ = &amp; P(A_1 \\cup A_2 \\cup \\ldots \\cup A_k) \\\\\\ \\leq &amp; \\sum_{i=1}^k P(A_i) \\\\\\ \\leq &amp; \\sum_{i=1}^k 2e^{-2\\gamma^2m} \\\\\\ = &amp; 2ke^{-2\\gamma^2m} \\end{array} \\] 可以表述为：存在\\(h_j\\)使\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma\\)的概率\\(\\leq 2ke^{-2\\gamma^2m}\\)。 等价于：不存在\\(h_j\\)使\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma\\)的概率\\(\\geq 1 - 2ke^{-2\\gamma^2m}\\)。 等价于：\\(\\cal H\\)中任意的\\(h_j\\)使得\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| \\leq \\gamma\\)的概率\\(\\geq 1 - 2ke^{-2\\gamma^2m}\\)。 我们将上面这个结论称之为一致收敛 Uniform Conversions，也就是说事实上，所有的假设，训练误差和泛化误差之间都存在上界。 样本复杂度，误差界以及偏差方差权衡 上面的结论，我们可以引出以下的一些推论： 样本复杂度 Sample Complexity 给定\\(\\gamma, \\delta\\)，需要多大的训练集合（\\(m\\)）？其中\\(\\delta\\)指的是泛化误差和训练误差之差大于\\(\\gamma\\)的概率。 我们知道，\\(\\delta \\leq 2ke^{-2\\gamma^2m}\\)，可求解： \\[ m \\geq \\frac{1}{2 \\gamma ^ 2} log(\\frac{2k}{\\delta}) \\] 这个，也被称为样本复杂度（类似于时间复杂度），指的是，只要满足上面这个条件，任意\\(h \\in \\cal H\\)，都能得到\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| \\leq \\gamma\\) 误差界 Error Bound 给定\\(\\delta, m\\)时，我们会得到多大的误差上界\\(\\gamma\\)。 经过求解可以得到： \\[ P(\\forall h \\in {\\cal H}, |{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| \\leq \\sqrt{\\frac{1}{2m}log(\\frac{2k}{\\delta})}) \\geq 1 - \\delta \\] 也就是误差上界是：\\(\\gamma = \\sqrt{\\frac{1}{2m}log(\\frac{2k}{\\delta})}\\)。 偏差方差权衡 Bias Variance Tradeoff 我们定义： \\[ \\begin{align} {\\hat h} &amp;= \\mathop{\\arg \\min}_{h \\in \\cal H} {\\hat \\varepsilon}(h) \\text{, 使得训练误差最小的h} &amp;\\tag{1} \\\\\\ h^\\ast &amp;= \\mathop{\\arg \\min}_{h \\in \\cal H} \\varepsilon(h) \\text{, 使得泛化误差最小的h} \\tag{2} \\end{align} \\] 假如： \\[ \\forall h \\in {\\cal H}, |{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| \\leq \\gamma \\tag{3} \\] 那么： \\[ \\begin{align} \\varepsilon(\\hat h) &amp;\\leq {\\hat \\varepsilon}({\\hat h}) + \\gamma, &amp;\\text{derived from (3)}\\\\\\ &amp;\\leq {\\hat \\varepsilon}(h^\\ast) + \\gamma, &amp;\\text{derived from (1)}\\\\\\ &amp;\\leq {\\varepsilon(h^\\ast)} + \\gamma + \\gamma, &amp;\\text{ derived from (3)} \\end{align} \\] 于是，我们得到如下定理： 给定大小为\\(k\\)的假设集合\\(\\cal H\\)，给定\\(m, \\delta\\)，那么： \\[ \\varepsilon(\\hat h) \\leq \\underbrace{(\\min_{h \\in {\\cal H}}\\varepsilon(h))}_{\\varepsilon(h^\\ast)} + 2 \\underbrace{\\sqrt{\\frac{1}{2m}log(\\frac{2k}{\\delta})}}_{\\gamma} \\] 的概率不低于\\(1-\\delta\\)。 可以想象，为了得到最佳的假设\\(h^\\ast\\)，我们尽可能增大\\(\\cal H\\)（能够减小\\(\\varepsilon(h^\\ast)\\)），但随之而来的就是\\(\\gamma\\)的增大，所以需要在这两者之间进行权衡，我们指的就是偏差方差权衡 Bias Variance Tradeoff。 image-20190106154201189 由此，我们得到一个推论： 给定\\(\\delta, \\gamma\\)，为了能够保证\\(\\varepsilon(\\hat h) \\leq (\\min_{h \\in {\\cal H}}\\varepsilon(h)) + 2\\gamma\\)的概率不小于\\(1-\\delta\\)（ERM得到的假设的一般误差，与最佳假设的一般误差之间，差值不大于\\(2\\gamma\\)） 我们需要保证： \\[ m \\geq \\frac{1}{2\\gamma^2}log(\\frac{2k}{\\delta}) \\]","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"ERM","slug":"ERM","permalink":"https://jiacheng-pan.github.io/wiki/tags/ERM/"},{"name":"经验风险","slug":"经验风险","permalink":"https://jiacheng-pan.github.io/wiki/tags/经验风险/"}]},{"title":"SVM（四）非线性决策边界","slug":"吴恩达·机器学习/13-SVM（四）非线性决策边界","date":"2018-07-25T05:53:00.000Z","updated":"2019-01-20T14:21:01.716Z","comments":true,"path":"2018/07/25/吴恩达·机器学习/13-SVM（四）非线性决策边界/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/07/25/吴恩达·机器学习/13-SVM（四）非线性决策边界/","excerpt":"","text":"当数据中存在异常点时，比如上述的情况，导致原先可以用直线a分割的数据现在不得不用b来进行，以保证完美的分割。由此我们引出了非线性决策边界（non-linear decision boundaries）来解决这样的问题。 观察原SVM问题的目标： \\[ \\min_{w, b} \\frac{1}{2}||w|^2 \\\\\\ \\text{ s.t. }y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq 1, i=1,\\ldots,m \\] 我们为原公式增加惩罚项，对不同的数据点增加不同的惩罚，使得所有样本能够更好地分割： \\[ \\min_{w, b} \\frac{1}{2}||w|^2+c\\sum_{i=1}^m\\xi_i, \\xi_i\\geq0 \\] 使得 \\[ y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq 1-\\xi_i, i=1,\\ldots,m \\] 注意到，我们之前认为$ y^{(i)} (w^T x^{(i)}+b) 1 $是分类正确的，在这里我们允许一部分样本小于1，也就是说明我们允许了一部分样本分类错误。 构建拉格朗日算子： \\[ {\\cal L}(w, b, \\xi, \\alpha, \\gamma) = \\frac{1}{2}||w|^2+c\\sum_i\\xi_i-\\sum_i^m\\alpha_i(y^{(i)} (w^T \\cdot x^{(i)}+b)-1+\\xi_i)-\\sum_i^m\\gamma_i\\xi_i \\] 对偶： \\[ \\max W(\\alpha) = \\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_jy_iy_j\\langle x_i \\cdot x_j \\rangle \\] 跟原先的SVM问题的唯一区别在于其限制条件为： \\[ \\sum_{i=1}^my^{(i)}\\alpha_i=0 \\\\\\ 0 \\leq \\alpha_i \\leq c \\] 其收敛条件： 对于大部分数据点： \\[ \\alpha_i=0 \\Rightarrow y^{(i)} (w^T \\cdot x^{(i)}+b) \\geq 1 \\] 对于异常点： \\[ \\alpha_i = c \\] 对于最近点： \\[ 0&lt;\\alpha_i&lt;c \\] 坐标上升法（Coordinate Ascent） 考虑优化问题： \\[ \\max W(\\alpha_1, \\alpha_2, ..., \\alpha_m) \\] 不考虑约束条件， 重复 { ​ For i = 1 to m: \\[ \\alpha_i := \\arg \\max_{\\hat{\\alpha}_i} W(\\alpha_1,\\ldots,{\\hat{\\alpha}}_i,\\ldots,\\alpha_m) \\] } 直到收敛； 这个算法，可以认为是执行了以下这个过程（以m=2为例）： 坐标上升法，不断沿着坐标轴方向前进 顺序最小优化算法（Sequential minimal optimization, SMO） 顺序最小优化算法的基本理念就是在坐标上升法的基础上，改成一次性优化其中两个\\(\\alpha\\)，而固定其他的\\(m-2\\)个\\(\\alpha\\)。 假如我们更新\\(\\alpha_1, \\alpha_2\\)： 因为在之前我们提到: \\[ \\sum_i^m \\alpha_iy^{(i)}=0 \\] 于是有： \\[ \\alpha_1 y^{(1)}+\\alpha_2 y^{(2)} = -\\sum_{i=3}^m\\alpha_iy^{(i)}= \\zeta \\] 那么 \\[ \\alpha_1=\\frac{\\zeta-\\alpha_2y^{(2)}}{y^{(1)}} \\] \\[ W(\\alpha_1, \\alpha_2, ..., \\alpha_m) = W(\\frac{\\zeta-\\alpha_2y^{(2)}}{y^{(1)}}, \\alpha_2, ..., \\alpha_m) \\] 在非线性决策边界优化问题中，其实\\(W\\)是一个关于\\(\\alpha_i\\)的二次函数，固定其他\\(\\alpha\\)之后，\\(W\\)函数就可以被简化为： \\[ a\\alpha_2^2+b \\alpha_2 + c \\] 很容易就能求解。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://jiacheng-pan.github.io/wiki/tags/SVM/"},{"name":"非线性决策边界","slug":"非线性决策边界","permalink":"https://jiacheng-pan.github.io/wiki/tags/非线性决策边界/"},{"name":"坐标上升法","slug":"坐标上升法","permalink":"https://jiacheng-pan.github.io/wiki/tags/坐标上升法/"}]},{"title":"SVM（三）核函数","slug":"吴恩达·机器学习/12-SVM（三）核函数","date":"2018-07-22T13:33:00.000Z","updated":"2019-01-21T07:29:17.147Z","comments":true,"path":"2018/07/22/吴恩达·机器学习/12-SVM（三）核函数/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/07/22/吴恩达·机器学习/12-SVM（三）核函数/","excerpt":"","text":"在SVM(二)中，我们看到了如下的表示形式： \\[ W(\\alpha)=\\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) \\] 这里，内积\\((x_i \\cdot x_j)\\)就是最简单的核函数的形式。一般核函数会被写成\\(\\langle x^{(i)}, x^{(j)} \\rangle\\)的形式。 有时候，我们会将一些特征转换到高维空间上，就像我们在之前的过拟合&amp;局部加权回归中提到的，比如特征\\(x\\)表示的是房屋面积，我们需要预测房子是否会在6个月内被卖出，我们有时候会将这个特征映射成如下的形式： \\[ x \\rightarrow \\begin{bmatrix} x \\\\\\ x^2 \\\\\\ x^3 \\\\\\ x^4 \\end{bmatrix} = \\phi(x) \\] 原先的特征的内积形式\\(\\langle x^{(i)}, x^{(j)} \\rangle\\)会被写成\\(\\langle \\phi(x^{(i)}), \\phi(x^{(j)}) \\rangle\\)，而且往往\\(\\phi(x)\\)会有很高的维度。因为在很多情况下，计算\\(\\phi(x)\\)会有很高的代价，或者表示\\(\\phi(x)\\)需要很高的代价，但是光是计算内核则可能代价较小。 比如：假如有两个输入：\\(x, z \\in \\Bbb R^n\\)，核函数被定义为： \\[ \\begin{align} k(x, z) = (x^T z)^2 &amp;= (\\sum_{i=1}^nx_iz_i)(\\sum_{j=1}^nx_jz_j) \\\\\\ &amp;=\\sum_{i=1}^n\\sum_{j=1}^n(x_ix_j)(z_iz_j) \\\\\\ &amp;= \\phi(x)^T\\phi(z) \\end{align} \\] 假如需要表示成高维向量，那么\\(\\phi(x)\\)是一个\\(n \\times n\\)维的向量，如果\\(n = 3\\)： \\[ \\phi(x) = \\begin{bmatrix} x_1x_1 \\\\\\ x_1x_2 \\\\\\ x_1x_3 \\\\\\ x_2x_1 \\\\\\ \\vdots \\\\\\ x_3x_3 \\end{bmatrix} \\] 所以，计算\\(\\phi(x)\\)的时间复杂度就达到了\\(O(n^2)\\)，而计算核函数仅仅需要计算\\(x^Tz\\)，复杂度为\\(O(n)\\)。 接下去我们为这个核函数增加常数项： \\[ k(x,z)=(x^Tz+c)^2 \\] 那么： \\[ \\phi(x) = \\begin{bmatrix} x_1x_1 \\\\\\ x_1x_2 \\\\\\ x_1x_3 \\\\\\ x_2x_1 \\\\\\ \\vdots \\\\\\ x_3x_3 \\\\\\ \\sqrt{2c}x_1 \\\\\\ \\sqrt{2c}x_2 \\\\\\ \\sqrt{2c}x_3 \\\\\\ c \\end{bmatrix} \\] 更一般的： \\[ k(x, z)=(x^Tz+c)^d \\] 有了核函数，即可替换SVM中的内积\\(\\langle x^{(i)}, x^{(j)} \\rangle\\)，比如常用的高斯核： \\[ k(x,z)=\\exp(-\\frac{||x-z||^2}{2\\sigma^2}) \\] 有了核函数，相当于把数据从原始空间转换到了高位空间，很多数据，在一维空间往往是线性不可分的，但是到了高维空间会变成可分的： 核函数的合法性 如何判断一个核函数是合法的呢？判断依据是：是否存在函数\\(\\phi\\)，使得\\(k(x,z)\\)能够被写成\\(\\langle \\phi(x), \\phi(z) \\rangle\\)。 定理：如果核函数合法，那么其对应的核矩阵（kernel matrix）是半正定的。 核矩阵指的是矩阵\\(K \\in \\Bbb R^{m\\times m}\\)，其中\\(K_{ij}=k(x^{(i)}, x^{(j)})\\)。半正定的意思是，对于任意向量\\(z\\)，都存在\\(z^TKz \\geq 0\\)，证明如下： \\[ \\begin{align} z^TKz &amp;= \\sum_i\\sum_jz_iK_{ij}z_j \\\\\\ &amp;= \\sum_i\\sum_jz_i\\phi_(x^{(i)})^T\\phi_(x^{(j)})z_j \\\\\\ &amp;= \\sum_i\\sum_jz_i\\cdot \\sum_k\\phi_(x^{(i)})_k\\underbrace{\\phi_(x^{(j)})_k}_{向量第k项} \\cdot z_j \\\\\\ &amp;= \\sum_k\\sum_i\\sum_jz_i\\cdot \\phi_(x^{(i)})_k\\phi_(x^{(j)})_k \\cdot z_j \\\\\\ &amp;= \\sum_k(\\sum_iz_i\\phi(x^{(i)}))^2 \\geq 0 \\end{align} \\] 事实上，上面的定理的逆命题也一样成立，总结起来： Merce定理：给定核函数\\(k(x, z)\\)，那么\\(k(x, z)\\)合法（也即\\(\\exists \\phi, k(x,z)=\\phi(x)^T\\phi(z)\\)），当且仅当，对所有的\\(\\lbrace x^{(1)}, \\ldots, x^{(m)} \\rbrace\\)，核矩阵\\(K \\in \\Bbb R^{m\\times m}\\)是一个对称的半正定矩阵。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://jiacheng-pan.github.io/wiki/tags/SVM/"},{"name":"核函数","slug":"核函数","permalink":"https://jiacheng-pan.github.io/wiki/tags/核函数/"}]},{"title":"SVM（二）最优间隔分类器","slug":"吴恩达·机器学习/11-SVM（二）最优间隔分类器","date":"2018-07-03T08:27:00.000Z","updated":"2019-01-21T07:29:06.672Z","comments":true,"path":"2018/07/03/吴恩达·机器学习/11-SVM（二）最优间隔分类器/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/07/03/吴恩达·机器学习/11-SVM（二）最优间隔分类器/","excerpt":"","text":"最优间隔分类器（Optimal Margin Classifier）。其目标是使得最小几何间隔最大化（SVM（一）概念）： \\[ \\text{目标(1):} \\\\\\ \\max_{w, b} \\gamma \\\\\\ \\text{ s.t. } y^{(i)} \\cdot ((\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||}) \\geq \\gamma, i=1,\\ldots,n \\] 我们知道，\\(\\hat{\\gamma} = \\frac{\\gamma}{||w||}\\)，所以上面的目标可以等同于： \\[ \\text{目标(2):} \\\\\\ \\max_{w, b} \\frac{\\hat{\\gamma}}{||w||} \\\\\\ \\text{ s.t. }y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq \\hat{\\gamma}, i=1,\\ldots,n \\] 为了最大化上述值，我们有两种策略。 增大\\(\\hat{\\gamma}\\) 减小\\(||w||\\) 针对第一种可能，我们要证明其无效性。假如，我们增大\\(\\hat{\\gamma}\\)到\\({\\hat{\\gamma}}_1 := \\lambda {\\hat{\\gamma}}\\)，因为\\(\\hat{\\gamma}=y(w^Tx+b)\\)，可以视作\\(w_1:=\\lambda w, b_1 = \\lambda b\\)。所以，此时 \\[ \\frac{\\hat{\\gamma_1}}{||w_1||}=\\frac{\\lambda \\hat{\\gamma}}{||\\lambda w||} = \\frac{\\hat{\\gamma}}{||w||} \\\\\\ \\] 没有发生任何改变，所以第一条策略不可行。于是，我们可以固定\\(\\hat{\\gamma}=1\\) 此时，上述目标(2)可以表述成： \\[ \\text{目标(3):} \\\\\\ \\min_{w, b} \\frac{1}{2}||w||^2 \\\\\\ \\text{ s.t. }y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq 1, i=1,\\ldots,n \\] 因为最小化\\(||w||\\)和最小化\\(\\frac{1}{2}||w||^2\\)是一致的。 拉格朗日乘子法（Lagrange Multiplier） 为了解决上述的凸优化问题，我们引入拉格朗日乘子法Lagrange Multiplier来解决这个问题。 我们首先来看看凸优化问题的定义： \\[ \\min_wf(w) \\\\\\ \\text{s.t. }g_i(w) \\leq 0, h_i(w) =0 \\] 构建拉格朗日乘子： \\[ {\\cal L}(w, \\alpha, \\beta) = f(w)+\\sum_i\\alpha_ig_i(w)+\\sum_i\\beta_ih_i(w) \\] 定义： \\[ \\theta_p(w) = \\max_{\\alpha_i&gt;0, \\beta}{\\cal L}(w, \\alpha, \\beta) \\] 观察\\(\\theta_p(w)\\)： 如果\\(g_i(w)&gt;0\\)，那么\\(\\theta_p(w)=+\\infty\\)（因为\\(\\alpha\\)可以取任意大值）。 如果\\(h_i(w) \\neq 0\\)，那么\\(\\theta_p(w)=+\\infty\\)（因为\\(\\beta\\)可以取\\(+\\infty/-\\infty\\)）。 所以，在满足约束的情况下，\\(\\theta_p(w)=f(w)\\)，\\(\\min_w \\theta_p(w)=\\min_w f(w)\\)，因为使得\\({\\cal L}(w, \\alpha, \\beta)\\)最大的方法，就是其他所有项全是0。那么，可以得出这样的结论： \\[ \\theta_p(w)=\\begin{cases} f(w), &amp;\\text{满足约束} \\\\\\ \\infty, &amp;\\text{不满足约束} \\end{cases} \\] 因此，在满足条件的情况下，\\(\\min_w\\theta_p(w)\\)等价于\\(min_wf(w)\\)。 我们将最优间隔分类器的目标重新表示一下： \\[ p^\\ast =\\min_{w, b}\\max_\\alpha {\\cal L(w, \\alpha, b)} \\\\\\ {\\cal L}(w, \\alpha, b) = \\frac{1}{2}||w||^2+\\sum_i\\alpha_i(1-y^{(i)}(w^T x^{(i)}+b)) \\] 其中，直接忽略了\\(h_i(w)=0\\)的约束，而\\(g_i(w,b)=1-y^{(i)}(w^Tx^{(i)}+b) \\leq 0, f(w)=\\frac{1}{2}||w||^2\\) 对偶问题（Dual Problem） 一般来说，将原始问题转化成对偶问题来求解。一是因为对偶问题往往比较容易求解，二是因为对偶问题引入了核函数，方便推广到非线性分类的情况。 我们看到，之前的原始问题，是 \\[ p^\\ast =\\min_{w, b}\\max_\\alpha {\\cal L}(w, \\alpha, b) \\] 那么，定义其对偶问题： \\[ l^\\ast =\\max_\\alpha\\min_{w,b}{\\cal L}(w, \\alpha, b) \\] 接下去，我们求解对偶问题： 先求解\\(\\min_{w,b}{\\cal L}(w, \\alpha, b)\\)： 分别求偏导，使其等于0，导出最小值： \\[ \\begin{align} &amp; \\nabla_w{\\cal L}(w, \\alpha, b) =w-\\sum_{i=1}\\alpha_iy^{(i)}x^{(i)}=0 \\\\\\ &amp; \\nabla_b{\\cal L}(w, \\alpha, b) =\\sum_{i=1}\\alpha_iy^{(i)}=0 \\end{align} \\] 得到： \\[ w =\\sum_{i=1}\\alpha_iy^{(i)}x^{(i)} \\\\\\ \\sum_{i=1}\\alpha_iy^{(i)} = 0 \\] 代入\\({\\cal L}(w, \\alpha, b)\\)，就可以得到最小值： \\[ \\begin{align} {\\cal L}(w, \\alpha, b) &amp;= \\frac{1}{2}||w||^2+\\sum_i\\alpha_i(1-y^{(i)}(w^T x^{(i)}+b)) \\\\\\ \\min_{w, b}{\\cal L}(w, \\alpha, b) &amp;=\\underbrace{\\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j)}_{W(\\alpha)} \\end{align} \\] 于是，我们的对偶问题简化到了对\\(W(\\alpha)\\)最大化： \\[ \\max_\\alpha W(\\alpha) \\\\\\ \\text{s.t. }\\alpha_i \\geq 0, \\sum_iy_i\\alpha_i=0 \\] 假设，我们解得的对偶问题的解为：\\(\\alpha^\\ast =[\\alpha_1^\\ast ,\\alpha_2^\\ast , \\ldots, \\alpha_m^\\ast ]\\)，那么最终原始问题的解可以表示成： \\[ w^\\ast =\\sum_{i=1}\\alpha_i^\\ast y^{(i)}x^{(i)} \\] 在原始问题中，还有\\(b\\)未得到解决。我们先来观察一下约束项： \\[ g_i(w,b)=1-y{(i)}(w^Tx^{(i)}+b) \\leq 0 \\] 我们知道，在数据中，只有少数的几个数据点，他们的函数距离为1（最小），也即\\(g_i(w,b)=0\\)，如图所示。 在整个数据集中，只有这些数据点对约束超平面起了作用，这些数据点被称为支持向量（support vector），其对应的\\(\\alpha_i^\\ast \\neq 0\\)，而其他不是支持向量的数据点，没有对约束超平面起作用，其\\(\\alpha_i^\\ast =0\\)。 此时，我们已经得到了$w\\(，而\\)b\\(的计算如下，找到一个数据点，其\\)_j^0$(也就是支持向量，其函数间隔为1)，我们就能得到： \\[ y^{(j)}(w^{*T}x^{(j)}+b^\\ast )=1 \\Rightarrow b^\\ast =y^{(j)}-\\sum_{i=1}\\alpha_i^\\ast y^{(i)}(x^{(i)} \\cdot x^{(j)}) \\]","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://jiacheng-pan.github.io/wiki/tags/SVM/"},{"name":"最优间隔分类器","slug":"最优间隔分类器","permalink":"https://jiacheng-pan.github.io/wiki/tags/最优间隔分类器/"},{"name":"拉格朗日乘子法","slug":"拉格朗日乘子法","permalink":"https://jiacheng-pan.github.io/wiki/tags/拉格朗日乘子法/"}]},{"title":"SVM（一）概念","slug":"吴恩达·机器学习/10-SVM（一）概念","date":"2018-07-02T08:27:00.000Z","updated":"2019-01-20T14:21:01.715Z","comments":true,"path":"2018/07/02/吴恩达·机器学习/10-SVM（一）概念/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/07/02/吴恩达·机器学习/10-SVM（一）概念/","excerpt":"","text":"SVM，指的是支持向量机（support vector machines）。 支持向量机，假设数据是线性可分的，那么我们就能找到一个超平面，将数据分成两类。但是一旦线性可分，我们就可能找到无数的超平面，都可以将数据分成两类： 但是很明显，上图中虽然a, c都对数据进行了有效的分割。但很明显，都不如b分割的好。 我们可以用“间隔”这个概念来定义这个超平面（在二维上是线）对数据的分割优劣。在分类正确的情况下，间隔越大，我们认为对数据的分类越好。 我们的目标是得到数据的分类：\\(y \\in \\lbrace -1, +1 \\rbrace\\)。 这个超平面，则可以表示成\\(w^Tx+b\\)，其中\\(w=[\\theta_1, \\ldots, \\theta_n]^T, b=\\theta_0\\)。这个超平面可以表达成一个\\(n+1\\)维向量。 判别函数： \\[ g(z)=\\begin{cases} +1, &amp; \\text{如果$z\\geq0$} \\\\\\ -1, &amp; \\text{otherwise} \\end{cases} \\] 假设则可以表示成：\\(h_{w,b}(x)=g(w^Tx+b)\\) 间隔 函数间隔（functional margin） 某个超平面\\((w,b)\\)和训练样本\\((x^{(i)}, y^{(i)})\\)之间的函数间隔被表示成： \\[ \\hat{\\gamma}^{(i)}=y^{(i)}(w^Tx^{(i)}+b) \\] 于是，我们可以知道： 当\\(y^{(i)}=1\\)，于是我们想获得更大的函数间隔（这是我们的目标），就需要使得\\(w^Tx^{(i)}+b \\gg 0\\) 相反，当\\(y^{(i)}=-1\\)，我们想获得更大的函数间隔，就需要使得\\(w^Tx^{(i)}+b \\ll 0\\) 并且，很明显，只有当函数间隔\\(\\hat{\\gamma}&gt;0\\)时，分类结果是正确的。 最后，超平面与数据集\\(\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots \\rbrace\\)之间的函数间隔，被定义为所有函数间隔中的最小值： \\[ \\hat{\\gamma}=\\min_i\\hat{\\gamma}^{(i)} \\] 几何间隔（geometric margin） 从点\\((x^{(i)}, y^{(i)})\\)出发，对超平面做垂线，得到点D，我们知道他们之间的距离，就是该超平面到数据点\\((x^{(i)}, y^{(i)})\\)的几何间隔。 经过推导，D的坐标可以表示为： \\[ x^{(i)}-\\gamma^{(i)}\\frac{w}{||w||} \\] 又因为，D在超平面\\(w^Tx+b=0\\)上，所以： \\[ \\begin{align} &amp; w^T(x^{(i)}-\\gamma^{(i)}\\frac{w}{||w||})+b=0 \\\\\\ &amp; \\Rightarrow w^Tx^{(i)}+b=\\gamma^{(i)} \\cdot \\frac{w^Tw}{||w||}=\\gamma^{(i)} \\cdot ||w|| \\\\\\ &amp; \\Rightarrow \\gamma^{(i)}=(\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||} \\end{align} \\] 加上正负分类的判断： \\[ \\gamma^{(i)}=y^{(i)} \\cdot ((\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||}) \\] 我们可以看到，几何间隔跟函数间隔之间存在如下的关系： \\[ \\hat{\\gamma}^{(i)} = \\frac{\\gamma^{(i)}}{||w||} \\] 同样的，超平面与数据集\\(\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots \\rbrace\\)之间的几何间隔，被定义为所有几何间隔中的最小值： \\[ \\gamma=\\min_i\\gamma^{(i)} \\] 最后，我们导出最优间隔分类器（Optimal Margin Classifier）问题：选择\\(w, b\\)，最大化\\(\\gamma\\)，同时满足\\(\\forall(x^{(i)}, y^{(i)})\\)，$ y^{(i)} (()^T x^{(i)}+) $（所有数据点的几何间隔都大于该最小几何间隔）。 目前为止，已经是SVM问题的一个简化版本。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://jiacheng-pan.github.io/wiki/tags/SVM/"},{"name":"函数间隔","slug":"函数间隔","permalink":"https://jiacheng-pan.github.io/wiki/tags/函数间隔/"},{"name":"几何间隔","slug":"几何间隔","permalink":"https://jiacheng-pan.github.io/wiki/tags/几何间隔/"}]},{"title":"生成学习算法的例子","slug":"吴恩达·机器学习/09-生成学习算法的例子","date":"2018-06-29T13:10:00.000Z","updated":"2019-01-21T07:28:52.424Z","comments":true,"path":"2018/06/29/吴恩达·机器学习/09-生成学习算法的例子/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/29/吴恩达·机器学习/09-生成学习算法的例子/","excerpt":"","text":"例一：高斯判别分析和logistic函数 我们来看一个例子，对于一个高斯判别分析问题，根据贝叶斯： \\[ \\begin{align} p(y=1|x) &amp;= \\frac{p(x|y=1)p(y=1)}{p(x)} \\\\\\ &amp;= \\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)} \\end{align} \\] 在这里，我们提出几个假设： \\(p(y)\\)是均匀分布的，也就是\\(p(y=1)=p(y=0)\\) \\(x\\)的条件概率分布（\\(p(x|y=0)\\)和\\(p(x|y=1)\\)）满足高斯分布。 考虑二维的情况： image-20180630164349595 蓝色数据表达的是\\(p(x|y=0)\\)的分布，红色数据表达的是\\(p(x|y=1)\\)的分布，两条蓝色和红色的曲线分别是它们的概率密度曲线。 而灰色的曲线则表示了\\(p(y=1|x)\\)的概率密度曲线。 假设\\(p(x|y=0) \\sim N(\\mu_0, \\sigma_0)\\)，\\(p(x|y=1) \\sim N(\\mu_1, \\sigma_1)\\)，而\\(p(y)\\)均匀分布那么： \\[ \\begin{align} p(y=1|x) &amp;= \\frac{N(\\mu_0,\\sigma_0)}{N(\\mu_0,\\sigma_0)+N(\\mu_1,\\sigma_1)} \\\\\\ &amp;= \\cdots \\\\\\ &amp;= \\frac{1}{1+\\frac{\\sigma_0}{\\sigma_1}exp(2\\sigma_1^2(x-\\mu_0)^2-2\\sigma_0^2(x-\\mu_1)^2} \\end{align} \\] 事实上，这条曲线跟我们之前见过的logistic曲线非常像，特别是当我们假设\\(\\sigma_0=\\sigma_1\\)的时候，就是一条logistic曲线。 我们有如下的推广结论： \\[ {\\begin{cases} p(x|y=1) \\sim Exp Family(\\eta_1) \\\\\\ p(x|y=0) \\sim Exp Family(\\eta_0) \\end{cases}} \\Rightarrow p(y=1|x)是logistic函数 \\] 但这个命题的逆命题并不成立，故而我们知道，logistic所需要的假设更少（无需假设\\(x\\)的条件概率分布），鲁棒性更强。而生成函数因为对数据的分布做出了假设，所以需要的数据量会少于logstic回归，我们需要在两者之间进行权衡。 例二：垃圾邮件分类（1） 这里我们会用朴素贝叶斯（Naive Bayes）来解决垃圾邮件分类问题（\\(y\\in \\lbrace 0, 1 \\rbrace\\)）。 首先对邮件进行建模，生成特征向量如下： \\[ x= \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 0 \\\\\\ \\vdots \\\\\\ 1 \\\\\\ \\vdots \\end{bmatrix} \\begin{matrix} a \\\\\\ advark \\\\\\ ausworth \\\\\\ \\vdots \\\\\\ buy \\\\\\ \\vdots \\end{matrix} \\] 这是一个类似于词频向量的特征向量，我们有一个50000个词的词典，如果邮件中出现了某个词汇，那么其在向量中对应的位置就会被标记为1，否则为0。 我们的目标是获取，垃圾邮件和非垃圾邮件的特征分别是怎么样的，也即\\(p(x|y)\\)。\\(x={\\lbrace 0, 1 \\rbrace}^n, y \\in \\lbrace 0, 1 \\rbrace\\)，这里我们的词典中词汇数量是50000，所以\\(n=50000\\)，特征向量\\(x\\)会有\\(2^{50000}\\)种可能，需要\\(2^{50000}-1\\)个参数。 我们假设\\(x_i|y\\)之间相互独立(虽然假设各个单词的出现概率相互独立不是很合理，但是即便这样，朴素贝叶斯的效果依旧不错)，根据朴素贝叶斯，我们得到： \\[ p(x_1, x_2, \\ldots, x_{50000}|y)=p(x_1|y)p(x_2|y) \\cdots p(x_{50000}|y) \\] 单独观察\\(p(x_j|y=1)​\\)： \\[ p(x_j|y=1) = p(x_j=1|y=1)^{x_j}p(x_j=0|y=1)^{1-x_j} \\] 给定三个参数： \\[ \\begin{align} \\phi_{j|y=1} &amp;= p(x_j=1|y=1) \\\\\\ \\phi_{j|y=0} &amp;= p(x_j=1|y=0) \\\\\\ \\phi_y &amp;= p(y = 1) \\end{align} \\] 故： \\[ \\begin{align} p(x_j|y=1) &amp;= \\phi_{j|y=1}^{x_j}(\\phi_y - \\phi_{j|y=1})^{1-x_j}\\\\\\ p(x_j|y=0) &amp;= \\phi_{j|y=0}^{x_j}(1-\\phi_y + \\phi_{j|y=0})^{1-x_j} \\\\\\ p(x_j|y) &amp;= p(x_j|y=1)^yp(x_j|y=0)^{1-y} \\\\\\ p(y) &amp;= \\phi_y^y(1-\\phi_y)^{1-y} \\end{align} \\] 按照上个博客生成学习算法的概念中所述，我们会选用联合概率分布的极大似然来导出最优解： \\[ l(\\phi_y,\\phi_{j|y=1},\\phi_{j|y=0}=\\prod_{i=1}^mp(x^{(i)},y^{(i)})=\\prod_{i=1}^mp(x^{(i)}|y^{(i)})p(y^{(i)}) \\] 可以解得： \\[ \\begin{align} \\phi_{j|y=1} &amp;= \\frac{\\sum_{i=1}^m1\\lbrace x_j{(i)}=1, y^{(i)}=1 \\rbrace}{\\sum_{i=1}^m1\\lbrace y^{(i)}=1 \\rbrace} = \\frac{统计所有包含词语j的垃圾邮件的数量}{垃圾邮件的总数}\\\\\\ \\phi_{j|y=0} &amp;= \\frac{\\sum_{i=1}^m1\\lbrace x_j{(i)}=1, y^{(i)}=0 \\rbrace}{\\sum_{i=1}^m1\\lbrace y^{(i)}=0 \\rbrace} = \\frac{统计所有包含词语j的非垃圾邮件的数量}{非垃圾邮件的总数} \\\\\\ \\phi_y &amp;= \\frac{\\sum_{i=1}^m1\\lbrace y^{(i)}=1 \\rbrace}{m} = \\frac{垃圾邮件的数量}{邮件的总数} \\end{align} \\] 通过以上的公式，我们已经可以完全推得\\(p(x_1, x_2, \\ldots, x_{50000}|y)\\)。 Laplace平滑 假设，训练集中，我们重来没有碰到过\"NIPS\"这个词汇，假设我们词典中包含这个词，位置是30000，也就是说： \\[ \\begin{align} p(x_{30000}=1|y=1) &amp;= 0\\\\\\ p(x_{30000}=0|y=1) &amp;= 0 \\end{align} \\\\\\ \\Downarrow \\\\\\ p(x|y=1) =\\prod_{i=1}^{50000}p(x_i|y=1)=0 \\\\\\ p(x|y=0) =\\prod_{i=1}^{50000}p(x_i|y=0)=0 \\] 故而在分类垃圾邮件时： \\[ \\begin{align} p(y=1|x) &amp;= \\frac{p(x|y=1)p(y=1)}{p(x)} \\\\\\ &amp;=\\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)} \\\\\\ &amp;= \\frac{0}{0+0} \\end{align} \\] 所以，我们提出\\(p(x_{30000}=1|y=1) = 0\\)这样的假设不够好。 Laplace平滑就是来帮助解决这个问题的。 举例而言，在计算： \\[ \\phi_y=p(y=1)=\\frac{\\text{numof(1)}}{\\text{numof(0)}+\\text{numof(1)}} \\] 其中，\\(\\text{numof(1)}\\)表示的是，被分类为1的训练集中数据个数。 在Laplace平滑中，我们会采取如下策略: \\[ \\phi_y=p(y=1)=\\frac{\\text{numof(1)}+1}{\\text{numof(0)}+1+\\text{numof(1)}+1} \\] 比如，A球队在之前的五场比赛里面都输了，我们预测下一场比赛赢的概率： \\[ p(y=1)=\\frac{0+1}{0+1+5+1}=\\frac{1}{7} \\] 而不是简单的认为（没有Laplace平滑）是0。 推广而言，在多分类问题中，\\(y\\in\\lbrace1, \\ldots, k \\rbrace\\)，那么： \\[ p(y=j) = \\frac{\\sum_{i=1}^m1\\lbrace y^{(i)} = j \\rbrace+1}{m+k} \\] 例三：垃圾邮件分类（2） 之前的垃圾分类模型里面，我们对邮件提取的特征向量是： \\[ x=[1,0,0,\\ldots,1,\\ldots]^T \\] 这种模型，我们称之为多元伯努利事件模型（Multivariate Bernoulli Event Model）。 现在，我们换一种特征向量提取方式，将邮件的特征向量表示为： \\[ x=[x_1,x_2,\\ldots,x_j,\\ldots]^T \\] \\(x_j\\)表示词汇\\(j\\)在邮件中出现的次数。上述的特征向量也就是词频向量了。这种模型，我们称为多项式事件模型（Multinomial Event Model）。 对联合概率分布\\(p(x,y)\\)进行极大似然估计，得到如下的参数： \\[ \\begin{align} \\phi_{k|y=1} &amp;= p(x_j=k|y=1) = \\frac{C_{x=k}+1}{C_{y=1}+n} \\\\\\ \\phi_{k|y=0} &amp;=p(x_j=k|y=0) = \\frac{C_{x=k}+1}{C_{y=0}+n} \\\\\\ \\phi_{y} &amp;= p(y=1) = \\frac{C_{y=1}+1}{C_{y=1}+1+C_{y=0}+1} \\end{align} \\] 其中： \\(n\\)表示词典中词汇的数量，也就是特征向量的长度； \\[ C_{x=k}=\\sum_{i=1}^m(1\\lbrace y^{(i)}=1 \\rbrace \\sum_{j=1}^{n_i}1 \\lbrace x_j^{(i)} = k \\rbrace) \\] 表示在训练集中，所有垃圾邮件中词汇\\(k\\)出现的次数（并不是邮件的次数，而是词汇的次数）； \\[ C_{y=1}=\\sum_{i=1}^n(1\\lbrace y^{(i)} = 1 \\rbrace \\cdot n_i) \\] 表示训练集中垃圾邮件的所有词汇总长； \\[ C_{y=0}=\\sum_{i=1}^n(1\\lbrace y^{(i)} = 0 \\rbrace \\cdot n_i) \\] 表示训练集中非垃圾邮件的所有词汇总长；","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"生成学习算法","slug":"生成学习算法","permalink":"https://jiacheng-pan.github.io/wiki/tags/生成学习算法/"},{"name":"拉普拉斯平滑","slug":"拉普拉斯平滑","permalink":"https://jiacheng-pan.github.io/wiki/tags/拉普拉斯平滑/"}]},{"title":"生成学习算法的概念","slug":"吴恩达·机器学习/08-生成学习算法的概念","date":"2018-06-29T04:40:00.000Z","updated":"2019-01-21T07:28:45.415Z","comments":true,"path":"2018/06/29/吴恩达·机器学习/08-生成学习算法的概念/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/29/吴恩达·机器学习/08-生成学习算法的概念/","excerpt":"","text":"生成学习算法，英文为Generative Learning Algorithm。 我们之前看到的都是判别学习算法（Discriminative Learning Algorithm）。判别学习算法可以分成两种： 学得\\(p(y|x)\\)，比如之前的线性模型 学得一个假设\\(h_\\theta (x) = \\lbrace 0, 1 \\rbrace\\)，比如二分类问题 上面都是根据特征\\(x\\)，来对输出\\(y\\)进行建模，也许\\(y\\)是一个连续的值，也可能是离散的，比如类别。 那么，生成学习算法（Generative Learning Algorithm）刚好跟判别学习算法（Discriminative Learning Algorithm）相反，其用输出\\(y\\)对\\(x\\)进行建模，也就是：学得\\(p(x|y)\\)。 详细解释： 对于一个二分类或多分类问题，生成学习算法，在给定样本所述的类别的条件下，会对其样本特征建立一个概率模型。举例而言，在给定癌症是良性或者是恶性的条件下，生成模型会对该癌症的特征的概率分布进行建模。 根据朴素贝叶斯， \\[ p(y=1|x)=\\frac{p(x|y=1) p(y=1)}{p(x)} \\] 因为给定了\\(x\\)，所以\\(p(x)\\)可以视作1，也即 \\[ p(y=1|x)=p(x|y=1) p(y=1) \\] 高斯判别算法 Gaussian Discriminant Algorithm 对特征满足高斯分布的二分类问题进行建模。 假设\\(x \\in \\Bbb R^n\\)，且\\(p(x|y)\\)满足高斯分布，因为\\(x\\)往往是一个特征向量，故而这里的高斯分布是一个多元高斯分布（multivariate Gaussian）。 多元高斯分布，\\(z \\sim N(\\mu, \\Sigma)\\)。 \\[ p(z)=\\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}exp(-\\frac{1}{2}(z-\\mu)^T\\Sigma^{-1}(z-\\mu)) \\] \\(z\\)是一个\\(n\\)维向量，\\(\\mu\\)则是均值向量，\\(\\Sigma\\)是协方差矩阵：\\(E[(z-\\mu)(z-\\mu)^T]\\)。 对于一个二分类判别问题，正则响应函数为\\(\\phi\\)，那么 \\[ p(y)=\\phi^y(1-\\phi)^{1-y} \\] 且 \\[ p(x|y=0) \\sim N(\\mu_0, \\Sigma) \\\\\\ p(x|y=1) \\sim N(\\mu_1, \\Sigma) \\] (这里假设了，对于\\(y=0\\)和\\(y=1\\)，是两个不同均值，但协方差矩阵相同的多元高斯分布，所以这也是该模型的限制之一) 我们写出对数似然函数： \\[ \\begin{align} l(\\phi, \\mu_0, \\mu_1, \\Sigma) &amp;= log\\prod_{i=1}^mp(x^{(i)}, (y^{(i)})) \\\\\\ &amp;= log\\prod_{i=1}^mp(x^{(i)}| (y^{(i)}))p(y^{(i)}) \\end{align} \\] 对比一下之前二分类问题中的对数似然估计函数： \\[ l(\\theta) = \\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta)) \\] 这里，我们使用了联合概率（joint likelihood）: \\(p(x^{(i)}, (y^{(i)}))\\)，而在之前的判别模型里，我们用的是条件概率（conditional likelihood）: \\(P(y^{(i)}|x^{(i)};\\theta)\\)。因为，在判别模型中，特征\\(x\\)是给定的，所以用条件概率来进行极大似然估计；而在生成模型中，特征\\(x\\)不给定，所以需要用联合概率来进行极大似然估计。 然后，我们进行极大似然估计，得到： \\[ \\phi=\\frac{1}{m}\\sum_{i=1}^my^{(i)}=\\frac{1}{m}\\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace \\] 也就是分类标签为1的训练样本的比例。 再看\\(\\mu_0, \\mu_1\\)： \\[ \\mu_0=\\underbrace{\\sum_{i=1}^m 1\\lbrace y^{(i)} = 0 \\rbrace \\cdot x^{(i)}}_{(1)} /\\underbrace{ \\sum_{i=1}^m 1\\lbrace y^{(i)} = 0 \\rbrace}_{(2)} \\\\\\ \\mu_1=\\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace \\cdot x^{(i)} / \\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace \\] 式(1)表达了分类为0的样本中，特征\\(x\\)的和；式(2)表达了分类为0的样本个数；故而\\(\\mu_0\\)表达了样本中，分类为0的样本的特征的均值。同理得到\\(\\mu_1\\)。 我们再次回顾上面的高斯判别算法，事实上，高斯判别算法，假设了特征\\(x\\)满足了多元高斯分布，利用极大似然估计，对不同类别的特征\\(x\\)的分布进行了建模。 下节课我们将会探讨其他的关于生成学习算法的案例。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"生成学习算法","slug":"生成学习算法","permalink":"https://jiacheng-pan.github.io/wiki/tags/生成学习算法/"},{"name":"高斯判别算法","slug":"高斯判别算法","permalink":"https://jiacheng-pan.github.io/wiki/tags/高斯判别算法/"}]},{"title":"广义线性模型","slug":"吴恩达·机器学习/07-广义线性模型","date":"2018-06-09T08:29:00.000Z","updated":"2019-01-20T14:20:52.669Z","comments":true,"path":"2018/06/09/吴恩达·机器学习/07-广义线性模型/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/09/吴恩达·机器学习/07-广义线性模型/","excerpt":"","text":"广义线性模型，英文名为Generalized Linear Model，简称GLM。 之前，涉及到两种的两种模型： 1. 线性拟合模型，假设了\\(P(y|x;\\theta)\\)是高斯分布 2. 二分类问题，假设了\\(P(y|x;\\theta)\\)满足伯努利分布 但以上两者知识一种更广泛的，被称为『指数分布族』（The Exponential Family）的特例。 指数分布族 \\[ P(y;\\eta)=b(y)exp(\\eta^TT(y)-a(\\eta)) \\] 可以被表示为以上形式的分布，都是指数分布族的某个特定分布，给定\\(a, b, T\\)，就可以定义一个概率分布的集合，以\\(\\eta\\)为参数，就可以得到不同的概率分布。 在广义线性模型中，会假设\\(\\eta=\\theta^Tx\\)，也就是\\(\\eta\\)和特征\\(x\\)线性相关。 伯努利分布 首先，我们给出\\(y=1\\)的概率： \\[ P(y=1;\\phi)=\\phi \\] 于是： \\[ \\begin{align} P(y;\\phi) &amp;= \\phi^y(1-\\phi)^T\\\\\\ &amp;= exp(log(\\phi^T(1-\\phi^T)))\\\\\\ &amp;= exp(ylog(\\phi)+(1-y)log(1-\\phi))\\\\\\ &amp;= exp(log\\frac{\\phi}{1-\\phi} \\cdot y + log(1-\\phi)) \\end{align} \\] 比较我们上面的概率形式和指数分布族的标准形式，可以得到： \\[ \\begin{cases} \\eta &amp;= log\\frac{\\phi}{1-\\phi}, \\text{于是} \\phi=\\frac{1}{1+e^{-\\eta}}\\\\\\ a(\\eta) &amp;= -log(1-\\phi)=log(1+e^\\eta)\\\\\\ T(y) &amp;= y\\\\\\ b(y) &amp;= 1 \\end{cases} \\] 这里的\\(\\phi\\)一般会被称为正则响应函数（canonic response function）： \\[ g(\\eta) = E[y|\\eta]=\\frac{1}{1+e^{-\\eta}} \\] 相对的，正则关联函数（canonic link function）则是\\(g^{-1}\\)。 高斯分布 \\[ N(\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}(y-\\mu)^2) \\] 这里，出于简洁考虑，假设\\(\\sigma=1\\)，经过一系列化简后，可以表示成： \\[ \\frac{1}{\\sqrt{2\\pi}} \\cdot exp(-\\frac{1}{2}y^2) \\cdot exp(\\mu y-\\frac{1}{2}\\mu^2) \\] 那么， \\[ \\begin{cases} \\eta &amp;= \\mu\\\\\\ a(\\eta) &amp;= \\frac{1}{2}\\mu^2=\\frac{1}{2}\\eta^2\\\\\\ T(y) &amp;= y\\\\\\ b(y) &amp;= \\frac{1}{\\sqrt{2\\pi}} \\cdot exp(-\\frac{1}{2}y^2) \\end{cases} \\] 多项式分布 建模 在二项分布中，\\(y\\in \\lbrace 1, 2 \\rbrace\\) 而多项式分布，\\(y \\in \\lbrace 1,\\cdots, k \\rbrace\\) 一般会被用来进行邮件分类或者进行病情分类等等 我们假设 \\[ P(y=i)=\\phi_i \\] 也即，邮件属于\\(i\\)类的概率是\\(\\phi_i\\)，是关于特征\\(x\\)的一个函数。 那么，可以用\\(k\\)个参数来建模多项式分布 \\[ P(y)=\\prod_{i=1}^k\\phi_i^{1\\lbrace y=i \\rbrace} \\] 其中，\\(1 \\lbrace \\cdots \\rbrace\\)的含义为，检验\\(\\cdots\\)是否为真命题，若为真命题，则取1，否则取0。 因为所有概率和为1，所以最后一个参数 \\[ \\begin{align} \\phi_k &amp;= 1-\\sum_{j=1}^{k-1}\\phi_j \\\\\\ 1 \\lbrace y=k \\rbrace &amp;=1-\\sum_{j=1}^{k-1}1 \\lbrace y=j \\rbrace \\end{align} \\] 经过化简，也可以表示成： \\[ P(y)=exp[\\sum_{i=1}^{k-1}(log(\\frac{\\phi_i}{\\phi_k}) \\cdot 1\\lbrace y=i \\rbrace )] + log(\\phi_k) \\] 故而 \\[ \\eta = \\begin{bmatrix} log(\\frac{\\phi_1}{\\phi_k}) \\\\\\ \\vdots \\\\\\ log(\\frac{\\phi_{k-1}}{\\phi_k}) \\end{bmatrix} \\in \\Bbb R^{k-1} \\] \\[ a(\\eta) = -log(\\phi_k) \\] \\[ T(y)= \\begin{bmatrix} 1 \\lbrace y=1 \\rbrace \\\\\\ \\vdots \\\\\\ 1 \\lbrace y=k-1 \\rbrace \\end{bmatrix} \\in (0, 1)^{k-1} \\] \\[ b(y) = 1 \\] 根据\\(\\eta\\)可得： \\[ \\phi_i = e^{\\eta_i} \\cdot \\phi_k \\] 又因为： \\[ \\sum_{i=1}^{k}\\phi_i=\\sum_{i=1}^k\\phi_ke^{\\eta_i}=1 \\] 故而： \\[ \\phi_k = \\frac{1}{\\sum_{i=1}^ke^{\\eta_i}}=\\frac{1}{e^{\\eta_k}+\\sum_{i=1}^{k-1}e^{\\eta_i}} = \\frac{1}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}} \\] 所以： \\[ \\begin{align} \\phi_i &amp;= e^{\\eta_i} \\cdot \\phi_k \\\\\\ &amp;= \\frac{e^{\\eta_i}}{1 + \\sum_{j=1}^{k-1}e^{\\eta_j}} \\\\\\ &amp;= \\frac{e^{\\theta_i^Tx_i}}{1 + \\sum_{j=1}^{k-1}e^{\\theta_j^Tx_j}} \\end{align} \\] 上述函数，被称为『softmax』函数，这个函数的作用经常用于进行归一化。 经过上述步骤，假设函数可以被写成如下形式： \\[ h_\\theta(x)= \\left[ \\begin{array}{c} 1\\lbrace y=1 \\rbrace \\\\\\ \\vdots \\\\\\ 1\\lbrace y=k-1 \\rbrace \\end{array} | x;\\theta \\right]= \\begin{bmatrix} \\phi_1\\\\\\ \\vdots\\\\\\ \\phi_{k-1} \\end{bmatrix} \\] 回归 在经过上述推导，当我们有一堆训练集（\\((x^{(1)}, y^{(1)}), \\cdots, (x^{(m)}, y^{(m)})\\)）用于训练的时候，则可以进行极大似然估计： \\[ L(\\theta) = \\prod_{i=1}^mP(y^{(i)} | x^{(i)};\\theta) = \\prod_{i=1}^m\\prod_{j=1}^k\\phi_j^{1\\lbrace y^{(i)}=j \\rbrace } \\]","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"softmax","slug":"softmax","permalink":"https://jiacheng-pan.github.io/wiki/tags/softmax/"},{"name":"指数分布族","slug":"指数分布族","permalink":"https://jiacheng-pan.github.io/wiki/tags/指数分布族/"}]},{"title":"牛顿法","slug":"吴恩达·机器学习/06-牛顿法","date":"2018-06-07T12:40:00.000Z","updated":"2019-01-20T14:20:52.652Z","comments":true,"path":"2018/06/07/吴恩达·机器学习/06-牛顿法/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/07/吴恩达·机器学习/06-牛顿法/","excerpt":"","text":"牛顿法（英语：Newton's method）又称为牛顿-拉弗森方法（英语：Newton-Raphson method），它是一种在实数域和复数域上近似求解方程的方法。方法使用函数\\(\\displaystyle f(x)\\)的泰勒级数的前面几项来寻找方程\\(\\displaystyle f(y)=0\\)的根。 ——维基百科 牛顿法可以通过迭代逼近的方法，求得函数\\(f(x)=0\\)的解。 先初始化某个点\\(x_0\\)，对该点求导数\\(f&#39;(x_0)\\)，可以得到一条切线； 切线会和横轴再有一个交点\\(x_1\\)，然后再重复第一步； 直到\\(f(x_n)=0\\) 通过一系列推导，我们可以得知： \\[ x_{i+1}-x_{i}=\\frac{f(x^{(i)})}{f&#39;(x^{(i)})} \\] 于是，我们可以将牛顿法用于极大似然估计，也就是求\\(l(\\theta)\\)的最大值，可以看做是求\\(l&#39;(\\theta)=0\\)的解。 那么，每次迭代就可以写成： \\[ \\theta^{(t+1)}=\\theta^{(t)}-\\frac{l&#39;(\\theta^{(t)})}{l&#39;&#39;(\\theta^{(t)}} \\] 更一般地，可以写成： \\[ \\theta^{(t+1)}=\\theta^{(t)}-H^{-1}\\nabla_\\theta l \\] 其中，\\(H\\)是\\(l(\\theta)\\)的Hessian矩阵： \\[ H_{ij}=\\frac{\\partial^2l}{\\partial\\theta_i\\partial\\theta_j} \\] 但这个方法有个缺点，每次迭代的时候，都需要重新计算\\(H^{-1}\\)，虽然牛顿法对函数\\(f\\)有很多要求和限制，但对于logistic函数而言，足够有效。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"牛顿法","slug":"牛顿法","permalink":"https://jiacheng-pan.github.io/wiki/tags/牛顿法/"}]},{"title":"二分类问题","slug":"吴恩达·机器学习/05-二分类问题","date":"2018-06-05T03:03:00.000Z","updated":"2019-01-20T14:20:52.653Z","comments":true,"path":"2018/06/05/吴恩达·机器学习/05-二分类问题/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/05/吴恩达·机器学习/05-二分类问题/","excerpt":"","text":"在二分类问题中，输出\\(y\\in \\{0, 1\\}\\)。同样的，我们也可以用线性拟合来尝试解决二分类问题（如下图左），但数据点比较异常时，容易出现下图右这样的情况： 一般，在二分类问题中，我们会选用『logistic函数』来拟合（因为形状像S，又称为『sigmoid函数』）： \\[ h_\\theta (x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}} \\] logistic函数\\(g(z)=1/(1+e^{-z})​\\)的形状如下： 可以定义 \\[ \\begin{align} P(y=1|x;\\theta)&amp; =h_\\theta (x) \\\\\\ P(y=0|x;\\theta)&amp; =1-h_\\theta(x) \\end{align} \\] 于是： \\[ P(y|x;\\theta)=h_\\theta(x)^y(1-h_\\theta(x))^{(1-y)} \\] 进行极大似然估计： \\[ L(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^mh_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{(1-y^{(i)})} \\] 为了计算方便，定义 \\[ \\begin{align} l(\\theta)&amp;=log(L(\\theta))\\\\\\ &amp;=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))\\\\\\ &amp;=\\sum_{i=1}^m(y^{(i)}\\cdot log(h_\\theta(x^{(i)}))+(1-y^{(i)})\\cdot log(1-h_\\theta(x^{(i)}))) \\end{align} \\] 利用梯度上升进行求解： \\[ \\theta := \\theta + \\alpha \\nabla_\\theta l(\\theta) \\] 其中 \\[ \\nabla_{\\theta_j} l(\\theta)=\\frac{\\partial}{\\partial\\theta_j}l(\\theta)=\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)}\\\\\\ \\theta_j:=\\theta_j+\\alpha \\cdot \\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)} \\] 最终的梯度上升结果几乎与线性拟合中的梯度下降结果一样。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"二分类问题","slug":"二分类问题","permalink":"https://jiacheng-pan.github.io/wiki/tags/二分类问题/"}]},{"title":"线性模型的概率解释","slug":"吴恩达·机器学习/04-线性模型的概率解释","date":"2018-06-05T03:02:00.000Z","updated":"2019-01-20T14:20:01.708Z","comments":true,"path":"2018/06/05/吴恩达·机器学习/04-线性模型的概率解释/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/05/吴恩达·机器学习/04-线性模型的概率解释/","excerpt":"","text":"关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？ 我们更新一下假设函数，使之变为： \\[ y^{(i)} = \\theta^Tx^{(i)} + \\varepsilon^{(i)} \\] 其中，\\(\\varepsilon^{(i)}\\)是误差项，表示未捕获的特征（unmodeled effects），比如房子存在壁炉也影响价格，或者其他的一些随机噪音（random noise）。 一般，会假设误差项\\(\\varepsilon^{(i)} \\sim N(0, \\sigma^2)\\)（满足正态分布），也就是： \\[ P(\\varepsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\varepsilon^{(i)})^2}{2\\sigma^2}) \\] 关于为什么假设正态分布的解释： 便于数学运算； 很多独立分布的变量之间相互叠加后会趋向于正态分布（中心极限定理），在大多数情况下能成立 所以，\\(y^{(i)}\\)的后验分布： \\[ P(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}) \\sim N(\\theta^Tx^{(i)}, \\sigma^2) \\] 之后，进行极大似然估计（maximum likelihood estimation）：\\(max L(\\theta)\\)，即选择合适的\\(\\theta\\)，使得\\(y^{(i)}\\)对于\\(x^{(i)}\\)出现的概率最高（有一些存在即合理的感觉），其中\\(L(\\theta)\\)的定义如下： \\[ L(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}) \\] 那么，为了计算方便，我们定义： \\[ l(\\theta) = log(L(\\theta))=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))=m\\cdot log(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\sum_{i=1}^m\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2} \\] 于是，极大似然估计变为最小化： \\[ \\sum_{i=1}^m\\frac{(y{(i)}-\\theta^Tx{(i)})2}{2\\sigma2} \\] 也即之前线性回归所需进行最小二乘拟合的\\(J(\\theta)\\)。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"线性回归","slug":"线性回归","permalink":"https://jiacheng-pan.github.io/wiki/tags/线性回归/"}]},{"title":"过拟合&局部加权回归","slug":"吴恩达·机器学习/03-过拟合&局部加权回归","date":"2018-06-04T10:36:00.000Z","updated":"2019-01-20T14:20:52.669Z","comments":true,"path":"2018/06/04/吴恩达·机器学习/03-过拟合&局部加权回归/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/04/吴恩达·机器学习/03-过拟合&局部加权回归/","excerpt":"","text":"欠拟合和过拟合 对于之前房价的例子，假设只有一个特征size。 假如，我们只用简单的线性拟合（\\(\\theta_0+\\theta_1x_1\\)，\\(x_1\\)表示size），最终拟合结果会变一条直线，就可能产生下图最左边的结果，我们称之为『欠拟合』。 当我们尝试用二次曲线来拟合（\\(\\theta_0+\\theta_1x_1+\\theta_2x_1^2\\)，可以假设\\(x_2=x_1^2\\)，再进行线性拟合），就可能产生中间的结果。 但如果再继续增加曲线的复杂度，对于下图这种五个样本的例子，假如我们用一个五次曲线来拟合它（\\(\\theta_0+\\theta_1x1+\\theta_2x1^2+\\cdots+\\theta_5x_1^5\\)）就会精确拟合所有数据，产生右图的结果，我们称之为『过拟合』。 局部加权回归（Locally Weighted Regression） 局部加权回归，是一种特定的非参数学习方法。 什么叫非参数学习方法，首先，简单了解一下『参数化学习方法』(parametric learning algorithm)，是一种参数固定的学习方法，如上所示。而『非参数化学习方法』（non-parametric learning algorithm）则不固定参数，参数的个数会随着训练集数量而增长。 我们回顾一下，线性拟合中，我们的目标是找到合适的参数\\(\\theta\\)，使得最小化\\(\\sum_i(Y^{(i)} - \\theta^TX^{(i)})^2\\)。 而『局部线性拟合』，则是在某个局部区域A进行线性拟合，目标是最小化\\(\\sum_iw^{(i)}(Y^{(i)} - \\theta^TX^{(i)})^2\\)，其中权重\\[w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2})\\]，当然，权重公式是可替换的。 我们观察一下\\(w^{(i)}\\)的形状，当数据\\(x^{(i)}\\)靠近\\(x\\)时，其权重就会较大，那么对目标函数的贡献就会大一些；而数据远离\\(x\\)的时候，权重就会较小，贡献就会较小。这样做，目标函数就会更关注\\(x\\)附近的数据点，从而达到局部的目的。 当然，可以调整权重函数，常用的另一个权重函数：\\[w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2 \\tau^2 })\\]（波长函数），\\(\\tau\\)越大，波形越平缓，局部性越差。 但问题在于，当训练数据较大时，该方法的代价会很高。每要预测一个值，就需要重新进行一次局部线性拟合。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"局部加权回归","slug":"局部加权回归","permalink":"https://jiacheng-pan.github.io/wiki/tags/局部加权回归/"}]},{"title":"线性回归","slug":"吴恩达·机器学习/02-线性回归","date":"2018-06-03T12:04:00.000Z","updated":"2019-01-20T14:20:52.652Z","comments":true,"path":"2018/06/03/吴恩达·机器学习/02-线性回归/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/03/吴恩达·机器学习/02-线性回归/","excerpt":"","text":"首先引入一些后面会用到的定理： 定义1：定义函数\\(f: \\Bbb R^{m \\times n} \\mapsto \\Bbb R\\)，\\(A \\in \\Bbb R^{m \\times n}\\)，定义 \\[ \\nabla_Af(A)= \\begin{bmatrix} \\frac{\\partial f}{\\partial A_{11}} &amp; \\cdots &amp; \\frac{\\partial f}{\\partial A_{1n}}\\\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\\\ \\frac{\\partial f}{\\partial A_{m1}} &amp; \\cdots &amp; \\frac{\\partial f}{\\partial A_{mn}} \\end{bmatrix} \\] 定义2：矩阵的迹（Trace）：如果\\(A \\in R^{n\\times n}\\)方阵，那么\\(A\\)的迹，是\\(A\\)对角线元素之和 \\[ tr A = \\sum_{i=1}^nA_{ii} \\] 定理1：\\(tr AB = tr BA\\) 定理2：\\(tr ABC = tr CAB = tr BCA\\) 定理3：\\(f(A)=tr AB \\Rightarrow \\nabla_Af(A)=B^T\\) 定理4：\\(trA = tr A^T\\) 定理5：\\(a \\in R \\Rightarrow tr a=a\\) 定理6：\\(\\nabla_AtrABA^TC=CAB+C^TAB^T\\) 线性回归 一些符号的改写 上一篇博客提到，梯度下降的每一步，对某个参数\\(\\theta_i\\)，执行： \\[ \\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta) \\] 那么，\\(h_\\theta(x)\\)的所有参数\\(\\theta\\)可以表示成一列向量： \\[ \\theta = \\left[ \\begin{array}{c} \\theta_0\\\\\\ \\theta_1\\\\\\ \\vdots\\\\\\ \\theta_n \\end{array} \\right] \\in R^{n+1} \\] 我们可以定义： \\[ \\nabla_\\theta J = \\left[ \\begin{array}{c} \\frac{\\partial}{\\partial \\theta_0}J\\\\\\ \\frac{\\partial}{\\partial \\theta_1}J\\\\\\ \\vdots\\\\\\ \\frac{\\partial}{\\partial \\theta_n}J \\end{array} \\right] \\in R^{n+1} \\] 梯度下降过程可以表示成： \\[ \\theta:=\\theta - \\alpha\\nabla_\\theta J \\] 其中，\\(\\theta\\)和\\(\\nabla_\\theta J\\)都说是n+1维向量。 对于训练集中所有的输入\\({x^{(1)}},x^{(2)},…,x^{(m)}\\)，其中 \\[ x^{(i)} = \\left[ \\begin{array}{c} 1\\\\\\ x_1^{(i)}\\\\\\ \\vdots\\\\\\ x_n^{(i)}\\\\\\ \\end{array} \\right] \\in R^{n+1} \\] \\(h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\\)，可以表示成向量： \\[ \\left[ \\begin{array}{c} h_\\theta(x^{(1)})\\\\\\ h_\\theta(x^{(2)})\\\\\\ \\vdots\\\\\\ h_\\theta(x^{(m)})\\\\\\ \\end{array} \\right] = \\left[ \\begin{array}{c} (x^{(1)})^T\\theta\\\\\\ (x^{(2)})^T\\theta\\\\\\ \\vdots\\\\\\ (x^{(m)})^T\\theta\\\\\\ \\end{array} \\right] = \\left[ \\begin{array}{c} (x^{(1)})^T\\\\\\ (x^{(2)})^T\\\\\\ \\vdots\\\\\\ (x^{(m)})^T \\end{array} \\right] \\cdot \\theta = X \\cdot \\theta \\] 而 \\[ Y = \\left[ \\begin{array}{c} y^{(1)}\\\\\\ y^{(2)}\\\\\\ \\vdots\\\\\\ y^{(m)} \\end{array} \\right] \\] 于是， \\[ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h(x^{(i)} - y^{(i)})^2)=\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y) \\] 推导过程 关于梯度下降法，可以直接简化为求梯度为0的位置，即求\\(\\nabla_\\theta J(\\theta) = \\vec{0}\\) 首先，简化： \\[ \\begin{align} \\nabla_\\theta J(\\theta) &amp; = \\nabla_\\theta\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y)\\\\\\ &amp; =\\frac{1}{2}\\nabla_\\theta tr(\\theta^TX^TX\\theta - \\theta^TX^TY - Y^TX\\theta + Y^TY)\\\\\\ &amp; =\\frac{1}{2}[\\nabla_\\theta tr(\\theta\\theta^TX^TX) - \\nabla_\\theta tr(Y^TX\\theta) - \\nabla tr(Y^TX\\theta)] \\end{align} \\] 其中，第一项： \\[ \\begin{align} \\nabla_\\theta tr(\\theta\\theta^TX^TX) &amp; = \\nabla_\\theta tr(\\theta I \\theta^TX^TX) &amp;\\text{定理6, set: $\\theta =^{set} A, I = B, X^TX=C$}\\\\\\ &amp; = X^TX\\theta I + X^TX\\theta I &amp; \\text{$CAB+C^TAB^T$}\\\\\\ &amp; = X^TX\\theta + X^TX\\theta \\end{align} \\] 第二项和第三项： \\[ \\nabla_\\theta tr(Y^TX\\theta) = X^TY\\\\\\ (定理3，set:Y^TX = B, \\theta = A) \\] 所以： \\[ \\nabla_\\theta J(\\theta) = X^TX\\theta - X^TY = 0\\\\\\ \\Rightarrow X^TX\\theta = X^TY\\\\\\ \\] 最后解得： \\[ \\theta = (X^TX)^{(-1)}X^TY \\] 当然，以上的解是有限制的，只有当\\(X^TX\\)满秩时，才能够求逆。 如果非满秩，说明方程数量不够，也就是当需要n个参数时，却不够n个输入样本。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"线性回归","slug":"线性回归","permalink":"https://jiacheng-pan.github.io/wiki/tags/线性回归/"}]},{"title":"监督学习&梯度下降法","slug":"吴恩达·机器学习/01-监督学习&梯度下降法","date":"2018-06-03T06:29:00.000Z","updated":"2019-01-20T14:19:47.141Z","comments":true,"path":"2018/06/03/吴恩达·机器学习/01-监督学习&梯度下降法/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/03/吴恩达·机器学习/01-监督学习&梯度下降法/","excerpt":"","text":"监督学习 符号定义： 符号 意义 \\(m\\) 训练集包含的数据个数 \\(x\\) 输入变量/特征（\u0004feature） \\(y\\) 输出变量/目标（target） \\((x, y)\\) 一个训连样本 \\((x^{(i)}, y^{(i)})\\) 第i个训练样本 监督学习的主要流程： 线性回归 以预测房价为例，我们的目标是导出一个函数（即假设），根据房子的特征（比如大小、卧室数量等等）来预测房价，那么： - 输入（特征）：\\(x_1, x_2, …\\)（比如大小、卧室数量等等） - 输出（目标）：\\(y\\)（房价） - 假设：\\(h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\\)，用于预测房价，其中\\(\\theta_i\\)是参数，\\(n\\)是特征数量 为了方便，可以将假设写成：\\(h(x)=\\sum_{i=0}^n\\theta_ix_i=\\theta^Tx​\\) 此时，学习函数（Learning Algorithm）的目标就是找到合适的参数\\(\\theta\\)，使之能够导出『合理』的假设\\(h(x)\\)，这里我们将『合理』理解为：\\(h_\\theta(x)\\)（假设）和\\(y\\)（目标）之间的差距最小，也即： \\[ \\displaystyle \\min_{\\theta}\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2 \\] 这里的\\(\\frac{1}{2}\\)是为了简化之后的计算。 我们定义\\[\\displaystyle J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2\\]，那么我们的目标就是去选取合适的\\(\\theta\\)，以最小化\\(J(\\theta)\\)。 梯度下降法 搜索算法（梯度下降） 目的：不断改变\\(\\theta\\)，从而来减少\\(J(\\theta)\\)。 原理：每次都往下降最快的地方走，从而找到一个局部最优解。 一般会初始化\\(\\vec{\\theta}=\\vec{0}\\)，然后每次都沿着梯度方向走，以保证每次都往下降最快的地方走： \\[ \\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta) \\] 其中，\\(:=\\)表示赋值操作，\\(\\alpha\\)为步长。 对于某个训练样本\\((x, y)​\\) \\[ \\displaystyle \\frac{\\partial}{\\partial \\theta_i}J(\\theta) = \\frac{\\partial}{\\partial \\theta_i}(\\frac{1}{2}(h_\\theta(x)-y)^2) \\] \\[ \\displaystyle = 2 \\times \\frac{1}{2}(h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(h_\\theta(x)-y) \\] \\[ \\displaystyle = (h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(\\theta_0x_0+…+\\theta_nx_n-y) \\] \\[ \\displaystyle =(h_\\theta(x)-y) \\times x_i \\] 那么， \\[ \\theta_i:=\\theta_i - \\alpha (h_\\theta (x) - y) \\times x_i \\] 批量梯度下降法（Batch Gradient Descent） 批量梯度下降法，使用的是所有训练样本的平均梯度： \\[ \\displaystyle \\theta_i:=\\theta_i - \\alpha \\frac{1}{m} \\sum_{j=1}^m(h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)} \\] 但每次下降都需要遍历所有样本，效率较低，具体过程可能如下： 随机梯度下降法（Stochastic Gradient Descent） 又称为『增量梯度下降法』 对每个样本\\((x_{(j)}, y_{(j)})\\)进行： \\[ \\displaystyle \\theta_i:=\\theta_i - \\alpha (h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)} \\] 直到收敛 这时，每次梯度下降只遍历一个样本，具体过程可能如下：","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"梯度下降","slug":"梯度下降","permalink":"https://jiacheng-pan.github.io/wiki/tags/梯度下降/"},{"name":"监督学习","slug":"监督学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/监督学习/"}]}]}