{"meta":{"title":"Jiacheng Pan's Wiki","subtitle":null,"description":null,"author":"Jiacheng Pan","url":"https://jiacheng-pan.github.io/wiki"},"pages":[{"title":"Jiacheng Pan","date":"2019-01-21T07:17:28.804Z","updated":"2019-01-21T07:17:28.804Z","comments":true,"path":"about/index.html","permalink":"https://jiacheng-pan.github.io/wiki/about/index.html","excerpt":"","text":"I'm a post graduate student in Zhejiang University, interested in visualization and visual analytics. Find my blog in Jackie Anxis. Say Hi to me through my github: JackieAnxis' GitHub or mail to me: jackieanxis@gmail.com"},{"title":"Categories","date":"2019-01-17T01:48:32.936Z","updated":"2019-01-17T01:48:32.936Z","comments":true,"path":"categories/index.html","permalink":"https://jiacheng-pan.github.io/wiki/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-01-17T01:48:32.937Z","updated":"2019-01-17T01:48:32.937Z","comments":true,"path":"tags/index.html","permalink":"https://jiacheng-pan.github.io/wiki/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"模型选择和特征选择","slug":"吴恩达·机器学习/16-模型选择和特征选择","date":"2019-01-18T05:53:00.000Z","updated":"2019-01-20T14:21:01.748Z","comments":true,"path":"2019/01/18/吴恩达·机器学习/16-模型选择和特征选择/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/18/吴恩达·机器学习/16-模型选择和特征选择/","excerpt":"","text":"首先来说几种验证训练结果的方法。 保留交叉验证法 hold out cross validation 给出一个训练集，随机分成两个，一部分当做训练子集（一般占70%）用于训练，另一部分当做保留交叉验证子集（30%）用于测试。 k重交叉验证法 k-fold cross validation 将整个数据集分成k部分，拿出k-1部分进行训练，将剩下一个用于测试。重复上述过程k次（每次都不一样），求出均值即为验证结果。一般k取10。 优点：增加了训练数据；缺点：计算代价高； 留一交叉验证法 leave one out cross validation 当k=m（训练数据量）时，称之为留一交叉验证法。一般用于训练数据过少的情况。 特征选择 在进行学习时，过多的特征容易带来过拟合，需要选出一个特征子集，其中的特征与学习算法最相关。 前向搜索法 forward search 过程如下： 初始化特征集\\(\\cal F\\) 对于不属于\\(\\cal F\\)的每一个特征，计算添加该特征后模型精度的提升； 选择提升最大的特征加入\\(\\cal F\\) 重复2和3，直到精度不在上升。 当然可以设置阈值k，当\\(\\cal F\\)包含了k个特征后即停止。 后向搜索法 backward search 基本步骤与上述类似，只是过程相反：首先从满的特征集开始，之后每次删除表现最差的特征。 上述的方法都被称为\"wrapping\" feature selection（“封装”特征选择），\"封装\"这个词，意味着： 当你进行选择时（前向搜索或者后向搜索）你需要重复使用学习算法去训练模型，根据结果来选择特征子集。 这种方法主要缺点是计算量大，但是它是一种比较准确的选择方法。 另外有一种算法，可能它的泛化误差不会太低，但是计算代价较小。 特征过滤 filter method 主要方法是，计算每一个特征的一些度量，来衡量对y（label）的影响有多大，一般使用互信息（Mutual Information）来度量： \\[ \\begin{align} MI(x_i, y) &amp;= \\sum_{x_i \\in \\lbrace 0,1 \\rbrace} \\sum_{y \\in \\lbrace 0,1 \\rbrace} p(x_i, y) \\log \\frac{p(x_i, y)}{p(x_i)p(y)} \\\\\\ &amp;= KL(p(x, y) || p(x)p(y)) \\text{, KL divergence} \\end{align} \\] 然后去选择最好的k个特征。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"特征选择","slug":"特征选择","permalink":"https://jiacheng-pan.github.io/wiki/tags/特征选择/"},{"name":"模型选择","slug":"模型选择","permalink":"https://jiacheng-pan.github.io/wiki/tags/模型选择/"}]},{"title":"Vapnik–Chervonenkis dimension","slug":"吴恩达·机器学习/15-Vapnik-Chervonenkis Dimension","date":"2019-01-10T05:53:00.000Z","updated":"2019-01-20T14:21:01.717Z","comments":true,"path":"2019/01/10/吴恩达·机器学习/15-Vapnik-Chervonenkis Dimension/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/10/吴恩达·机器学习/15-Vapnik-Chervonenkis Dimension/","excerpt":"","text":"延续上节课的内容。 给定\\(|\\cal H| = k\\)，给定\\(\\delta, \\gamma\\)，为了保证： \\[ \\varepsilon({\\hat h}) \\leq \\varepsilon(h) + 2\\gamma \\] 的概率不小于\\(1-\\delta\\)，那么\\(m\\)需要满足： \\[ m \\geq \\frac{1}{2\\gamma^2}\\log \\frac{2k}{\\delta} = O(\\frac{1}{\\gamma^2}\\log \\frac{k}{\\delta}) \\] 假如\\(\\cal H\\)是以\\(d\\)个实数为参数的(比如为了解决n个特征的分类问题，d就等于n+1)，而在计算机中，实数多以64位浮点数保存，d个实数就需要64d位来存储，那么\\(\\cal H\\)的整个假设空间大小就为\\(2^{64d}\\)，也即\\(k=2^{64d}\\)，那么： \\[ m \\geq O(\\frac{1}{\\gamma^2}\\log \\frac{k}{\\delta}) = O(\\frac{d}{\\gamma^2}\\log \\frac{1}{\\delta}) \\] 最直观的解释就是\\(m\\)与假设类的参数数量几乎是成正比的。 定义Shatter（分散）：给定一个由\\(d\\)个点构成的集合：\\(S=\\lbrace x^{(1)}, \\ldots, x^{(d)} \\rbrace\\)，我们说一个假设类\\(\\cal H\\)能够分散(shatter)一个集合\\(S\\)，如果\\(\\cal H\\)能够实现对\\(S\\)的任意一种标记方式，也即，对\\(S\\)的任意一种标记方式，我们都可以从\\(\\cal H\\)中找到对应的假设来进行分割。 举例而言，如果\\({\\cal H} = \\lbrace \\text{linear classification in 2D} \\rbrace\\)(二维线性分类器的集合)，对于二维平面上的三个点，有8种标记方式： image-20190111094049430 那么，蓝线所代表的线性分类器，都能完成对它们的标记，所以我们称\\(\\cal H\\)能够分散平面上三个点所构成的集合。但是对于平面上四个点，就有存在以下这种情况，没有任何的线性分类器能够实现这种标记： image-20190111094649618 定义Vapnik-Chervonenkis dimension（VC维）：假设集\\(\\cal H\\)的VC维，写成\\(VC({\\cal H})\\)，指的是能够被\\(\\cal H\\)分散的最大集合的大小。 举例而言，如果\\(\\cal H\\)是所有二维线性分类器构成的集合，那么\\(VC(\\cal H) = 3\\)。当然并不是说\\(\\cal H\\)要能分散所有三个点构成的集合，只要有某个三个点构成的集合能被\\(\\cal H\\)分散即可，比如下面这种标记方式，\\(\\cal H\\)就无法实现，但是我们还是称\\(VC(\\cal H) = 3\\)。 image-20190111095306079 有一个推论： \\(VC({\\text{linear classification of n D}}) = n + 1\\) 定理：给定假设集合\\(\\cal H\\)，令\\(VC({\\cal H})=d\\)，那么，对于任意的\\(h \\in {\\cal H}\\)： \\[ |\\varepsilon(h)-{\\hat \\varepsilon}(h)| \\leq O(\\sqrt{\\frac{d}{m} \\log \\frac{m}{d} + \\frac{1}{m} \\log \\frac{1}{\\delta}}) \\] 的概率不小于\\(1 - \\delta\\)，以及 \\[ \\varepsilon({\\hat h}) \\leq \\varepsilon(h^\\ast) + 2 \\gamma, \\gamma = O(\\sqrt{\\frac{d}{m} \\log \\frac{m}{d} + \\frac{1}{m} \\log \\frac{1}{\\delta}}) \\] 的概率不小于\\(1-\\delta​\\)。 引理：为了保证\\(\\varepsilon({\\hat h}) \\leq \\varepsilon(h ^ \\ast) + 2 \\gamma\\)至少在\\(1 - \\delta\\)的概率下成立，应该满足： \\[ m = O_{\\gamma, \\delta}(d) \\] \\(O_{\\gamma, \\delta}(d)\\)指的是，在固定\\(\\gamma, \\delta\\)的情况下，与\\(d\\)线性相关。 也即，\\(m\\)必须与\\(\\cal H\\)的VC维保持一致，也可以这么理解，为了使泛化误差和训练误差近似，训练样本数目必须和模型的参数数量成正比。 在SVM中，给定数据集，如果我们只考虑半径R以内的点，以及间隔至少为\\(\\gamma\\)的线性分类器构成的假设类，那么： \\[ VC({\\cal H}) \\leq \\lceil \\frac{R^2}{4\\gamma^2} \\rceil + 1 \\] 也就说明，\\(\\cal H​\\) 的VC维上限，并不依赖于数据集中点\\(x​\\)的维度，换句话说，虽然点可能位于无限维的空间中，但是如果只考虑那些具有较大函数间隔的分类器所组成的假设类，那么VC维就存在上界。 所以SVM会自动尝试找到一个具有较小VC维的假设类，所以它不会过拟合（模型参数不会过大）","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"VC维","slug":"VC维","permalink":"https://jiacheng-pan.github.io/wiki/tags/VC维/"},{"name":"特征选择","slug":"特征选择","permalink":"https://jiacheng-pan.github.io/wiki/tags/特征选择/"}]},{"title":"经验风险最小化","slug":"吴恩达·机器学习/14-经验风险最小化","date":"2019-01-05T05:53:00.000Z","updated":"2019-01-20T14:21:01.717Z","comments":true,"path":"2019/01/05/吴恩达·机器学习/14-经验风险最小化/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2019/01/05/吴恩达·机器学习/14-经验风险最小化/","excerpt":"","text":"就线性分类模型而言，可以将其表示为： \\[ h_\\theta(x)=g(\\theta^Tx), \\\\\\ g(z) = 1\\lbrace z \\geq 0 \\rbrace \\] 其中，训练集表示为： \\[ S=\\lbrace (x^{(i)}, y^{(i)}) \\rbrace _ {i = 1} ^ m, (x^{(i)}, y^{(i)}) \\sim {\\cal D} \\] 这里假设了训练数据都是独立同分布的。 那么，我们认为，这个线性分类器的训练误差就可以表示为它分类错误的样本比例： \\[ {\\hat{\\varepsilon}}(h_\\theta) = {\\hat{\\varepsilon}}_s(h_\\theta) = \\frac{1}{m}\\sum_{i=1}^m1\\lbrace h_\\theta (x^{(i)}) \\neq y^{(i)} \\rbrace \\] 在这里，我们把训练误差也称为风险（risk），由此我们导出了经验风险最小化。 经验风险最小化 Empirical Risk Minimization，ERM 经验风险最小化，最终导出一组参数，能够使得训练误差最小： \\[ {\\hat{\\theta}} = \\arg \\min {\\hat{\\varepsilon}}_s(h_\\theta) \\] 我们再定义一个假设类\\({\\cal{H}} = \\lbrace h_\\theta, \\theta \\in {\\Bbb R}^{n+1} \\rbrace\\)，它是所有假设的集合。在线性分类中，也就是所有线性分类器的集合。 那么，我们可以重新定义一次ERM： \\[ {\\hat h} = \\mathop{\\arg \\min}_{h \\in {\\cal H}} {\\hat \\varepsilon}(h) \\] 对上述公式的直观理解就是：从假设类中选取一个假设，使得训练误差最小。我们这里用了\\(\\hat{h}\\)表示估计，因为毕竟不可能得到最好的假设，只能得到对这个最好的假设的估计。 但这仍然不是目标，我们的目标是使得泛化误差 Generalization Error最小化，也即新的数据集上分类错误的概率： \\[ \\varepsilon(h)=P_{(x,y) \\sim {\\cal D}}(h(x) \\neq y) \\] 接下去，为了证明： （1）\\({\\hat \\varepsilon} \\approx \\varepsilon\\)，训练误差近似于泛化误差（理解为，泛化误差和训练误差之间的差异存在上界） （2）ERM输出的泛化误差\\(\\varepsilon({\\hat h})\\)存在上界； 我们引出两个引理： 联合界引理（Union Bound） \\(A_1, A_2, \\ldots , A_k\\)是k个事件，他们之间并不一定是独立分布的，有： \\[ P(A_1 \\cup \\ldots \\cup A_k) \\leq P(A_1) + \\dots + P(A_k) \\] Hoeffding不等式（Hoeffding Inequality） \\(z_1, \\ldots z_m\\)是m个iid（independent and identically distribution，独立同分布），他们都服从伯努利分布，\\(P(z_i=1) = \\phi\\)，那么对\\(\\phi\\)的估计： \\[ {\\hat \\phi} = \\frac{1}{m}\\sum_{i=1}^m z_i \\] 于是，给定\\(\\gamma &gt; 0\\)，有： \\[ P(|{\\hat{\\phi}} - \\phi| &gt; \\gamma) \\leq 2 exp(-2\\gamma^2m) \\] Hoeffding不等式的直观解释就是，下图中的阴影面积，会有上界。 image-20190106143941030 一致收敛 Uniform Conversions 对于某个\\(h_j \\in \\cal{H}\\)，我们定义\\(z_i = 1 \\lbrace h_j(x^{(i)}) \\neq y^{(i)}\\rbrace \\in \\lbrace{}\\)为第i个样本被分类错误的指示函数的值，对于logistic而言，它服从伯努利分布。 那么： 泛化误差：\\(P(z_i=1) = \\varepsilon(h_j)\\) 训练误差：\\({\\hat{\\varepsilon}}(h_j) = \\frac{1}{m}\\sum_{i=1}^m z_i\\) 根据Hoeffding不等式，我们能够得到： \\[ P(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma) \\leq 2e^{-2\\gamma^2m} \\] 接着，我们定义训练误差和泛化误差之间的差大于\\(\\gamma\\)（\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma\\)）为事件\\(A_j\\)，根据以上结论，我们可知： \\[ P(A_j) \\leq 2e^{-2\\gamma^2m} \\] 那么根据联合界引理： \\[ \\begin{array}{l} &amp; P(\\exists h_j \\in H, |{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma) \\\\\\ = &amp; P(A_1 \\cup A_2 \\cup \\ldots \\cup A_k) \\\\\\ \\leq &amp; \\sum_{i=1}^k P(A_i) \\\\\\ \\leq &amp; \\sum_{i=1}^k 2e^{-2\\gamma^2m} \\\\\\ = &amp; 2ke^{-2\\gamma^2m} \\end{array} \\] 可以表述为：存在\\(h_j\\)使\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma\\)的概率\\(\\leq 2ke^{-2\\gamma^2m}\\)。 等价于：不存在\\(h_j\\)使\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| &gt; \\gamma\\)的概率\\(\\geq 1 - 2ke^{-2\\gamma^2m}\\)。 等价于：\\(\\cal H\\)中任意的\\(h_j\\)使得\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| \\leq \\gamma\\)的概率\\(\\geq 1 - 2ke^{-2\\gamma^2m}\\)。 我们将上面这个结论称之为一致收敛 Uniform Conversions，也就是说事实上，所有的假设，训练误差和泛化误差之间都存在上界。 样本复杂度，误差界以及偏差方差权衡 上面的结论，我们可以引出以下的一些推论： 样本复杂度 Sample Complexity 给定\\(\\gamma, \\delta\\)，需要多大的训练集合（\\(m\\)）？其中\\(\\delta\\)指的是泛化误差和训练误差之差大于\\(\\gamma\\)的概率。 我们知道，\\(\\delta \\leq 2ke^{-2\\gamma^2m}\\)，可求解： \\[ m \\geq \\frac{1}{2 \\gamma ^ 2} log(\\frac{2k}{\\delta}) \\] 这个，也被称为样本复杂度（类似于时间复杂度），指的是，只要满足上面这个条件，任意\\(h \\in \\cal H\\)，都能得到\\(|{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| \\leq \\gamma\\) 误差界 Error Bound 给定\\(\\delta, m\\)时，我们会得到多大的误差上界\\(\\gamma\\)。 经过求解可以得到： \\[ P(\\forall h \\in {\\cal H}, |{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| \\leq \\sqrt{\\frac{1}{2m}log(\\frac{2k}{\\delta})}) \\geq 1 - \\delta \\] 也就是误差上界是：\\(\\gamma = \\sqrt{\\frac{1}{2m}log(\\frac{2k}{\\delta})}\\)。 偏差方差权衡 Bias Variance Tradeoff 我们定义： \\[ \\begin{align} {\\hat h} &amp;= \\mathop{\\arg \\min}_{h \\in \\cal H} {\\hat \\varepsilon}(h) \\text{, 使得训练误差最小的h} &amp;\\tag{1} \\\\\\ h^\\ast &amp;= \\mathop{\\arg \\min}_{h \\in \\cal H} \\varepsilon(h) \\text{, 使得泛化误差最小的h} \\tag{2} \\end{align} \\] 假如： \\[ \\forall h \\in {\\cal H}, |{\\hat{\\varepsilon}}(h_j) - \\varepsilon(h_j)| \\leq \\gamma \\tag{3} \\] 那么： \\[ \\begin{align} \\varepsilon(\\hat h) &amp;\\leq {\\hat \\varepsilon}({\\hat h}) + \\gamma, &amp;\\text{derived from (3)}\\\\\\ &amp;\\leq {\\hat \\varepsilon}(h^\\ast) + \\gamma, &amp;\\text{derived from (1)}\\\\\\ &amp;\\leq {\\varepsilon(h^\\ast)} + \\gamma + \\gamma, &amp;\\text{ derived from (3)} \\end{align} \\] 于是，我们得到如下定理： 给定大小为\\(k\\)的假设集合\\(\\cal H\\)，给定\\(m, \\delta\\)，那么： \\[ \\varepsilon(\\hat h) \\leq \\underbrace{(\\min_{h \\in {\\cal H}}\\varepsilon(h))}_{\\varepsilon(h^\\ast)} + 2 \\underbrace{\\sqrt{\\frac{1}{2m}log(\\frac{2k}{\\delta})}}_{\\gamma} \\] 的概率不低于\\(1-\\delta\\)。 可以想象，为了得到最佳的假设\\(h^\\ast\\)，我们尽可能增大\\(\\cal H\\)（能够减小\\(\\varepsilon(h^\\ast)\\)），但随之而来的就是\\(\\gamma\\)的增大，所以需要在这两者之间进行权衡，我们指的就是偏差方差权衡 Bias Variance Tradeoff。 image-20190106154201189 由此，我们得到一个推论： 给定\\(\\delta, \\gamma\\)，为了能够保证\\(\\varepsilon(\\hat h) \\leq (\\min_{h \\in {\\cal H}}\\varepsilon(h)) + 2\\gamma\\)的概率不小于\\(1-\\delta\\)（ERM得到的假设的一般误差，与最佳假设的一般误差之间，差值不大于\\(2\\gamma\\)） 我们需要保证： \\[ m \\geq \\frac{1}{2\\gamma^2}log(\\frac{2k}{\\delta}) \\]","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"ERM","slug":"ERM","permalink":"https://jiacheng-pan.github.io/wiki/tags/ERM/"},{"name":"经验风险","slug":"经验风险","permalink":"https://jiacheng-pan.github.io/wiki/tags/经验风险/"}]},{"title":"SVM（四）非线性决策边界","slug":"吴恩达·机器学习/13-SVM（四）非线性决策边界","date":"2018-07-25T05:53:00.000Z","updated":"2019-01-20T14:21:01.716Z","comments":true,"path":"2018/07/25/吴恩达·机器学习/13-SVM（四）非线性决策边界/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/07/25/吴恩达·机器学习/13-SVM（四）非线性决策边界/","excerpt":"","text":"当数据中存在异常点时，比如上述的情况，导致原先可以用直线a分割的数据现在不得不用b来进行，以保证完美的分割。由此我们引出了非线性决策边界（non-linear decision boundaries）来解决这样的问题。 观察原SVM问题的目标： \\[ \\min_{w, b} \\frac{1}{2}||w|^2 \\\\\\ \\text{ s.t. }y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq 1, i=1,\\ldots,m \\] 我们为原公式增加惩罚项，对不同的数据点增加不同的惩罚，使得所有样本能够更好地分割： \\[ \\min_{w, b} \\frac{1}{2}||w|^2+c\\sum_{i=1}^m\\xi_i, \\xi_i\\geq0 \\] 使得 \\[ y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq 1-\\xi_i, i=1,\\ldots,m \\] 注意到，我们之前认为$ y^{(i)} (w^T x^{(i)}+b) 1 $是分类正确的，在这里我们允许一部分样本小于1，也就是说明我们允许了一部分样本分类错误。 构建拉格朗日算子： \\[ {\\cal L}(w, b, \\xi, \\alpha, \\gamma) = \\frac{1}{2}||w|^2+c\\sum_i\\xi_i-\\sum_i^m\\alpha_i(y^{(i)} (w^T \\cdot x^{(i)}+b)-1+\\xi_i)-\\sum_i^m\\gamma_i\\xi_i \\] 对偶： \\[ \\max W(\\alpha) = \\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_jy_iy_j\\langle x_i \\cdot x_j \\rangle \\] 跟原先的SVM问题的唯一区别在于其限制条件为： \\[ \\sum_{i=1}^my^{(i)}\\alpha_i=0 \\\\\\ 0 \\leq \\alpha_i \\leq c \\] 其收敛条件： 对于大部分数据点： \\[ \\alpha_i=0 \\Rightarrow y^{(i)} (w^T \\cdot x^{(i)}+b) \\geq 1 \\] 对于异常点： \\[ \\alpha_i = c \\] 对于最近点： \\[ 0&lt;\\alpha_i&lt;c \\] 坐标上升法（Coordinate Ascent） 考虑优化问题： \\[ \\max W(\\alpha_1, \\alpha_2, ..., \\alpha_m) \\] 不考虑约束条件， 重复 { ​ For i = 1 to m: \\[ \\alpha_i := \\arg \\max_{\\hat{\\alpha}_i} W(\\alpha_1,\\ldots,{\\hat{\\alpha}}_i,\\ldots,\\alpha_m) \\] } 直到收敛； 这个算法，可以认为是执行了以下这个过程（以m=2为例）： 坐标上升法，不断沿着坐标轴方向前进 顺序最小优化算法（Sequential minimal optimization, SMO） 顺序最小优化算法的基本理念就是在坐标上升法的基础上，改成一次性优化其中两个\\(\\alpha\\)，而固定其他的\\(m-2\\)个\\(\\alpha\\)。 假如我们更新\\(\\alpha_1, \\alpha_2\\)： 因为在之前我们提到: \\[ \\sum_i^m \\alpha_iy^{(i)}=0 \\] 于是有： \\[ \\alpha_1 y^{(1)}+\\alpha_2 y^{(2)} = -\\sum_{i=3}^m\\alpha_iy^{(i)}= \\zeta \\] 那么 \\[ \\alpha_1=\\frac{\\zeta-\\alpha_2y^{(2)}}{y^{(1)}} \\] \\[ W(\\alpha_1, \\alpha_2, ..., \\alpha_m) = W(\\frac{\\zeta-\\alpha_2y^{(2)}}{y^{(1)}}, \\alpha_2, ..., \\alpha_m) \\] 在非线性决策边界优化问题中，其实\\(W\\)是一个关于\\(\\alpha_i\\)的二次函数，固定其他\\(\\alpha\\)之后，\\(W\\)函数就可以被简化为： \\[ a\\alpha_2^2+b \\alpha_2 + c \\] 很容易就能求解。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://jiacheng-pan.github.io/wiki/tags/SVM/"},{"name":"非线性决策边界","slug":"非线性决策边界","permalink":"https://jiacheng-pan.github.io/wiki/tags/非线性决策边界/"},{"name":"坐标上升法","slug":"坐标上升法","permalink":"https://jiacheng-pan.github.io/wiki/tags/坐标上升法/"}]},{"title":"SVM（三）核函数","slug":"吴恩达·机器学习/12-SVM（三）核函数","date":"2018-07-22T13:33:00.000Z","updated":"2019-01-21T07:29:17.147Z","comments":true,"path":"2018/07/22/吴恩达·机器学习/12-SVM（三）核函数/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/07/22/吴恩达·机器学习/12-SVM（三）核函数/","excerpt":"","text":"在SVM(二)中，我们看到了如下的表示形式： \\[ W(\\alpha)=\\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) \\] 这里，内积\\((x_i \\cdot x_j)\\)就是最简单的核函数的形式。一般核函数会被写成\\(\\langle x^{(i)}, x^{(j)} \\rangle\\)的形式。 有时候，我们会将一些特征转换到高维空间上，就像我们在之前的过拟合&amp;局部加权回归中提到的，比如特征\\(x\\)表示的是房屋面积，我们需要预测房子是否会在6个月内被卖出，我们有时候会将这个特征映射成如下的形式： \\[ x \\rightarrow \\begin{bmatrix} x \\\\\\ x^2 \\\\\\ x^3 \\\\\\ x^4 \\end{bmatrix} = \\phi(x) \\] 原先的特征的内积形式\\(\\langle x^{(i)}, x^{(j)} \\rangle\\)会被写成\\(\\langle \\phi(x^{(i)}), \\phi(x^{(j)}) \\rangle\\)，而且往往\\(\\phi(x)\\)会有很高的维度。因为在很多情况下，计算\\(\\phi(x)\\)会有很高的代价，或者表示\\(\\phi(x)\\)需要很高的代价，但是光是计算内核则可能代价较小。 比如：假如有两个输入：\\(x, z \\in \\Bbb R^n\\)，核函数被定义为： \\[ \\begin{align} k(x, z) = (x^T z)^2 &amp;= (\\sum_{i=1}^nx_iz_i)(\\sum_{j=1}^nx_jz_j) \\\\\\ &amp;=\\sum_{i=1}^n\\sum_{j=1}^n(x_ix_j)(z_iz_j) \\\\\\ &amp;= \\phi(x)^T\\phi(z) \\end{align} \\] 假如需要表示成高维向量，那么\\(\\phi(x)\\)是一个\\(n \\times n\\)维的向量，如果\\(n = 3\\)： \\[ \\phi(x) = \\begin{bmatrix} x_1x_1 \\\\\\ x_1x_2 \\\\\\ x_1x_3 \\\\\\ x_2x_1 \\\\\\ \\vdots \\\\\\ x_3x_3 \\end{bmatrix} \\] 所以，计算\\(\\phi(x)\\)的时间复杂度就达到了\\(O(n^2)\\)，而计算核函数仅仅需要计算\\(x^Tz\\)，复杂度为\\(O(n)\\)。 接下去我们为这个核函数增加常数项： \\[ k(x,z)=(x^Tz+c)^2 \\] 那么： \\[ \\phi(x) = \\begin{bmatrix} x_1x_1 \\\\\\ x_1x_2 \\\\\\ x_1x_3 \\\\\\ x_2x_1 \\\\\\ \\vdots \\\\\\ x_3x_3 \\\\\\ \\sqrt{2c}x_1 \\\\\\ \\sqrt{2c}x_2 \\\\\\ \\sqrt{2c}x_3 \\\\\\ c \\end{bmatrix} \\] 更一般的： \\[ k(x, z)=(x^Tz+c)^d \\] 有了核函数，即可替换SVM中的内积\\(\\langle x^{(i)}, x^{(j)} \\rangle\\)，比如常用的高斯核： \\[ k(x,z)=\\exp(-\\frac{||x-z||^2}{2\\sigma^2}) \\] 有了核函数，相当于把数据从原始空间转换到了高位空间，很多数据，在一维空间往往是线性不可分的，但是到了高维空间会变成可分的： 核函数的合法性 如何判断一个核函数是合法的呢？判断依据是：是否存在函数\\(\\phi\\)，使得\\(k(x,z)\\)能够被写成\\(\\langle \\phi(x), \\phi(z) \\rangle\\)。 定理：如果核函数合法，那么其对应的核矩阵（kernel matrix）是半正定的。 核矩阵指的是矩阵\\(K \\in \\Bbb R^{m\\times m}\\)，其中\\(K_{ij}=k(x^{(i)}, x^{(j)})\\)。半正定的意思是，对于任意向量\\(z\\)，都存在\\(z^TKz \\geq 0\\)，证明如下： \\[ \\begin{align} z^TKz &amp;= \\sum_i\\sum_jz_iK_{ij}z_j \\\\\\ &amp;= \\sum_i\\sum_jz_i\\phi_(x^{(i)})^T\\phi_(x^{(j)})z_j \\\\\\ &amp;= \\sum_i\\sum_jz_i\\cdot \\sum_k\\phi_(x^{(i)})_k\\underbrace{\\phi_(x^{(j)})_k}_{向量第k项} \\cdot z_j \\\\\\ &amp;= \\sum_k\\sum_i\\sum_jz_i\\cdot \\phi_(x^{(i)})_k\\phi_(x^{(j)})_k \\cdot z_j \\\\\\ &amp;= \\sum_k(\\sum_iz_i\\phi(x^{(i)}))^2 \\geq 0 \\end{align} \\] 事实上，上面的定理的逆命题也一样成立，总结起来： Merce定理：给定核函数\\(k(x, z)\\)，那么\\(k(x, z)\\)合法（也即\\(\\exists \\phi, k(x,z)=\\phi(x)^T\\phi(z)\\)），当且仅当，对所有的\\(\\lbrace x^{(1)}, \\ldots, x^{(m)} \\rbrace\\)，核矩阵\\(K \\in \\Bbb R^{m\\times m}\\)是一个对称的半正定矩阵。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://jiacheng-pan.github.io/wiki/tags/SVM/"},{"name":"核函数","slug":"核函数","permalink":"https://jiacheng-pan.github.io/wiki/tags/核函数/"}]},{"title":"SVM（二）最优间隔分类器","slug":"吴恩达·机器学习/11-SVM（二）最优间隔分类器","date":"2018-07-03T08:27:00.000Z","updated":"2019-01-21T07:29:06.672Z","comments":true,"path":"2018/07/03/吴恩达·机器学习/11-SVM（二）最优间隔分类器/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/07/03/吴恩达·机器学习/11-SVM（二）最优间隔分类器/","excerpt":"","text":"最优间隔分类器（Optimal Margin Classifier）。其目标是使得最小几何间隔最大化（SVM（一）概念）： \\[ \\text{目标(1):} \\\\\\ \\max_{w, b} \\gamma \\\\\\ \\text{ s.t. } y^{(i)} \\cdot ((\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||}) \\geq \\gamma, i=1,\\ldots,n \\] 我们知道，\\(\\hat{\\gamma} = \\frac{\\gamma}{||w||}\\)，所以上面的目标可以等同于： \\[ \\text{目标(2):} \\\\\\ \\max_{w, b} \\frac{\\hat{\\gamma}}{||w||} \\\\\\ \\text{ s.t. }y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq \\hat{\\gamma}, i=1,\\ldots,n \\] 为了最大化上述值，我们有两种策略。 增大\\(\\hat{\\gamma}\\) 减小\\(||w||\\) 针对第一种可能，我们要证明其无效性。假如，我们增大\\(\\hat{\\gamma}\\)到\\({\\hat{\\gamma}}_1 := \\lambda {\\hat{\\gamma}}\\)，因为\\(\\hat{\\gamma}=y(w^Tx+b)\\)，可以视作\\(w_1:=\\lambda w, b_1 = \\lambda b\\)。所以，此时 \\[ \\frac{\\hat{\\gamma_1}}{||w_1||}=\\frac{\\lambda \\hat{\\gamma}}{||\\lambda w||} = \\frac{\\hat{\\gamma}}{||w||} \\\\\\ \\] 没有发生任何改变，所以第一条策略不可行。于是，我们可以固定\\(\\hat{\\gamma}=1\\) 此时，上述目标(2)可以表述成： \\[ \\text{目标(3):} \\\\\\ \\min_{w, b} \\frac{1}{2}||w||^2 \\\\\\ \\text{ s.t. }y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq 1, i=1,\\ldots,n \\] 因为最小化\\(||w||\\)和最小化\\(\\frac{1}{2}||w||^2\\)是一致的。 拉格朗日乘子法（Lagrange Multiplier） 为了解决上述的凸优化问题，我们引入拉格朗日乘子法Lagrange Multiplier来解决这个问题。 我们首先来看看凸优化问题的定义： \\[ \\min_wf(w) \\\\\\ \\text{s.t. }g_i(w) \\leq 0, h_i(w) =0 \\] 构建拉格朗日乘子： \\[ {\\cal L}(w, \\alpha, \\beta) = f(w)+\\sum_i\\alpha_ig_i(w)+\\sum_i\\beta_ih_i(w) \\] 定义： \\[ \\theta_p(w) = \\max_{\\alpha_i&gt;0, \\beta}{\\cal L}(w, \\alpha, \\beta) \\] 观察\\(\\theta_p(w)\\)： 如果\\(g_i(w)&gt;0\\)，那么\\(\\theta_p(w)=+\\infty\\)（因为\\(\\alpha\\)可以取任意大值）。 如果\\(h_i(w) \\neq 0\\)，那么\\(\\theta_p(w)=+\\infty\\)（因为\\(\\beta\\)可以取\\(+\\infty/-\\infty\\)）。 所以，在满足约束的情况下，\\(\\theta_p(w)=f(w)\\)，\\(\\min_w \\theta_p(w)=\\min_w f(w)\\)，因为使得\\({\\cal L}(w, \\alpha, \\beta)\\)最大的方法，就是其他所有项全是0。那么，可以得出这样的结论： \\[ \\theta_p(w)=\\begin{cases} f(w), &amp;\\text{满足约束} \\\\\\ \\infty, &amp;\\text{不满足约束} \\end{cases} \\] 因此，在满足条件的情况下，\\(\\min_w\\theta_p(w)\\)等价于\\(min_wf(w)\\)。 我们将最优间隔分类器的目标重新表示一下： \\[ p^\\ast =\\min_{w, b}\\max_\\alpha {\\cal L(w, \\alpha, b)} \\\\\\ {\\cal L}(w, \\alpha, b) = \\frac{1}{2}||w||^2+\\sum_i\\alpha_i(1-y^{(i)}(w^T x^{(i)}+b)) \\] 其中，直接忽略了\\(h_i(w)=0\\)的约束，而\\(g_i(w,b)=1-y^{(i)}(w^Tx^{(i)}+b) \\leq 0, f(w)=\\frac{1}{2}||w||^2\\) 对偶问题（Dual Problem） 一般来说，将原始问题转化成对偶问题来求解。一是因为对偶问题往往比较容易求解，二是因为对偶问题引入了核函数，方便推广到非线性分类的情况。 我们看到，之前的原始问题，是 \\[ p^\\ast =\\min_{w, b}\\max_\\alpha {\\cal L}(w, \\alpha, b) \\] 那么，定义其对偶问题： \\[ l^\\ast =\\max_\\alpha\\min_{w,b}{\\cal L}(w, \\alpha, b) \\] 接下去，我们求解对偶问题： 先求解\\(\\min_{w,b}{\\cal L}(w, \\alpha, b)\\)： 分别求偏导，使其等于0，导出最小值： \\[ \\begin{align} &amp; \\nabla_w{\\cal L}(w, \\alpha, b) =w-\\sum_{i=1}\\alpha_iy^{(i)}x^{(i)}=0 \\\\\\ &amp; \\nabla_b{\\cal L}(w, \\alpha, b) =\\sum_{i=1}\\alpha_iy^{(i)}=0 \\end{align} \\] 得到： \\[ w =\\sum_{i=1}\\alpha_iy^{(i)}x^{(i)} \\\\\\ \\sum_{i=1}\\alpha_iy^{(i)} = 0 \\] 代入\\({\\cal L}(w, \\alpha, b)\\)，就可以得到最小值： \\[ \\begin{align} {\\cal L}(w, \\alpha, b) &amp;= \\frac{1}{2}||w||^2+\\sum_i\\alpha_i(1-y^{(i)}(w^T x^{(i)}+b)) \\\\\\ \\min_{w, b}{\\cal L}(w, \\alpha, b) &amp;=\\underbrace{\\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j)}_{W(\\alpha)} \\end{align} \\] 于是，我们的对偶问题简化到了对\\(W(\\alpha)\\)最大化： \\[ \\max_\\alpha W(\\alpha) \\\\\\ \\text{s.t. }\\alpha_i \\geq 0, \\sum_iy_i\\alpha_i=0 \\] 假设，我们解得的对偶问题的解为：\\(\\alpha^\\ast =[\\alpha_1^\\ast ,\\alpha_2^\\ast , \\ldots, \\alpha_m^\\ast ]\\)，那么最终原始问题的解可以表示成： \\[ w^\\ast =\\sum_{i=1}\\alpha_i^\\ast y^{(i)}x^{(i)} \\] 在原始问题中，还有\\(b\\)未得到解决。我们先来观察一下约束项： \\[ g_i(w,b)=1-y{(i)}(w^Tx^{(i)}+b) \\leq 0 \\] 我们知道，在数据中，只有少数的几个数据点，他们的函数距离为1（最小），也即\\(g_i(w,b)=0\\)，如图所示。 在整个数据集中，只有这些数据点对约束超平面起了作用，这些数据点被称为支持向量（support vector），其对应的\\(\\alpha_i^\\ast \\neq 0\\)，而其他不是支持向量的数据点，没有对约束超平面起作用，其\\(\\alpha_i^\\ast =0\\)。 此时，我们已经得到了$w\\(，而\\)b\\(的计算如下，找到一个数据点，其\\)_j^0$(也就是支持向量，其函数间隔为1)，我们就能得到： \\[ y^{(j)}(w^{*T}x^{(j)}+b^\\ast )=1 \\Rightarrow b^\\ast =y^{(j)}-\\sum_{i=1}\\alpha_i^\\ast y^{(i)}(x^{(i)} \\cdot x^{(j)}) \\]","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://jiacheng-pan.github.io/wiki/tags/SVM/"},{"name":"最优间隔分类器","slug":"最优间隔分类器","permalink":"https://jiacheng-pan.github.io/wiki/tags/最优间隔分类器/"},{"name":"拉格朗日乘子法","slug":"拉格朗日乘子法","permalink":"https://jiacheng-pan.github.io/wiki/tags/拉格朗日乘子法/"}]},{"title":"SVM（一）概念","slug":"吴恩达·机器学习/10-SVM（一）概念","date":"2018-07-02T08:27:00.000Z","updated":"2019-01-20T14:21:01.715Z","comments":true,"path":"2018/07/02/吴恩达·机器学习/10-SVM（一）概念/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/07/02/吴恩达·机器学习/10-SVM（一）概念/","excerpt":"","text":"SVM，指的是支持向量机（support vector machines）。 支持向量机，假设数据是线性可分的，那么我们就能找到一个超平面，将数据分成两类。但是一旦线性可分，我们就可能找到无数的超平面，都可以将数据分成两类： 但是很明显，上图中虽然a, c都对数据进行了有效的分割。但很明显，都不如b分割的好。 我们可以用“间隔”这个概念来定义这个超平面（在二维上是线）对数据的分割优劣。在分类正确的情况下，间隔越大，我们认为对数据的分类越好。 我们的目标是得到数据的分类：\\(y \\in \\lbrace -1, +1 \\rbrace\\)。 这个超平面，则可以表示成\\(w^Tx+b\\)，其中\\(w=[\\theta_1, \\ldots, \\theta_n]^T, b=\\theta_0\\)。这个超平面可以表达成一个\\(n+1\\)维向量。 判别函数： \\[ g(z)=\\begin{cases} +1, &amp; \\text{如果$z\\geq0$} \\\\\\ -1, &amp; \\text{otherwise} \\end{cases} \\] 假设则可以表示成：\\(h_{w,b}(x)=g(w^Tx+b)\\) 间隔 函数间隔（functional margin） 某个超平面\\((w,b)\\)和训练样本\\((x^{(i)}, y^{(i)})\\)之间的函数间隔被表示成： \\[ \\hat{\\gamma}^{(i)}=y^{(i)}(w^Tx^{(i)}+b) \\] 于是，我们可以知道： 当\\(y^{(i)}=1\\)，于是我们想获得更大的函数间隔（这是我们的目标），就需要使得\\(w^Tx^{(i)}+b \\gg 0\\) 相反，当\\(y^{(i)}=-1\\)，我们想获得更大的函数间隔，就需要使得\\(w^Tx^{(i)}+b \\ll 0\\) 并且，很明显，只有当函数间隔\\(\\hat{\\gamma}&gt;0\\)时，分类结果是正确的。 最后，超平面与数据集\\(\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots \\rbrace\\)之间的函数间隔，被定义为所有函数间隔中的最小值： \\[ \\hat{\\gamma}=\\min_i\\hat{\\gamma}^{(i)} \\] 几何间隔（geometric margin） 从点\\((x^{(i)}, y^{(i)})\\)出发，对超平面做垂线，得到点D，我们知道他们之间的距离，就是该超平面到数据点\\((x^{(i)}, y^{(i)})\\)的几何间隔。 经过推导，D的坐标可以表示为： \\[ x^{(i)}-\\gamma^{(i)}\\frac{w}{||w||} \\] 又因为，D在超平面\\(w^Tx+b=0\\)上，所以： \\[ \\begin{align} &amp; w^T(x^{(i)}-\\gamma^{(i)}\\frac{w}{||w||})+b=0 \\\\\\ &amp; \\Rightarrow w^Tx^{(i)}+b=\\gamma^{(i)} \\cdot \\frac{w^Tw}{||w||}=\\gamma^{(i)} \\cdot ||w|| \\\\\\ &amp; \\Rightarrow \\gamma^{(i)}=(\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||} \\end{align} \\] 加上正负分类的判断： \\[ \\gamma^{(i)}=y^{(i)} \\cdot ((\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||}) \\] 我们可以看到，几何间隔跟函数间隔之间存在如下的关系： \\[ \\hat{\\gamma}^{(i)} = \\frac{\\gamma^{(i)}}{||w||} \\] 同样的，超平面与数据集\\(\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots \\rbrace\\)之间的几何间隔，被定义为所有几何间隔中的最小值： \\[ \\gamma=\\min_i\\gamma^{(i)} \\] 最后，我们导出最优间隔分类器（Optimal Margin Classifier）问题：选择\\(w, b\\)，最大化\\(\\gamma\\)，同时满足\\(\\forall(x^{(i)}, y^{(i)})\\)，$ y^{(i)} (()^T x^{(i)}+) $（所有数据点的几何间隔都大于该最小几何间隔）。 目前为止，已经是SVM问题的一个简化版本。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"https://jiacheng-pan.github.io/wiki/tags/SVM/"},{"name":"函数间隔","slug":"函数间隔","permalink":"https://jiacheng-pan.github.io/wiki/tags/函数间隔/"},{"name":"几何间隔","slug":"几何间隔","permalink":"https://jiacheng-pan.github.io/wiki/tags/几何间隔/"}]},{"title":"生成学习算法的例子","slug":"吴恩达·机器学习/09-生成学习算法的例子","date":"2018-06-29T13:10:00.000Z","updated":"2019-01-21T07:28:52.424Z","comments":true,"path":"2018/06/29/吴恩达·机器学习/09-生成学习算法的例子/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/29/吴恩达·机器学习/09-生成学习算法的例子/","excerpt":"","text":"例一：高斯判别分析和logistic函数 我们来看一个例子，对于一个高斯判别分析问题，根据贝叶斯： \\[ \\begin{align} p(y=1|x) &amp;= \\frac{p(x|y=1)p(y=1)}{p(x)} \\\\\\ &amp;= \\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)} \\end{align} \\] 在这里，我们提出几个假设： \\(p(y)\\)是均匀分布的，也就是\\(p(y=1)=p(y=0)\\) \\(x\\)的条件概率分布（\\(p(x|y=0)\\)和\\(p(x|y=1)\\)）满足高斯分布。 考虑二维的情况： image-20180630164349595 蓝色数据表达的是\\(p(x|y=0)\\)的分布，红色数据表达的是\\(p(x|y=1)\\)的分布，两条蓝色和红色的曲线分别是它们的概率密度曲线。 而灰色的曲线则表示了\\(p(y=1|x)\\)的概率密度曲线。 假设\\(p(x|y=0) \\sim N(\\mu_0, \\sigma_0)\\)，\\(p(x|y=1) \\sim N(\\mu_1, \\sigma_1)\\)，而\\(p(y)\\)均匀分布那么： \\[ \\begin{align} p(y=1|x) &amp;= \\frac{N(\\mu_0,\\sigma_0)}{N(\\mu_0,\\sigma_0)+N(\\mu_1,\\sigma_1)} \\\\\\ &amp;= \\cdots \\\\\\ &amp;= \\frac{1}{1+\\frac{\\sigma_0}{\\sigma_1}exp(2\\sigma_1^2(x-\\mu_0)^2-2\\sigma_0^2(x-\\mu_1)^2} \\end{align} \\] 事实上，这条曲线跟我们之前见过的logistic曲线非常像，特别是当我们假设\\(\\sigma_0=\\sigma_1\\)的时候，就是一条logistic曲线。 我们有如下的推广结论： \\[ {\\begin{cases} p(x|y=1) \\sim Exp Family(\\eta_1) \\\\\\ p(x|y=0) \\sim Exp Family(\\eta_0) \\end{cases}} \\Rightarrow p(y=1|x)是logistic函数 \\] 但这个命题的逆命题并不成立，故而我们知道，logistic所需要的假设更少（无需假设\\(x\\)的条件概率分布），鲁棒性更强。而生成函数因为对数据的分布做出了假设，所以需要的数据量会少于logstic回归，我们需要在两者之间进行权衡。 例二：垃圾邮件分类（1） 这里我们会用朴素贝叶斯（Naive Bayes）来解决垃圾邮件分类问题（\\(y\\in \\lbrace 0, 1 \\rbrace\\)）。 首先对邮件进行建模，生成特征向量如下： \\[ x= \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 0 \\\\\\ \\vdots \\\\\\ 1 \\\\\\ \\vdots \\end{bmatrix} \\begin{matrix} a \\\\\\ advark \\\\\\ ausworth \\\\\\ \\vdots \\\\\\ buy \\\\\\ \\vdots \\end{matrix} \\] 这是一个类似于词频向量的特征向量，我们有一个50000个词的词典，如果邮件中出现了某个词汇，那么其在向量中对应的位置就会被标记为1，否则为0。 我们的目标是获取，垃圾邮件和非垃圾邮件的特征分别是怎么样的，也即\\(p(x|y)\\)。\\(x={\\lbrace 0, 1 \\rbrace}^n, y \\in \\lbrace 0, 1 \\rbrace\\)，这里我们的词典中词汇数量是50000，所以\\(n=50000\\)，特征向量\\(x\\)会有\\(2^{50000}\\)种可能，需要\\(2^{50000}-1\\)个参数。 我们假设\\(x_i|y\\)之间相互独立(虽然假设各个单词的出现概率相互独立不是很合理，但是即便这样，朴素贝叶斯的效果依旧不错)，根据朴素贝叶斯，我们得到： \\[ p(x_1, x_2, \\ldots, x_{50000}|y)=p(x_1|y)p(x_2|y) \\cdots p(x_{50000}|y) \\] 单独观察\\(p(x_j|y=1)​\\)： \\[ p(x_j|y=1) = p(x_j=1|y=1)^{x_j}p(x_j=0|y=1)^{1-x_j} \\] 给定三个参数： \\[ \\begin{align} \\phi_{j|y=1} &amp;= p(x_j=1|y=1) \\\\\\ \\phi_{j|y=0} &amp;= p(x_j=1|y=0) \\\\\\ \\phi_y &amp;= p(y = 1) \\end{align} \\] 故： \\[ \\begin{align} p(x_j|y=1) &amp;= \\phi_{j|y=1}^{x_j}(\\phi_y - \\phi_{j|y=1})^{1-x_j}\\\\\\ p(x_j|y=0) &amp;= \\phi_{j|y=0}^{x_j}(1-\\phi_y + \\phi_{j|y=0})^{1-x_j} \\\\\\ p(x_j|y) &amp;= p(x_j|y=1)^yp(x_j|y=0)^{1-y} \\\\\\ p(y) &amp;= \\phi_y^y(1-\\phi_y)^{1-y} \\end{align} \\] 按照上个博客生成学习算法的概念中所述，我们会选用联合概率分布的极大似然来导出最优解： \\[ l(\\phi_y,\\phi_{j|y=1},\\phi_{j|y=0}=\\prod_{i=1}^mp(x^{(i)},y^{(i)})=\\prod_{i=1}^mp(x^{(i)}|y^{(i)})p(y^{(i)}) \\] 可以解得： \\[ \\begin{align} \\phi_{j|y=1} &amp;= \\frac{\\sum_{i=1}^m1\\lbrace x_j{(i)}=1, y^{(i)}=1 \\rbrace}{\\sum_{i=1}^m1\\lbrace y^{(i)}=1 \\rbrace} = \\frac{统计所有包含词语j的垃圾邮件的数量}{垃圾邮件的总数}\\\\\\ \\phi_{j|y=0} &amp;= \\frac{\\sum_{i=1}^m1\\lbrace x_j{(i)}=1, y^{(i)}=0 \\rbrace}{\\sum_{i=1}^m1\\lbrace y^{(i)}=0 \\rbrace} = \\frac{统计所有包含词语j的非垃圾邮件的数量}{非垃圾邮件的总数} \\\\\\ \\phi_y &amp;= \\frac{\\sum_{i=1}^m1\\lbrace y^{(i)}=1 \\rbrace}{m} = \\frac{垃圾邮件的数量}{邮件的总数} \\end{align} \\] 通过以上的公式，我们已经可以完全推得\\(p(x_1, x_2, \\ldots, x_{50000}|y)\\)。 Laplace平滑 假设，训练集中，我们重来没有碰到过\"NIPS\"这个词汇，假设我们词典中包含这个词，位置是30000，也就是说： \\[ \\begin{align} p(x_{30000}=1|y=1) &amp;= 0\\\\\\ p(x_{30000}=0|y=1) &amp;= 0 \\end{align} \\\\\\ \\Downarrow \\\\\\ p(x|y=1) =\\prod_{i=1}^{50000}p(x_i|y=1)=0 \\\\\\ p(x|y=0) =\\prod_{i=1}^{50000}p(x_i|y=0)=0 \\] 故而在分类垃圾邮件时： \\[ \\begin{align} p(y=1|x) &amp;= \\frac{p(x|y=1)p(y=1)}{p(x)} \\\\\\ &amp;=\\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)} \\\\\\ &amp;= \\frac{0}{0+0} \\end{align} \\] 所以，我们提出\\(p(x_{30000}=1|y=1) = 0\\)这样的假设不够好。 Laplace平滑就是来帮助解决这个问题的。 举例而言，在计算： \\[ \\phi_y=p(y=1)=\\frac{\\text{numof(1)}}{\\text{numof(0)}+\\text{numof(1)}} \\] 其中，\\(\\text{numof(1)}\\)表示的是，被分类为1的训练集中数据个数。 在Laplace平滑中，我们会采取如下策略: \\[ \\phi_y=p(y=1)=\\frac{\\text{numof(1)}+1}{\\text{numof(0)}+1+\\text{numof(1)}+1} \\] 比如，A球队在之前的五场比赛里面都输了，我们预测下一场比赛赢的概率： \\[ p(y=1)=\\frac{0+1}{0+1+5+1}=\\frac{1}{7} \\] 而不是简单的认为（没有Laplace平滑）是0。 推广而言，在多分类问题中，\\(y\\in\\lbrace1, \\ldots, k \\rbrace\\)，那么： \\[ p(y=j) = \\frac{\\sum_{i=1}^m1\\lbrace y^{(i)} = j \\rbrace+1}{m+k} \\] 例三：垃圾邮件分类（2） 之前的垃圾分类模型里面，我们对邮件提取的特征向量是： \\[ x=[1,0,0,\\ldots,1,\\ldots]^T \\] 这种模型，我们称之为多元伯努利事件模型（Multivariate Bernoulli Event Model）。 现在，我们换一种特征向量提取方式，将邮件的特征向量表示为： \\[ x=[x_1,x_2,\\ldots,x_j,\\ldots]^T \\] \\(x_j\\)表示词汇\\(j\\)在邮件中出现的次数。上述的特征向量也就是词频向量了。这种模型，我们称为多项式事件模型（Multinomial Event Model）。 对联合概率分布\\(p(x,y)\\)进行极大似然估计，得到如下的参数： \\[ \\begin{align} \\phi_{k|y=1} &amp;= p(x_j=k|y=1) = \\frac{C_{x=k}+1}{C_{y=1}+n} \\\\\\ \\phi_{k|y=0} &amp;=p(x_j=k|y=0) = \\frac{C_{x=k}+1}{C_{y=0}+n} \\\\\\ \\phi_{y} &amp;= p(y=1) = \\frac{C_{y=1}+1}{C_{y=1}+1+C_{y=0}+1} \\end{align} \\] 其中： \\(n\\)表示词典中词汇的数量，也就是特征向量的长度； \\[ C_{x=k}=\\sum_{i=1}^m(1\\lbrace y^{(i)}=1 \\rbrace \\sum_{j=1}^{n_i}1 \\lbrace x_j^{(i)} = k \\rbrace) \\] 表示在训练集中，所有垃圾邮件中词汇\\(k\\)出现的次数（并不是邮件的次数，而是词汇的次数）； \\[ C_{y=1}=\\sum_{i=1}^n(1\\lbrace y^{(i)} = 1 \\rbrace \\cdot n_i) \\] 表示训练集中垃圾邮件的所有词汇总长； \\[ C_{y=0}=\\sum_{i=1}^n(1\\lbrace y^{(i)} = 0 \\rbrace \\cdot n_i) \\] 表示训练集中非垃圾邮件的所有词汇总长；","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"生成学习算法","slug":"生成学习算法","permalink":"https://jiacheng-pan.github.io/wiki/tags/生成学习算法/"},{"name":"拉普拉斯平滑","slug":"拉普拉斯平滑","permalink":"https://jiacheng-pan.github.io/wiki/tags/拉普拉斯平滑/"}]},{"title":"生成学习算法的概念","slug":"吴恩达·机器学习/08-生成学习算法的概念","date":"2018-06-29T04:40:00.000Z","updated":"2019-01-21T07:28:45.415Z","comments":true,"path":"2018/06/29/吴恩达·机器学习/08-生成学习算法的概念/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/29/吴恩达·机器学习/08-生成学习算法的概念/","excerpt":"","text":"生成学习算法，英文为Generative Learning Algorithm。 我们之前看到的都是判别学习算法（Discriminative Learning Algorithm）。判别学习算法可以分成两种： 学得\\(p(y|x)\\)，比如之前的线性模型 学得一个假设\\(h_\\theta (x) = \\lbrace 0, 1 \\rbrace\\)，比如二分类问题 上面都是根据特征\\(x\\)，来对输出\\(y\\)进行建模，也许\\(y\\)是一个连续的值，也可能是离散的，比如类别。 那么，生成学习算法（Generative Learning Algorithm）刚好跟判别学习算法（Discriminative Learning Algorithm）相反，其用输出\\(y\\)对\\(x\\)进行建模，也就是：学得\\(p(x|y)\\)。 详细解释： 对于一个二分类或多分类问题，生成学习算法，在给定样本所述的类别的条件下，会对其样本特征建立一个概率模型。举例而言，在给定癌症是良性或者是恶性的条件下，生成模型会对该癌症的特征的概率分布进行建模。 根据朴素贝叶斯， \\[ p(y=1|x)=\\frac{p(x|y=1) p(y=1)}{p(x)} \\] 因为给定了\\(x\\)，所以\\(p(x)\\)可以视作1，也即 \\[ p(y=1|x)=p(x|y=1) p(y=1) \\] 高斯判别算法 Gaussian Discriminant Algorithm 对特征满足高斯分布的二分类问题进行建模。 假设\\(x \\in \\Bbb R^n\\)，且\\(p(x|y)\\)满足高斯分布，因为\\(x\\)往往是一个特征向量，故而这里的高斯分布是一个多元高斯分布（multivariate Gaussian）。 多元高斯分布，\\(z \\sim N(\\mu, \\Sigma)\\)。 \\[ p(z)=\\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}exp(-\\frac{1}{2}(z-\\mu)^T\\Sigma^{-1}(z-\\mu)) \\] \\(z\\)是一个\\(n\\)维向量，\\(\\mu\\)则是均值向量，\\(\\Sigma\\)是协方差矩阵：\\(E[(z-\\mu)(z-\\mu)^T]\\)。 对于一个二分类判别问题，正则响应函数为\\(\\phi\\)，那么 \\[ p(y)=\\phi^y(1-\\phi)^{1-y} \\] 且 \\[ p(x|y=0) \\sim N(\\mu_0, \\Sigma) \\\\\\ p(x|y=1) \\sim N(\\mu_1, \\Sigma) \\] (这里假设了，对于\\(y=0\\)和\\(y=1\\)，是两个不同均值，但协方差矩阵相同的多元高斯分布，所以这也是该模型的限制之一) 我们写出对数似然函数： \\[ \\begin{align} l(\\phi, \\mu_0, \\mu_1, \\Sigma) &amp;= log\\prod_{i=1}^mp(x^{(i)}, (y^{(i)})) \\\\\\ &amp;= log\\prod_{i=1}^mp(x^{(i)}| (y^{(i)}))p(y^{(i)}) \\end{align} \\] 对比一下之前二分类问题中的对数似然估计函数： \\[ l(\\theta) = \\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta)) \\] 这里，我们使用了联合概率（joint likelihood）: \\(p(x^{(i)}, (y^{(i)}))\\)，而在之前的判别模型里，我们用的是条件概率（conditional likelihood）: \\(P(y^{(i)}|x^{(i)};\\theta)\\)。因为，在判别模型中，特征\\(x\\)是给定的，所以用条件概率来进行极大似然估计；而在生成模型中，特征\\(x\\)不给定，所以需要用联合概率来进行极大似然估计。 然后，我们进行极大似然估计，得到： \\[ \\phi=\\frac{1}{m}\\sum_{i=1}^my^{(i)}=\\frac{1}{m}\\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace \\] 也就是分类标签为1的训练样本的比例。 再看\\(\\mu_0, \\mu_1\\)： \\[ \\mu_0=\\underbrace{\\sum_{i=1}^m 1\\lbrace y^{(i)} = 0 \\rbrace \\cdot x^{(i)}}_{(1)} /\\underbrace{ \\sum_{i=1}^m 1\\lbrace y^{(i)} = 0 \\rbrace}_{(2)} \\\\\\ \\mu_1=\\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace \\cdot x^{(i)} / \\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace \\] 式(1)表达了分类为0的样本中，特征\\(x\\)的和；式(2)表达了分类为0的样本个数；故而\\(\\mu_0\\)表达了样本中，分类为0的样本的特征的均值。同理得到\\(\\mu_1\\)。 我们再次回顾上面的高斯判别算法，事实上，高斯判别算法，假设了特征\\(x\\)满足了多元高斯分布，利用极大似然估计，对不同类别的特征\\(x\\)的分布进行了建模。 下节课我们将会探讨其他的关于生成学习算法的案例。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"生成学习算法","slug":"生成学习算法","permalink":"https://jiacheng-pan.github.io/wiki/tags/生成学习算法/"},{"name":"高斯判别算法","slug":"高斯判别算法","permalink":"https://jiacheng-pan.github.io/wiki/tags/高斯判别算法/"}]},{"title":"广义线性模型","slug":"吴恩达·机器学习/07-广义线性模型","date":"2018-06-09T08:29:00.000Z","updated":"2019-01-20T14:20:52.669Z","comments":true,"path":"2018/06/09/吴恩达·机器学习/07-广义线性模型/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/09/吴恩达·机器学习/07-广义线性模型/","excerpt":"","text":"广义线性模型，英文名为Generalized Linear Model，简称GLM。 之前，涉及到两种的两种模型： 1. 线性拟合模型，假设了\\(P(y|x;\\theta)\\)是高斯分布 2. 二分类问题，假设了\\(P(y|x;\\theta)\\)满足伯努利分布 但以上两者知识一种更广泛的，被称为『指数分布族』（The Exponential Family）的特例。 指数分布族 \\[ P(y;\\eta)=b(y)exp(\\eta^TT(y)-a(\\eta)) \\] 可以被表示为以上形式的分布，都是指数分布族的某个特定分布，给定\\(a, b, T\\)，就可以定义一个概率分布的集合，以\\(\\eta\\)为参数，就可以得到不同的概率分布。 在广义线性模型中，会假设\\(\\eta=\\theta^Tx\\)，也就是\\(\\eta\\)和特征\\(x\\)线性相关。 伯努利分布 首先，我们给出\\(y=1\\)的概率： \\[ P(y=1;\\phi)=\\phi \\] 于是： \\[ \\begin{align} P(y;\\phi) &amp;= \\phi^y(1-\\phi)^T\\\\\\ &amp;= exp(log(\\phi^T(1-\\phi^T)))\\\\\\ &amp;= exp(ylog(\\phi)+(1-y)log(1-\\phi))\\\\\\ &amp;= exp(log\\frac{\\phi}{1-\\phi} \\cdot y + log(1-\\phi)) \\end{align} \\] 比较我们上面的概率形式和指数分布族的标准形式，可以得到： \\[ \\begin{cases} \\eta &amp;= log\\frac{\\phi}{1-\\phi}, \\text{于是} \\phi=\\frac{1}{1+e^{-\\eta}}\\\\\\ a(\\eta) &amp;= -log(1-\\phi)=log(1+e^\\eta)\\\\\\ T(y) &amp;= y\\\\\\ b(y) &amp;= 1 \\end{cases} \\] 这里的\\(\\phi\\)一般会被称为正则响应函数（canonic response function）： \\[ g(\\eta) = E[y|\\eta]=\\frac{1}{1+e^{-\\eta}} \\] 相对的，正则关联函数（canonic link function）则是\\(g^{-1}\\)。 高斯分布 \\[ N(\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}(y-\\mu)^2) \\] 这里，出于简洁考虑，假设\\(\\sigma=1\\)，经过一系列化简后，可以表示成： \\[ \\frac{1}{\\sqrt{2\\pi}} \\cdot exp(-\\frac{1}{2}y^2) \\cdot exp(\\mu y-\\frac{1}{2}\\mu^2) \\] 那么， \\[ \\begin{cases} \\eta &amp;= \\mu\\\\\\ a(\\eta) &amp;= \\frac{1}{2}\\mu^2=\\frac{1}{2}\\eta^2\\\\\\ T(y) &amp;= y\\\\\\ b(y) &amp;= \\frac{1}{\\sqrt{2\\pi}} \\cdot exp(-\\frac{1}{2}y^2) \\end{cases} \\] 多项式分布 建模 在二项分布中，\\(y\\in \\lbrace 1, 2 \\rbrace\\) 而多项式分布，\\(y \\in \\lbrace 1,\\cdots, k \\rbrace\\) 一般会被用来进行邮件分类或者进行病情分类等等 我们假设 \\[ P(y=i)=\\phi_i \\] 也即，邮件属于\\(i\\)类的概率是\\(\\phi_i\\)，是关于特征\\(x\\)的一个函数。 那么，可以用\\(k\\)个参数来建模多项式分布 \\[ P(y)=\\prod_{i=1}^k\\phi_i^{1\\lbrace y=i \\rbrace} \\] 其中，\\(1 \\lbrace \\cdots \\rbrace\\)的含义为，检验\\(\\cdots\\)是否为真命题，若为真命题，则取1，否则取0。 因为所有概率和为1，所以最后一个参数 \\[ \\begin{align} \\phi_k &amp;= 1-\\sum_{j=1}^{k-1}\\phi_j \\\\\\ 1 \\lbrace y=k \\rbrace &amp;=1-\\sum_{j=1}^{k-1}1 \\lbrace y=j \\rbrace \\end{align} \\] 经过化简，也可以表示成： \\[ P(y)=exp[\\sum_{i=1}^{k-1}(log(\\frac{\\phi_i}{\\phi_k}) \\cdot 1\\lbrace y=i \\rbrace )] + log(\\phi_k) \\] 故而 \\[ \\eta = \\begin{bmatrix} log(\\frac{\\phi_1}{\\phi_k}) \\\\\\ \\vdots \\\\\\ log(\\frac{\\phi_{k-1}}{\\phi_k}) \\end{bmatrix} \\in \\Bbb R^{k-1} \\] \\[ a(\\eta) = -log(\\phi_k) \\] \\[ T(y)= \\begin{bmatrix} 1 \\lbrace y=1 \\rbrace \\\\\\ \\vdots \\\\\\ 1 \\lbrace y=k-1 \\rbrace \\end{bmatrix} \\in (0, 1)^{k-1} \\] \\[ b(y) = 1 \\] 根据\\(\\eta\\)可得： \\[ \\phi_i = e^{\\eta_i} \\cdot \\phi_k \\] 又因为： \\[ \\sum_{i=1}^{k}\\phi_i=\\sum_{i=1}^k\\phi_ke^{\\eta_i}=1 \\] 故而： \\[ \\phi_k = \\frac{1}{\\sum_{i=1}^ke^{\\eta_i}}=\\frac{1}{e^{\\eta_k}+\\sum_{i=1}^{k-1}e^{\\eta_i}} = \\frac{1}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}} \\] 所以： \\[ \\begin{align} \\phi_i &amp;= e^{\\eta_i} \\cdot \\phi_k \\\\\\ &amp;= \\frac{e^{\\eta_i}}{1 + \\sum_{j=1}^{k-1}e^{\\eta_j}} \\\\\\ &amp;= \\frac{e^{\\theta_i^Tx_i}}{1 + \\sum_{j=1}^{k-1}e^{\\theta_j^Tx_j}} \\end{align} \\] 上述函数，被称为『softmax』函数，这个函数的作用经常用于进行归一化。 经过上述步骤，假设函数可以被写成如下形式： \\[ h_\\theta(x)= \\left[ \\begin{array}{c} 1\\lbrace y=1 \\rbrace \\\\\\ \\vdots \\\\\\ 1\\lbrace y=k-1 \\rbrace \\end{array} | x;\\theta \\right]= \\begin{bmatrix} \\phi_1\\\\\\ \\vdots\\\\\\ \\phi_{k-1} \\end{bmatrix} \\] 回归 在经过上述推导，当我们有一堆训练集（\\((x^{(1)}, y^{(1)}), \\cdots, (x^{(m)}, y^{(m)})\\)）用于训练的时候，则可以进行极大似然估计： \\[ L(\\theta) = \\prod_{i=1}^mP(y^{(i)} | x^{(i)};\\theta) = \\prod_{i=1}^m\\prod_{j=1}^k\\phi_j^{1\\lbrace y^{(i)}=j \\rbrace } \\]","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"softmax","slug":"softmax","permalink":"https://jiacheng-pan.github.io/wiki/tags/softmax/"},{"name":"指数分布族","slug":"指数分布族","permalink":"https://jiacheng-pan.github.io/wiki/tags/指数分布族/"}]},{"title":"牛顿法","slug":"吴恩达·机器学习/06-牛顿法","date":"2018-06-07T12:40:00.000Z","updated":"2019-01-20T14:20:52.652Z","comments":true,"path":"2018/06/07/吴恩达·机器学习/06-牛顿法/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/07/吴恩达·机器学习/06-牛顿法/","excerpt":"","text":"牛顿法（英语：Newton's method）又称为牛顿-拉弗森方法（英语：Newton-Raphson method），它是一种在实数域和复数域上近似求解方程的方法。方法使用函数\\(\\displaystyle f(x)\\)的泰勒级数的前面几项来寻找方程\\(\\displaystyle f(y)=0\\)的根。 ——维基百科 牛顿法可以通过迭代逼近的方法，求得函数\\(f(x)=0\\)的解。 先初始化某个点\\(x_0\\)，对该点求导数\\(f&#39;(x_0)\\)，可以得到一条切线； 切线会和横轴再有一个交点\\(x_1\\)，然后再重复第一步； 直到\\(f(x_n)=0\\) 通过一系列推导，我们可以得知： \\[ x_{i+1}-x_{i}=\\frac{f(x^{(i)})}{f&#39;(x^{(i)})} \\] 于是，我们可以将牛顿法用于极大似然估计，也就是求\\(l(\\theta)\\)的最大值，可以看做是求\\(l&#39;(\\theta)=0\\)的解。 那么，每次迭代就可以写成： \\[ \\theta^{(t+1)}=\\theta^{(t)}-\\frac{l&#39;(\\theta^{(t)})}{l&#39;&#39;(\\theta^{(t)}} \\] 更一般地，可以写成： \\[ \\theta^{(t+1)}=\\theta^{(t)}-H^{-1}\\nabla_\\theta l \\] 其中，\\(H\\)是\\(l(\\theta)\\)的Hessian矩阵： \\[ H_{ij}=\\frac{\\partial^2l}{\\partial\\theta_i\\partial\\theta_j} \\] 但这个方法有个缺点，每次迭代的时候，都需要重新计算\\(H^{-1}\\)，虽然牛顿法对函数\\(f\\)有很多要求和限制，但对于logistic函数而言，足够有效。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"牛顿法","slug":"牛顿法","permalink":"https://jiacheng-pan.github.io/wiki/tags/牛顿法/"}]},{"title":"二分类问题","slug":"吴恩达·机器学习/05-二分类问题","date":"2018-06-05T03:03:00.000Z","updated":"2019-01-20T14:20:52.653Z","comments":true,"path":"2018/06/05/吴恩达·机器学习/05-二分类问题/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/05/吴恩达·机器学习/05-二分类问题/","excerpt":"","text":"在二分类问题中，输出\\(y\\in \\{0, 1\\}\\)。同样的，我们也可以用线性拟合来尝试解决二分类问题（如下图左），但数据点比较异常时，容易出现下图右这样的情况： 一般，在二分类问题中，我们会选用『logistic函数』来拟合（因为形状像S，又称为『sigmoid函数』）： \\[ h_\\theta (x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}} \\] logistic函数\\(g(z)=1/(1+e^{-z})​\\)的形状如下： 可以定义 \\[ \\begin{align} P(y=1|x;\\theta)&amp; =h_\\theta (x) \\\\\\ P(y=0|x;\\theta)&amp; =1-h_\\theta(x) \\end{align} \\] 于是： \\[ P(y|x;\\theta)=h_\\theta(x)^y(1-h_\\theta(x))^{(1-y)} \\] 进行极大似然估计： \\[ L(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^mh_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{(1-y^{(i)})} \\] 为了计算方便，定义 \\[ \\begin{align} l(\\theta)&amp;=log(L(\\theta))\\\\\\ &amp;=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))\\\\\\ &amp;=\\sum_{i=1}^m(y^{(i)}\\cdot log(h_\\theta(x^{(i)}))+(1-y^{(i)})\\cdot log(1-h_\\theta(x^{(i)}))) \\end{align} \\] 利用梯度上升进行求解： \\[ \\theta := \\theta + \\alpha \\nabla_\\theta l(\\theta) \\] 其中 \\[ \\nabla_{\\theta_j} l(\\theta)=\\frac{\\partial}{\\partial\\theta_j}l(\\theta)=\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)}\\\\\\ \\theta_j:=\\theta_j+\\alpha \\cdot \\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)} \\] 最终的梯度上升结果几乎与线性拟合中的梯度下降结果一样。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"二分类问题","slug":"二分类问题","permalink":"https://jiacheng-pan.github.io/wiki/tags/二分类问题/"}]},{"title":"线性模型的概率解释","slug":"吴恩达·机器学习/04-线性模型的概率解释","date":"2018-06-05T03:02:00.000Z","updated":"2019-01-20T14:20:01.708Z","comments":true,"path":"2018/06/05/吴恩达·机器学习/04-线性模型的概率解释/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/05/吴恩达·机器学习/04-线性模型的概率解释/","excerpt":"","text":"关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？ 我们更新一下假设函数，使之变为： \\[ y^{(i)} = \\theta^Tx^{(i)} + \\varepsilon^{(i)} \\] 其中，\\(\\varepsilon^{(i)}\\)是误差项，表示未捕获的特征（unmodeled effects），比如房子存在壁炉也影响价格，或者其他的一些随机噪音（random noise）。 一般，会假设误差项\\(\\varepsilon^{(i)} \\sim N(0, \\sigma^2)\\)（满足正态分布），也就是： \\[ P(\\varepsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\varepsilon^{(i)})^2}{2\\sigma^2}) \\] 关于为什么假设正态分布的解释： 便于数学运算； 很多独立分布的变量之间相互叠加后会趋向于正态分布（中心极限定理），在大多数情况下能成立 所以，\\(y^{(i)}\\)的后验分布： \\[ P(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}) \\sim N(\\theta^Tx^{(i)}, \\sigma^2) \\] 之后，进行极大似然估计（maximum likelihood estimation）：\\(max L(\\theta)\\)，即选择合适的\\(\\theta\\)，使得\\(y^{(i)}\\)对于\\(x^{(i)}\\)出现的概率最高（有一些存在即合理的感觉），其中\\(L(\\theta)\\)的定义如下： \\[ L(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}) \\] 那么，为了计算方便，我们定义： \\[ l(\\theta) = log(L(\\theta))=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))=m\\cdot log(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\sum_{i=1}^m\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2} \\] 于是，极大似然估计变为最小化： \\[ \\sum_{i=1}^m\\frac{(y{(i)}-\\theta^Tx{(i)})2}{2\\sigma2} \\] 也即之前线性回归所需进行最小二乘拟合的\\(J(\\theta)\\)。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"线性回归","slug":"线性回归","permalink":"https://jiacheng-pan.github.io/wiki/tags/线性回归/"}]},{"title":"过拟合&局部加权回归","slug":"吴恩达·机器学习/03-过拟合&局部加权回归","date":"2018-06-04T10:36:00.000Z","updated":"2019-01-20T14:20:52.669Z","comments":true,"path":"2018/06/04/吴恩达·机器学习/03-过拟合&局部加权回归/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/04/吴恩达·机器学习/03-过拟合&局部加权回归/","excerpt":"","text":"欠拟合和过拟合 对于之前房价的例子，假设只有一个特征size。 假如，我们只用简单的线性拟合（\\(\\theta_0+\\theta_1x_1\\)，\\(x_1\\)表示size），最终拟合结果会变一条直线，就可能产生下图最左边的结果，我们称之为『欠拟合』。 当我们尝试用二次曲线来拟合（\\(\\theta_0+\\theta_1x_1+\\theta_2x_1^2\\)，可以假设\\(x_2=x_1^2\\)，再进行线性拟合），就可能产生中间的结果。 但如果再继续增加曲线的复杂度，对于下图这种五个样本的例子，假如我们用一个五次曲线来拟合它（\\(\\theta_0+\\theta_1x1+\\theta_2x1^2+\\cdots+\\theta_5x_1^5\\)）就会精确拟合所有数据，产生右图的结果，我们称之为『过拟合』。 局部加权回归（Locally Weighted Regression） 局部加权回归，是一种特定的非参数学习方法。 什么叫非参数学习方法，首先，简单了解一下『参数化学习方法』(parametric learning algorithm)，是一种参数固定的学习方法，如上所示。而『非参数化学习方法』（non-parametric learning algorithm）则不固定参数，参数的个数会随着训练集数量而增长。 我们回顾一下，线性拟合中，我们的目标是找到合适的参数\\(\\theta\\)，使得最小化\\(\\sum_i(Y^{(i)} - \\theta^TX^{(i)})^2\\)。 而『局部线性拟合』，则是在某个局部区域A进行线性拟合，目标是最小化\\(\\sum_iw^{(i)}(Y^{(i)} - \\theta^TX^{(i)})^2\\)，其中权重\\[w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2})\\]，当然，权重公式是可替换的。 我们观察一下\\(w^{(i)}\\)的形状，当数据\\(x^{(i)}\\)靠近\\(x\\)时，其权重就会较大，那么对目标函数的贡献就会大一些；而数据远离\\(x\\)的时候，权重就会较小，贡献就会较小。这样做，目标函数就会更关注\\(x\\)附近的数据点，从而达到局部的目的。 当然，可以调整权重函数，常用的另一个权重函数：\\[w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2 \\tau^2 })\\]（波长函数），\\(\\tau\\)越大，波形越平缓，局部性越差。 但问题在于，当训练数据较大时，该方法的代价会很高。每要预测一个值，就需要重新进行一次局部线性拟合。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"局部加权回归","slug":"局部加权回归","permalink":"https://jiacheng-pan.github.io/wiki/tags/局部加权回归/"}]},{"title":"线性回归","slug":"吴恩达·机器学习/02-线性回归","date":"2018-06-03T12:04:00.000Z","updated":"2019-01-20T14:20:52.652Z","comments":true,"path":"2018/06/03/吴恩达·机器学习/02-线性回归/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/03/吴恩达·机器学习/02-线性回归/","excerpt":"","text":"首先引入一些后面会用到的定理： 定义1：定义函数\\(f: \\Bbb R^{m \\times n} \\mapsto \\Bbb R\\)，\\(A \\in \\Bbb R^{m \\times n}\\)，定义 \\[ \\nabla_Af(A)= \\begin{bmatrix} \\frac{\\partial f}{\\partial A_{11}} &amp; \\cdots &amp; \\frac{\\partial f}{\\partial A_{1n}}\\\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\\\ \\frac{\\partial f}{\\partial A_{m1}} &amp; \\cdots &amp; \\frac{\\partial f}{\\partial A_{mn}} \\end{bmatrix} \\] 定义2：矩阵的迹（Trace）：如果\\(A \\in R^{n\\times n}\\)方阵，那么\\(A\\)的迹，是\\(A\\)对角线元素之和 \\[ tr A = \\sum_{i=1}^nA_{ii} \\] 定理1：\\(tr AB = tr BA\\) 定理2：\\(tr ABC = tr CAB = tr BCA\\) 定理3：\\(f(A)=tr AB \\Rightarrow \\nabla_Af(A)=B^T\\) 定理4：\\(trA = tr A^T\\) 定理5：\\(a \\in R \\Rightarrow tr a=a\\) 定理6：\\(\\nabla_AtrABA^TC=CAB+C^TAB^T\\) 线性回归 一些符号的改写 上一篇博客提到，梯度下降的每一步，对某个参数\\(\\theta_i\\)，执行： \\[ \\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta) \\] 那么，\\(h_\\theta(x)\\)的所有参数\\(\\theta\\)可以表示成一列向量： \\[ \\theta = \\left[ \\begin{array}{c} \\theta_0\\\\\\ \\theta_1\\\\\\ \\vdots\\\\\\ \\theta_n \\end{array} \\right] \\in R^{n+1} \\] 我们可以定义： \\[ \\nabla_\\theta J = \\left[ \\begin{array}{c} \\frac{\\partial}{\\partial \\theta_0}J\\\\\\ \\frac{\\partial}{\\partial \\theta_1}J\\\\\\ \\vdots\\\\\\ \\frac{\\partial}{\\partial \\theta_n}J \\end{array} \\right] \\in R^{n+1} \\] 梯度下降过程可以表示成： \\[ \\theta:=\\theta - \\alpha\\nabla_\\theta J \\] 其中，\\(\\theta\\)和\\(\\nabla_\\theta J\\)都说是n+1维向量。 对于训练集中所有的输入\\({x^{(1)}},x^{(2)},…,x^{(m)}\\)，其中 \\[ x^{(i)} = \\left[ \\begin{array}{c} 1\\\\\\ x_1^{(i)}\\\\\\ \\vdots\\\\\\ x_n^{(i)}\\\\\\ \\end{array} \\right] \\in R^{n+1} \\] \\(h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\\)，可以表示成向量： \\[ \\left[ \\begin{array}{c} h_\\theta(x^{(1)})\\\\\\ h_\\theta(x^{(2)})\\\\\\ \\vdots\\\\\\ h_\\theta(x^{(m)})\\\\\\ \\end{array} \\right] = \\left[ \\begin{array}{c} (x^{(1)})^T\\theta\\\\\\ (x^{(2)})^T\\theta\\\\\\ \\vdots\\\\\\ (x^{(m)})^T\\theta\\\\\\ \\end{array} \\right] = \\left[ \\begin{array}{c} (x^{(1)})^T\\\\\\ (x^{(2)})^T\\\\\\ \\vdots\\\\\\ (x^{(m)})^T \\end{array} \\right] \\cdot \\theta = X \\cdot \\theta \\] 而 \\[ Y = \\left[ \\begin{array}{c} y^{(1)}\\\\\\ y^{(2)}\\\\\\ \\vdots\\\\\\ y^{(m)} \\end{array} \\right] \\] 于是， \\[ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h(x^{(i)} - y^{(i)})^2)=\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y) \\] 推导过程 关于梯度下降法，可以直接简化为求梯度为0的位置，即求\\(\\nabla_\\theta J(\\theta) = \\vec{0}\\) 首先，简化： \\[ \\begin{align} \\nabla_\\theta J(\\theta) &amp; = \\nabla_\\theta\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y)\\\\\\ &amp; =\\frac{1}{2}\\nabla_\\theta tr(\\theta^TX^TX\\theta - \\theta^TX^TY - Y^TX\\theta + Y^TY)\\\\\\ &amp; =\\frac{1}{2}[\\nabla_\\theta tr(\\theta\\theta^TX^TX) - \\nabla_\\theta tr(Y^TX\\theta) - \\nabla tr(Y^TX\\theta)] \\end{align} \\] 其中，第一项： \\[ \\begin{align} \\nabla_\\theta tr(\\theta\\theta^TX^TX) &amp; = \\nabla_\\theta tr(\\theta I \\theta^TX^TX) &amp;\\text{定理6, set: $\\theta =^{set} A, I = B, X^TX=C$}\\\\\\ &amp; = X^TX\\theta I + X^TX\\theta I &amp; \\text{$CAB+C^TAB^T$}\\\\\\ &amp; = X^TX\\theta + X^TX\\theta \\end{align} \\] 第二项和第三项： \\[ \\nabla_\\theta tr(Y^TX\\theta) = X^TY\\\\\\ (定理3，set:Y^TX = B, \\theta = A) \\] 所以： \\[ \\nabla_\\theta J(\\theta) = X^TX\\theta - X^TY = 0\\\\\\ \\Rightarrow X^TX\\theta = X^TY\\\\\\ \\] 最后解得： \\[ \\theta = (X^TX)^{(-1)}X^TY \\] 当然，以上的解是有限制的，只有当\\(X^TX\\)满秩时，才能够求逆。 如果非满秩，说明方程数量不够，也就是当需要n个参数时，却不够n个输入样本。","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"线性回归","slug":"线性回归","permalink":"https://jiacheng-pan.github.io/wiki/tags/线性回归/"}]},{"title":"监督学习&梯度下降法","slug":"吴恩达·机器学习/01-监督学习&梯度下降法","date":"2018-06-03T06:29:00.000Z","updated":"2019-01-20T14:19:47.141Z","comments":true,"path":"2018/06/03/吴恩达·机器学习/01-监督学习&梯度下降法/","link":"","permalink":"https://jiacheng-pan.github.io/wiki/2018/06/03/吴恩达·机器学习/01-监督学习&梯度下降法/","excerpt":"","text":"监督学习 符号定义： 符号 意义 \\(m\\) 训练集包含的数据个数 \\(x\\) 输入变量/特征（\u0004feature） \\(y\\) 输出变量/目标（target） \\((x, y)\\) 一个训连样本 \\((x^{(i)}, y^{(i)})\\) 第i个训练样本 监督学习的主要流程： 线性回归 以预测房价为例，我们的目标是导出一个函数（即假设），根据房子的特征（比如大小、卧室数量等等）来预测房价，那么： - 输入（特征）：\\(x_1, x_2, …\\)（比如大小、卧室数量等等） - 输出（目标）：\\(y\\)（房价） - 假设：\\(h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\\)，用于预测房价，其中\\(\\theta_i\\)是参数，\\(n\\)是特征数量 为了方便，可以将假设写成：\\(h(x)=\\sum_{i=0}^n\\theta_ix_i=\\theta^Tx​\\) 此时，学习函数（Learning Algorithm）的目标就是找到合适的参数\\(\\theta\\)，使之能够导出『合理』的假设\\(h(x)\\)，这里我们将『合理』理解为：\\(h_\\theta(x)\\)（假设）和\\(y\\)（目标）之间的差距最小，也即： \\[ \\displaystyle \\min_{\\theta}\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2 \\] 这里的\\(\\frac{1}{2}\\)是为了简化之后的计算。 我们定义\\[\\displaystyle J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2\\]，那么我们的目标就是去选取合适的\\(\\theta\\)，以最小化\\(J(\\theta)\\)。 梯度下降法 搜索算法（梯度下降） 目的：不断改变\\(\\theta\\)，从而来减少\\(J(\\theta)\\)。 原理：每次都往下降最快的地方走，从而找到一个局部最优解。 一般会初始化\\(\\vec{\\theta}=\\vec{0}\\)，然后每次都沿着梯度方向走，以保证每次都往下降最快的地方走： \\[ \\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta) \\] 其中，\\(:=\\)表示赋值操作，\\(\\alpha\\)为步长。 对于某个训练样本\\((x, y)​\\) \\[ \\displaystyle \\frac{\\partial}{\\partial \\theta_i}J(\\theta) = \\frac{\\partial}{\\partial \\theta_i}(\\frac{1}{2}(h_\\theta(x)-y)^2) \\] \\[ \\displaystyle = 2 \\times \\frac{1}{2}(h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(h_\\theta(x)-y) \\] \\[ \\displaystyle = (h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(\\theta_0x_0+…+\\theta_nx_n-y) \\] \\[ \\displaystyle =(h_\\theta(x)-y) \\times x_i \\] 那么， \\[ \\theta_i:=\\theta_i - \\alpha (h_\\theta (x) - y) \\times x_i \\] 批量梯度下降法（Batch Gradient Descent） 批量梯度下降法，使用的是所有训练样本的平均梯度： \\[ \\displaystyle \\theta_i:=\\theta_i - \\alpha \\frac{1}{m} \\sum_{j=1}^m(h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)} \\] 但每次下降都需要遍历所有样本，效率较低，具体过程可能如下： 随机梯度下降法（Stochastic Gradient Descent） 又称为『增量梯度下降法』 对每个样本\\((x_{(j)}, y_{(j)})\\)进行： \\[ \\displaystyle \\theta_i:=\\theta_i - \\alpha (h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)} \\] 直到收敛 这时，每次梯度下降只遍历一个样本，具体过程可能如下：","categories":[{"name":"吴恩达·机器学习","slug":"吴恩达·机器学习","permalink":"https://jiacheng-pan.github.io/wiki/categories/吴恩达·机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/机器学习/"},{"name":"梯度下降","slug":"梯度下降","permalink":"https://jiacheng-pan.github.io/wiki/tags/梯度下降/"},{"name":"监督学习","slug":"监督学习","permalink":"https://jiacheng-pan.github.io/wiki/tags/监督学习/"}]}]}