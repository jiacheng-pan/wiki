<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jiacheng Pan&#39;s Wiki</title>
  
  
  <link href="/wiki/atom.xml" rel="self"/>
  
  <link href="https://jiacheng-pan.github.io/wiki/"/>
  <updated>2019-01-21T07:56:33.441Z</updated>
  <id>https://jiacheng-pan.github.io/wiki/</id>
  
  <author>
    <name>Jiacheng Pan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>矩阵相关的概念</title>
    <link href="https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/%E7%9F%A9%E9%98%B5%E7%9B%B8%E5%85%B3%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    <id>https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/矩阵相关的概念/</id>
    <published>2019-01-18T05:56:00.000Z</published>
    <updated>2019-01-21T07:56:33.441Z</updated>
    
    <content type="html"><![CDATA[<h3 id="adjacency-matrix-邻接矩阵">adjacency matrix 邻接矩阵</h3><p>一个图的邻接矩阵的定义是：<span class="math inline">\(A=\lbrace a_{ij} \rbrace\)</span>，其中<span class="math inline">\(a_{ij}=e_{ij}\)</span>，第<span class="math inline">\(ij\)</span>个元素指的是边<span class="math inline">\(e_{ij}\)</span>的值。</p><h3 id="incidence-matrix-关联矩阵">incidence matrix 关联矩阵</h3><p>一个图的关联矩阵定义为：<span class="math inline">\(M=\lbrace m_{ij} \rbrace\)</span>，其中<span class="math inline">\(m_{ij}\)</span>为1，如果节点<span class="math inline">\(v_i\)</span>和边<span class="math inline">\(e_j\)</span>相关，举例而言：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/GraphKnoledge/image-20190116204213867.png"></p><p>的关联矩阵为：</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">e1</th><th style="text-align: center;">e2</th><th style="text-align: center;">e3</th><th style="text-align: center;">e4</th><th style="text-align: center;">e5</th><th style="text-align: center;">e6</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">v1</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td></tr><tr class="even"><td style="text-align: center;">v2</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">v3</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td></tr><tr class="even"><td style="text-align: center;">v4</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td></tr><tr class="odd"><td style="text-align: center;">v5</td><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;adjacency-matrix-邻接矩阵&quot;&gt;adjacency matrix 邻接矩阵&lt;/h3&gt;
&lt;p&gt;一个图的邻接矩阵的定义是：&lt;span class=&quot;math inline&quot;&gt;\(A=\lbrace a_{ij} \rbrace\)&lt;/span&gt;，其中&lt;s
      
    
    </summary>
    
      <category term="GraphKnowledge" scheme="https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"/>
    
    
      <category term="图" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%9B%BE/"/>
    
      <category term="graph" scheme="https://jiacheng-pan.github.io/wiki/tags/graph/"/>
    
  </entry>
  
  <entry>
    <title>结构相关的概念</title>
    <link href="https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/%E7%BB%93%E6%9E%84%E7%9B%B8%E5%85%B3%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    <id>https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/结构相关的概念/</id>
    <published>2019-01-18T05:55:00.000Z</published>
    <updated>2019-01-21T13:07:59.255Z</updated>
    
    <content type="html"><![CDATA[<h3 id="bipartite-二部">bipartite 二部</h3><p>将图的顶点分成两个非空集合<span class="math inline">\(V_1\)</span>和<span class="math inline">\(V_2\)</span>，如果这个图的每一条边的两个端点都分别属于<span class="math inline">\(V_1\)</span>和<span class="math inline">\(V_2\)</span>，那么称这个图是二部图（bipartite）。</p><p>在这个概念的基础上，还有完全二部图（complete bipartite graph）的概念，指的是在bipartite的基础上，<span class="math inline">\(V_1\)</span>中的每个点，都与<span class="math inline">\(V_2\)</span>中的每个点相连。</p><h3 id="graphlets-图元">graphlets 图元</h3><p>图元，指的是一系列异构的（<em>isomorphism</em>）导出（<em>induced</em>）子图。</p><p>下面列举所有的3节点、4节点、5节点的图元：</p><figure><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-9-1/21892478.jpg" alt="3节点、4节点、5节点的所有图元"><figcaption>3节点、4节点、5节点的所有图元</figcaption></figure><h3 id="induced-导出">induced 导出</h3><p>给定一个图<span class="math inline">\(G\)</span>，以及这个图的子图（subgraph）<span class="math inline">\(G&#39;=(V&#39;, E&#39;)\)</span>，其中<span class="math inline">\(V&#39;\subseteq V\)</span>并且<span class="math inline">\(E&#39; \subseteq E\)</span>。如果<span class="math inline">\(E&#39; = \{(v_i,v_j)|(v_i,v_j) \in E \text{ and }v_i, v_j \in V&#39;\}\)</span>，也就是对于所有属于<span class="math inline">\(V&#39;\)</span>的节点，他们在原图<span class="math inline">\(G\)</span>的中出现的所有边，也都出现在<span class="math inline">\(G&#39;\)</span>的<span class="math inline">\(E&#39;\)</span>中，那么，这个子图<span class="math inline">\(G&#39;\)</span>称为<span class="math inline">\(G\)</span>的导出（induced）子图。</p><p>下面是导出子图的一个示例：</p><figure><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/2018-06-25-085552.png" alt="导出子图"><figcaption>导出子图</figcaption></figure><h3 id="isomorphism-同构">isomorphism 同构</h3><p>如果两个图<span class="math inline">\(G = (V,E)\)</span> 和 <span class="math inline">\(G&#39; = (V&#39;, E&#39;)\)</span> 之间存在一个双射（bijection）函数 <span class="math inline">\(f: V \rightarrow V&#39;\)</span>，使得$ (v_i, v_j) E (f(v_i),f(v_j)) E' v_i,v_j V$，那么这两个图互为同构图（isomorphism）</p><h3 id="subgraph-子图">subgraph 子图</h3><p>给定一个图<span class="math inline">\(G\)</span>，那么定义这个图的子图<span class="math inline">\(G&#39;=(V&#39;, E&#39;)\)</span>，其中<span class="math inline">\(V&#39;\subseteq V\)</span>并且<span class="math inline">\(E&#39; \subseteq E\)</span>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;bipartite-二部&quot;&gt;bipartite 二部&lt;/h3&gt;
&lt;p&gt;将图的顶点分成两个非空集合&lt;span class=&quot;math inline&quot;&gt;\(V_1\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(V_2\)&lt;/span&gt;，如果
      
    
    </summary>
    
      <category term="GraphKnowledge" scheme="https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"/>
    
    
      <category term="图" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%9B%BE/"/>
    
      <category term="graph" scheme="https://jiacheng-pan.github.io/wiki/tags/graph/"/>
    
  </entry>
  
  <entry>
    <title>链接相关的概念</title>
    <link href="https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/%E9%93%BE%E6%8E%A5%E7%9B%B8%E5%85%B3%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    <id>https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/链接相关的概念/</id>
    <published>2019-01-18T05:55:00.000Z</published>
    <updated>2019-01-21T12:45:10.953Z</updated>
    
    <content type="html"><![CDATA[<h3 id="bridgecut-edgeisthmus-桥边割边峡谷">bridge/cut edge/isthmus 桥边/割边/峡谷</h3><p><code>Bridge</code>，也被称为<code>cut edge</code>或者<code>isthmus</code>，指的是某一条边，如果在图中删掉该边，这个图中的连通子图的数量会增加，那么这条边就会被称为<code>bridge</code>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;bridgecut-edgeisthmus-桥边割边峡谷&quot;&gt;bridge/cut edge/isthmus 桥边/割边/峡谷&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Bridge&lt;/code&gt;，也被称为&lt;code&gt;cut edge&lt;/code&gt;或者&lt;code&gt;isthmus&lt;
      
    
    </summary>
    
      <category term="GraphKnowledge" scheme="https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"/>
    
    
      <category term="图" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%9B%BE/"/>
    
      <category term="graph" scheme="https://jiacheng-pan.github.io/wiki/tags/graph/"/>
    
  </entry>
  
  <entry>
    <title>节点相关的概念</title>
    <link href="https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/%E8%8A%82%E7%82%B9%E7%9B%B8%E5%85%B3%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    <id>https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/节点相关的概念/</id>
    <published>2019-01-18T05:54:00.000Z</published>
    <updated>2019-01-21T13:23:19.237Z</updated>
    
    <content type="html"><![CDATA[<h3 id="betweenness-介数">betweenness 介数</h3><p>节点的betweenness，可以定义为： <span class="math display">\[betweenness(v) = \sum_{s \neq v \neq t \in V}\frac{\sigma_{st}(v)}{\sigma_{st}}\]</span> 其中<span class="math inline">\(\sigma_{st}\)</span>指的是节点<span class="math inline">\(s, t\)</span>之间的最短路的数量，而<span class="math inline">\(\sigma_{st}(v)\)</span>则是其中经过节点<span class="math inline">\(v\)</span>的路径数量。</p><p>上述定义可以阐述为：</p><p>计算网络中任意两个节点之间的所有最短路径，那么某个节点的bewteenness可以被定义为路过这个节点的最短路数量比例。</p><h3 id="centrality-中心度">centrality 中心度</h3><p>centrality，中心度，可以用于回答“重要节点的特征是什么？”之类的问题。一般而言，有这三种度量方式比较常用：</p><ul><li>degree centrality[/#degree-度]</li><li>betweenness centrality[/#betweenness-介数]</li><li>closeness centrality[#closeness-亲密度]</li></ul><h3 id="closeness-亲密度">closeness 亲密度</h3><p>节点的closeness centrality，可以被定义为： <span class="math display">\[closeness(v) = \frac{1}{\sum_u d(v,u)}\]</span> 其中，<span class="math inline">\(d\)</span>指两个节点之间的距离，也即它们之间的最短路长度。如果一个节点距离整个图的所有节点都很接近，那么它的closeness也就会很高。</p><h3 id="degree-度">degree 度</h3><p>一个顶点的度（degree）指与该顶点关联的边的数目。当边有权重时，就是所有边的权重和。记做<span class="math inline">\(deg(v)\)</span>。</p><p>在有向图中，还有出度（out-degree）和入度（in-degree）的概念。</p><p>出度（out-degree）指以该顶点为起点的边的权重和，入度（in-degree）指的是以该顶点为终点的边的权重和。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;betweenness-介数&quot;&gt;betweenness 介数&lt;/h3&gt;
&lt;p&gt;节点的betweenness，可以定义为： &lt;span class=&quot;math display&quot;&gt;\[
betweenness(v) = \sum_{s \neq v \neq t \i
      
    
    </summary>
    
      <category term="GraphKnowledge" scheme="https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"/>
    
    
      <category term="图" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%9B%BE/"/>
    
      <category term="graph" scheme="https://jiacheng-pan.github.io/wiki/tags/graph/"/>
    
  </entry>
  
  <entry>
    <title>Introduction</title>
    <link href="https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/Home/"/>
    <id>https://jiacheng-pan.github.io/wiki/2019/01/18/GraphKnowledge/Home/</id>
    <published>2019-01-18T05:53:00.000Z</published>
    <updated>2019-01-21T07:54:50.910Z</updated>
    
    <content type="html"><![CDATA[<p>一些图论、图挖掘等图相关领域的知识收集、汇总。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一些图论、图挖掘等图相关领域的知识收集、汇总。&lt;/p&gt;

      
    
    </summary>
    
      <category term="GraphKnowledge" scheme="https://jiacheng-pan.github.io/wiki/categories/GraphKnowledge/"/>
    
    
      <category term="图" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%9B%BE/"/>
    
      <category term="graph" scheme="https://jiacheng-pan.github.io/wiki/tags/graph/"/>
    
  </entry>
  
  <entry>
    <title>模型选择和特征选择</title>
    <link href="https://jiacheng-pan.github.io/wiki/2019/01/18/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/16-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <id>https://jiacheng-pan.github.io/wiki/2019/01/18/吴恩达·机器学习/16-模型选择和特征选择/</id>
    <published>2019-01-18T05:53:00.000Z</published>
    <updated>2019-01-20T14:21:01.748Z</updated>
    
    <content type="html"><![CDATA[<p>首先来说几种验证训练结果的方法。</p><ul><li><p>保留交叉验证法 hold out cross validation</p><p>给出一个训练集，随机分成两个，一部分当做训练子集（一般占70%）用于训练，另一部分当做保留交叉验证子集（30%）用于测试。</p></li><li><p>k重交叉验证法 k-fold cross validation</p><p>将整个数据集分成k部分，拿出k-1部分进行训练，将剩下一个用于测试。重复上述过程k次（每次都不一样），求出均值即为验证结果。一般k取10。</p><p>优点：增加了训练数据；缺点：计算代价高；</p></li><li><p>留一交叉验证法 leave one out cross validation</p><p>当k=m（训练数据量）时，称之为留一交叉验证法。一般用于训练数据过少的情况。</p></li></ul><h2 id="特征选择">特征选择</h2><p>在进行学习时，过多的特征容易带来过拟合，需要选出一个特征子集，其中的特征与学习算法最相关。</p><h3 id="前向搜索法-forward-search">前向搜索法 forward search</h3><p>过程如下：</p><ol type="1"><li>初始化特征集<span class="math inline">\(\cal F\)</span></li><li>对于不属于<span class="math inline">\(\cal F\)</span>的每一个特征，计算添加该特征后模型精度的提升；</li><li>选择提升最大的特征加入<span class="math inline">\(\cal F\)</span></li><li>重复2和3，直到精度不在上升。</li></ol><p>当然可以设置阈值k，当<span class="math inline">\(\cal F\)</span>包含了k个特征后即停止。</p><h3 id="后向搜索法-backward-search">后向搜索法 backward search</h3><p>基本步骤与上述类似，只是过程相反：首先从满的特征集开始，之后每次删除表现最差的特征。</p><hr><p>上述的方法都被称为<strong>"wrapping" feature selection</strong>（“封装”特征选择），"封装"这个词，意味着：</p><p>当你进行选择时（前向搜索或者后向搜索）你需要重复使用学习算法去训练模型，根据结果来选择特征子集。</p><p>这种方法主要缺点是计算量大，但是它是一种比较准确的选择方法。</p><p>另外有一种算法，可能它的泛化误差不会太低，但是计算代价较小。</p><h3 id="特征过滤-filter-method">特征过滤 filter method</h3><p>主要方法是，计算每一个特征的一些度量，来衡量对y（label）的影响有多大，一般使用互信息（Mutual Information）来度量： <span class="math display">\[\begin{align}MI(x_i, y) &amp;= \sum_{x_i \in \lbrace 0,1 \rbrace} \sum_{y \in \lbrace 0,1 \rbrace} p(x_i, y) \log \frac{p(x_i, y)}{p(x_i)p(y)} \\\&amp;= KL(p(x, y) || p(x)p(y)) \text{, KL divergence}\end{align}\]</span> 然后去选择最好的k个特征。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先来说几种验证训练结果的方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;保留交叉验证法 hold out cross validation&lt;/p&gt;
&lt;p&gt;给出一个训练集，随机分成两个，一部分当做训练子集（一般占70%）用于训练，另一部分当做保留交叉验证子集（30%）用于测试。&lt;/
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型选择" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9/"/>
    
      <category term="特征选择" scheme="https://jiacheng-pan.github.io/wiki/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
  </entry>
  
  <entry>
    <title>Vapnik–Chervonenkis dimension</title>
    <link href="https://jiacheng-pan.github.io/wiki/2019/01/10/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/15-Vapnik-Chervonenkis%20Dimension/"/>
    <id>https://jiacheng-pan.github.io/wiki/2019/01/10/吴恩达·机器学习/15-Vapnik-Chervonenkis Dimension/</id>
    <published>2019-01-10T05:53:00.000Z</published>
    <updated>2019-01-20T14:21:01.717Z</updated>
    
    <content type="html"><![CDATA[<p>延续上节课的内容。</p><p>给定<span class="math inline">\(|\cal H| = k\)</span>，给定<span class="math inline">\(\delta, \gamma\)</span>，为了保证： <span class="math display">\[\varepsilon({\hat h}) \leq \varepsilon(h) + 2\gamma\]</span> 的概率不小于<span class="math inline">\(1-\delta\)</span>，那么<span class="math inline">\(m\)</span>需要满足： <span class="math display">\[m \geq \frac{1}{2\gamma^2}\log \frac{2k}{\delta} = O(\frac{1}{\gamma^2}\log \frac{k}{\delta})\]</span></p><p>假如<span class="math inline">\(\cal H\)</span>是以<span class="math inline">\(d\)</span>个实数为参数的(比如为了解决<em>n</em>个特征的分类问题，<em>d</em>就等于<em>n+1</em>)，而在计算机中，实数多以64位浮点数保存，d个实数就需要64d位来存储，那么<span class="math inline">\(\cal H\)</span>的整个假设空间大小就为<span class="math inline">\(2^{64d}\)</span>，也即<span class="math inline">\(k=2^{64d}\)</span>，那么： <span class="math display">\[m \geq O(\frac{1}{\gamma^2}\log \frac{k}{\delta}) = O(\frac{d}{\gamma^2}\log \frac{1}{\delta})\]</span> 最直观的解释就是<span class="math inline">\(m\)</span>与假设类的参数数量几乎是成正比的。</p><hr><p>定义<strong>Shatter（分散）</strong>：给定一个由<span class="math inline">\(d\)</span>个点构成的集合：<span class="math inline">\(S=\lbrace x^{(1)}, \ldots, x^{(d)} \rbrace\)</span>，我们说一个假设类<span class="math inline">\(\cal H\)</span>能够<strong>分散(shatter)</strong>一个集合<span class="math inline">\(S\)</span>，如果<span class="math inline">\(\cal H\)</span>能够实现对<span class="math inline">\(S\)</span>的任意一种标记方式，也即，对<span class="math inline">\(S\)</span>的任意一种标记方式，我们都可以从<span class="math inline">\(\cal H\)</span>中找到对应的假设来进行分割。 举例而言，如果<span class="math inline">\({\cal H} = \lbrace \text{linear classification in 2D} \rbrace\)</span>(二维线性分类器的集合)，对于二维平面上的三个点，有8种标记方式：</p><figure><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/19-1-11/image-20190111094049430.png" alt="image-20190111094049430"><figcaption>image-20190111094049430</figcaption></figure><p>那么，蓝线所代表的线性分类器，都能完成对它们的标记，所以我们称<span class="math inline">\(\cal H\)</span>能够分散平面上三个点所构成的集合。但是对于平面上四个点，就有存在以下这种情况，没有任何的线性分类器能够实现这种标记：</p><figure><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/19-1-11/image-20190111094649618.png" alt="image-20190111094649618"><figcaption>image-20190111094649618</figcaption></figure><hr><p>定义<strong>Vapnik-Chervonenkis dimension（VC维）</strong>：假设集<span class="math inline">\(\cal H\)</span>的<strong>VC维</strong>，写成<span class="math inline">\(VC({\cal H})\)</span>，指的是能够被<span class="math inline">\(\cal H\)</span>分散的最大集合的大小。 举例而言，如果<span class="math inline">\(\cal H\)</span>是所有二维线性分类器构成的集合，那么<span class="math inline">\(VC(\cal H) = 3\)</span>。当然并不是说<span class="math inline">\(\cal H\)</span>要能分散所有三个点构成的集合，只要有某个三个点构成的集合能被<span class="math inline">\(\cal H\)</span>分散即可，比如下面这种标记方式，<span class="math inline">\(\cal H\)</span>就无法实现，但是我们还是称<span class="math inline">\(VC(\cal H) = 3\)</span>。</p><figure><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/19-1-11/image-20190111095306079.png" alt="image-20190111095306079"><figcaption>image-20190111095306079</figcaption></figure><p>有一个推论：</p><p><span class="math inline">\(VC({\text{linear classification of n D}}) = n + 1\)</span></p><hr><p>定理：给定假设集合<span class="math inline">\(\cal H\)</span>，令<span class="math inline">\(VC({\cal H})=d\)</span>，那么，对于任意的<span class="math inline">\(h \in {\cal H}\)</span>： <span class="math display">\[|\varepsilon(h)-{\hat \varepsilon}(h)| \leq O(\sqrt{\frac{d}{m} \log \frac{m}{d} + \frac{1}{m} \log \frac{1}{\delta}})\]</span> 的概率不小于<span class="math inline">\(1 - \delta\)</span>，以及 <span class="math display">\[\varepsilon({\hat h}) \leq \varepsilon(h^\ast) + 2 \gamma, \gamma = O(\sqrt{\frac{d}{m} \log \frac{m}{d} + \frac{1}{m} \log \frac{1}{\delta}})\]</span> 的概率不小于<span class="math inline">\(1-\delta​\)</span>。</p><p>引理：为了保证<span class="math inline">\(\varepsilon({\hat h}) \leq \varepsilon(h ^ \ast) + 2 \gamma\)</span>至少在<span class="math inline">\(1 - \delta\)</span>的概率下成立，应该满足： <span class="math display">\[m = O_{\gamma, \delta}(d)\]</span> <span class="math inline">\(O_{\gamma, \delta}(d)\)</span>指的是，在固定<span class="math inline">\(\gamma, \delta\)</span>的情况下，与<span class="math inline">\(d\)</span>线性相关。</p><p>也即，<span class="math inline">\(m\)</span>必须与<span class="math inline">\(\cal H\)</span>的VC维保持一致，也可以这么理解，为了使泛化误差和训练误差近似，训练样本数目必须和模型的参数数量成正比。</p><hr><p>在SVM中，给定数据集，如果我们只考虑半径R以内的点，以及间隔至少为<span class="math inline">\(\gamma\)</span>的线性分类器构成的假设类，那么： <span class="math display">\[VC({\cal H}) \leq \lceil \frac{R^2}{4\gamma^2} \rceil + 1\]</span></p><p>也就说明，<span class="math inline">\(\cal H​\)</span> 的VC维上限，并不依赖于数据集中点<span class="math inline">\(x​\)</span>的维度，换句话说，虽然点可能位于无限维的空间中，但是如果只考虑那些具有较大函数间隔的分类器所组成的假设类，那么VC维就存在上界。</p><p>所以SVM会自动尝试找到一个具有较小VC维的假设类，所以它不会过拟合（模型参数不会过大）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;延续上节课的内容。&lt;/p&gt;
&lt;p&gt;给定&lt;span class=&quot;math inline&quot;&gt;\(|\cal H| = k\)&lt;/span&gt;，给定&lt;span class=&quot;math inline&quot;&gt;\(\delta, \gamma\)&lt;/span&gt;，为了保证： &lt;span cl
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征选择" scheme="https://jiacheng-pan.github.io/wiki/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="VC维" scheme="https://jiacheng-pan.github.io/wiki/tags/VC%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>经验风险最小化</title>
    <link href="https://jiacheng-pan.github.io/wiki/2019/01/05/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14-%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96/"/>
    <id>https://jiacheng-pan.github.io/wiki/2019/01/05/吴恩达·机器学习/14-经验风险最小化/</id>
    <published>2019-01-05T05:53:00.000Z</published>
    <updated>2019-01-20T14:21:01.717Z</updated>
    
    <content type="html"><![CDATA[<p>就线性分类模型而言，可以将其表示为： <span class="math display">\[h_\theta(x)=g(\theta^Tx), \\\g(z) = 1\lbrace z \geq 0 \rbrace\]</span> 其中，训练集表示为： <span class="math display">\[S=\lbrace (x^{(i)}, y^{(i)}) \rbrace _ {i = 1} ^ m, (x^{(i)}, y^{(i)}) \sim {\cal D}\]</span> 这里假设了训练数据都是独立同分布的。</p><p>那么，我们认为，这个线性分类器的<strong>训练误差</strong>就可以表示为它分类错误的样本比例： <span class="math display">\[{\hat{\varepsilon}}(h_\theta) = {\hat{\varepsilon}}_s(h_\theta) = \frac{1}{m}\sum_{i=1}^m1\lbrace h_\theta (x^{(i)}) \neq y^{(i)} \rbrace\]</span> 在这里，我们把训练误差也称为<strong>风险</strong>（risk），由此我们导出了经验风险最小化。</p><h3 id="经验风险最小化">经验风险最小化</h3><h3 id="empirical-risk-minimizationerm">Empirical Risk Minimization，ERM</h3><p>经验风险最小化，最终导出一组参数，能够使得训练误差最小： <span class="math display">\[{\hat{\theta}} = \arg \min {\hat{\varepsilon}}_s(h_\theta)\]</span></p><p>我们再定义一个假设类<span class="math inline">\({\cal{H}} = \lbrace h_\theta, \theta \in {\Bbb R}^{n+1} \rbrace\)</span>，它是所有假设的集合。在线性分类中，也就是所有线性分类器的集合。</p><p>那么，我们可以重新定义一次ERM： <span class="math display">\[{\hat h} = \mathop{\arg \min}_{h \in {\cal H}} {\hat \varepsilon}(h)\]</span> 对上述公式的直观理解就是：从假设类中选取一个假设，使得训练误差最小。我们这里用了<span class="math inline">\(\hat{h}\)</span>表示估计，因为毕竟不可能得到最好的假设，只能得到对这个最好的假设的估计。</p><p>但这仍然不是目标，我们的目标是使得<strong>泛化误差 Generalization Error</strong>最小化，也即新的数据集上分类错误的概率： <span class="math display">\[\varepsilon(h)=P_{(x,y) \sim {\cal D}}(h(x) \neq y)\]</span> 接下去，为了证明：</p><ul><li><p>（1）<span class="math inline">\({\hat \varepsilon} \approx \varepsilon\)</span>，训练误差近似于泛化误差（理解为，泛化误差和训练误差之间的差异存在上界）</p></li><li><p>（2）ERM输出的泛化误差<span class="math inline">\(\varepsilon({\hat h})\)</span>存在上界；</p></li></ul><p>我们引出两个引理：</p><ul><li><p>联合界引理（Union Bound）</p><blockquote><p><span class="math inline">\(A_1, A_2, \ldots , A_k\)</span>是k个事件，他们之间并不一定是独立分布的，有： <span class="math display">\[P(A_1 \cup \ldots \cup A_k) \leq P(A_1) + \dots + P(A_k)\]</span></p></blockquote></li><li><p>Hoeffding不等式（Hoeffding Inequality）</p><blockquote><p><span class="math inline">\(z_1, \ldots z_m\)</span>是m个iid（independent and identically distribution，独立同分布），他们都服从伯努利分布，<span class="math inline">\(P(z_i=1) = \phi\)</span>，那么对<span class="math inline">\(\phi\)</span>的估计： <span class="math display">\[{\hat \phi} = \frac{1}{m}\sum_{i=1}^m z_i\]</span> 于是，给定<span class="math inline">\(\gamma &gt; 0\)</span>，有： <span class="math display">\[P(|{\hat{\phi}} - \phi| &gt; \gamma) \leq 2 exp(-2\gamma^2m)\]</span></p></blockquote><p>Hoeffding不等式的直观解释就是，下图中的阴影面积，会有上界。</p><figure><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/19-01-06/image-20190106143941030.png" alt="image-20190106143941030"><figcaption>image-20190106143941030</figcaption></figure></li></ul><h3 id="一致收敛">一致收敛</h3><h3 id="uniform-conversions">Uniform Conversions</h3><p>对于某个<span class="math inline">\(h_j \in \cal{H}\)</span>，我们定义<span class="math inline">\(z_i = 1 \lbrace h_j(x^{(i)}) \neq y^{(i)}\rbrace \in \lbrace{}\)</span>为第i个样本被分类错误的指示函数的值，对于logistic而言，它服从伯努利分布。</p><p>那么：</p><ol type="1"><li>泛化误差：<span class="math inline">\(P(z_i=1) = \varepsilon(h_j)\)</span></li><li>训练误差：<span class="math inline">\({\hat{\varepsilon}}(h_j) = \frac{1}{m}\sum_{i=1}^m z_i\)</span></li></ol><p>根据Hoeffding不等式，我们能够得到： <span class="math display">\[P(|{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| &gt; \gamma) \leq 2e^{-2\gamma^2m}\]</span> 接着，我们定义训练误差和泛化误差之间的差大于<span class="math inline">\(\gamma\)</span>（<span class="math inline">\(|{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| &gt; \gamma\)</span>）为事件<span class="math inline">\(A_j\)</span>，根据以上结论，我们可知： <span class="math display">\[P(A_j) \leq 2e^{-2\gamma^2m}\]</span> 那么根据联合界引理： <span class="math display">\[\begin{array}{l}&amp; P(\exists h_j \in H, |{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| &gt; \gamma) \\\= &amp; P(A_1 \cup A_2 \cup \ldots \cup A_k) \\\\leq &amp; \sum_{i=1}^k P(A_i) \\\\leq &amp; \sum_{i=1}^k 2e^{-2\gamma^2m} \\\= &amp; 2ke^{-2\gamma^2m}\end{array}\]</span> 可以表述为：存在<span class="math inline">\(h_j\)</span>使<span class="math inline">\(|{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| &gt; \gamma\)</span>的概率<span class="math inline">\(\leq 2ke^{-2\gamma^2m}\)</span>。</p><p>等价于：不存在<span class="math inline">\(h_j\)</span>使<span class="math inline">\(|{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| &gt; \gamma\)</span>的概率<span class="math inline">\(\geq 1 - 2ke^{-2\gamma^2m}\)</span>。</p><p>等价于：<span class="math inline">\(\cal H\)</span>中任意的<span class="math inline">\(h_j\)</span>使得<span class="math inline">\(|{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| \leq \gamma\)</span>的概率<span class="math inline">\(\geq 1 - 2ke^{-2\gamma^2m}\)</span>。</p><p>我们将上面这个结论称之为<strong>一致收敛 Uniform Conversions</strong>，也就是说事实上，所有的假设，训练误差和泛化误差之间都存在上界。</p><h3 id="样本复杂度误差界以及偏差方差权衡">样本复杂度，误差界以及偏差方差权衡</h3><p>上面的结论，我们可以引出以下的一些推论：</p><h4 id="样本复杂度-sample-complexity">样本复杂度 Sample Complexity</h4><p>给定<span class="math inline">\(\gamma, \delta\)</span>，需要多大的训练集合（<span class="math inline">\(m\)</span>）？其中<span class="math inline">\(\delta\)</span>指的是泛化误差和训练误差之差大于<span class="math inline">\(\gamma\)</span>的概率。</p><p>我们知道，<span class="math inline">\(\delta \leq 2ke^{-2\gamma^2m}\)</span>，可求解： <span class="math display">\[m \geq \frac{1}{2 \gamma ^ 2} log(\frac{2k}{\delta})\]</span> 这个，也被称为样本复杂度（类似于时间复杂度），指的是，只要满足上面这个条件，任意<span class="math inline">\(h \in \cal H\)</span>，都能得到<span class="math inline">\(|{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| \leq \gamma\)</span></p><h4 id="误差界-error-bound">误差界 Error Bound</h4><p>给定<span class="math inline">\(\delta, m\)</span>时，我们会得到多大的误差上界<span class="math inline">\(\gamma\)</span>。</p><p>经过求解可以得到： <span class="math display">\[P(\forall h \in {\cal H}, |{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| \leq \sqrt{\frac{1}{2m}log(\frac{2k}{\delta})}) \geq 1 - \delta\]</span> 也就是误差上界是：<span class="math inline">\(\gamma = \sqrt{\frac{1}{2m}log(\frac{2k}{\delta})}\)</span>。</p><h3 id="偏差方差权衡-bias-variance-tradeoff">偏差方差权衡 Bias Variance Tradeoff</h3><p>我们定义： <span class="math display">\[\begin{align}{\hat h} &amp;= \mathop{\arg \min}_{h \in \cal H} {\hat \varepsilon}(h) \text{, 使得训练误差最小的h}&amp;\tag{1} \\\h^\ast &amp;= \mathop{\arg \min}_{h \in \cal H} \varepsilon(h) \text{, 使得泛化误差最小的h} \tag{2}\end{align}\]</span> 假如： <span class="math display">\[\forall h \in {\cal H}, |{\hat{\varepsilon}}(h_j) - \varepsilon(h_j)| \leq \gamma \tag{3}\]</span></p><p>那么： <span class="math display">\[\begin{align}\varepsilon(\hat h) &amp;\leq {\hat \varepsilon}({\hat h}) + \gamma, &amp;\text{derived from (3)}\\\&amp;\leq {\hat \varepsilon}(h^\ast) + \gamma, &amp;\text{derived from (1)}\\\&amp;\leq {\varepsilon(h^\ast)} + \gamma + \gamma, &amp;\text{ derived from (3)}\end{align}\]</span> 于是，我们得到如下定理：</p><p>给定大小为<span class="math inline">\(k\)</span>的假设集合<span class="math inline">\(\cal H\)</span>，给定<span class="math inline">\(m, \delta\)</span>，那么： <span class="math display">\[\varepsilon(\hat h) \leq \underbrace{(\min_{h \in {\cal H}}\varepsilon(h))}_{\varepsilon(h^\ast)} + 2 \underbrace{\sqrt{\frac{1}{2m}log(\frac{2k}{\delta})}}_{\gamma}\]</span> 的概率不低于<span class="math inline">\(1-\delta\)</span>。</p><p>可以想象，为了得到最佳的假设<span class="math inline">\(h^\ast\)</span>，我们尽可能增大<span class="math inline">\(\cal H\)</span>（能够减小<span class="math inline">\(\varepsilon(h^\ast)\)</span>），但随之而来的就是<span class="math inline">\(\gamma\)</span>的增大，所以需要在这两者之间进行权衡，我们指的就是<strong>偏差方差权衡 Bias Variance Tradeoff</strong>。</p><figure><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/19-01-06/image-20190106154201189.png" alt="image-20190106154201189"><figcaption>image-20190106154201189</figcaption></figure><p>由此，我们得到一个推论：</p><p>给定<span class="math inline">\(\delta, \gamma\)</span>，为了能够保证<span class="math inline">\(\varepsilon(\hat h) \leq (\min_{h \in {\cal H}}\varepsilon(h)) + 2\gamma\)</span>的概率不小于<span class="math inline">\(1-\delta\)</span>（ERM得到的假设的一般误差，与最佳假设的一般误差之间，差值不大于<span class="math inline">\(2\gamma\)</span>）</p><p>我们需要保证： <span class="math display">\[m \geq \frac{1}{2\gamma^2}log(\frac{2k}{\delta})\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;就线性分类模型而言，可以将其表示为： &lt;span class=&quot;math display&quot;&gt;\[
h_\theta(x)=g(\theta^Tx), \\\
g(z) = 1\lbrace z \geq 0 \rbrace
\]&lt;/span&gt; 其中，训练集表示为： &lt;spa
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="ERM" scheme="https://jiacheng-pan.github.io/wiki/tags/ERM/"/>
    
      <category term="经验风险" scheme="https://jiacheng-pan.github.io/wiki/tags/%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9/"/>
    
  </entry>
  
  <entry>
    <title>SVM（四）非线性决策边界</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/07/25/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/13-SVM%EF%BC%88%E5%9B%9B%EF%BC%89%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/07/25/吴恩达·机器学习/13-SVM（四）非线性决策边界/</id>
    <published>2018-07-25T05:53:00.000Z</published>
    <updated>2019-01-20T14:21:01.716Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-25/83102162.jpg"></p><p>当数据中存在异常点时，比如上述的情况，导致原先可以用直线a分割的数据现在不得不用b来进行，以保证完美的分割。由此我们引出了<strong>非线性决策边界</strong>（<em>non-linear decision boundaries</em>）来解决这样的问题。</p><p>观察原SVM问题的目标： <span class="math display">\[\min_{w, b} \frac{1}{2}||w|^2 \\\\text{ s.t. }y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq 1, i=1,\ldots,m\]</span> 我们为原公式增加惩罚项，对不同的数据点增加不同的惩罚，使得所有样本能够更好地分割： <span class="math display">\[\min_{w, b} \frac{1}{2}||w|^2+c\sum_{i=1}^m\xi_i, \xi_i\geq0\]</span> 使得 <span class="math display">\[y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq 1-\xi_i, i=1,\ldots,m\]</span> 注意到，我们之前认为$ y^{(i)} (w^T x^{(i)}+b) 1 $是分类正确的，在这里我们允许一部分样本小于1，也就是说明我们允许了一部分样本分类错误。</p><p>构建拉格朗日算子： <span class="math display">\[{\cal L}(w, b, \xi, \alpha, \gamma) = \frac{1}{2}||w|^2+c\sum_i\xi_i-\sum_i^m\alpha_i(y^{(i)} (w^T \cdot x^{(i)}+b)-1+\xi_i)-\sum_i^m\gamma_i\xi_i\]</span> 对偶： <span class="math display">\[\max W(\alpha) = \sum_{i=1}\alpha_i-\frac{1}{2}\sum_{i=1}\sum_{j=1}\alpha_i\alpha_jy_iy_j\langle x_i \cdot x_j \rangle\]</span> 跟原先的SVM问题的唯一区别在于其限制条件为： <span class="math display">\[\sum_{i=1}^my^{(i)}\alpha_i=0 \\\0 \leq \alpha_i \leq c\]</span> 其收敛条件：</p><ul><li><p>对于大部分数据点：</p><p><span class="math display">\[\alpha_i=0 \Rightarrow y^{(i)} (w^T \cdot x^{(i)}+b) \geq 1\]</span></p></li><li><p>对于异常点：</p><p><span class="math display">\[\alpha_i = c\]</span></p></li><li><p>对于最近点：</p><p><span class="math display">\[0&lt;\alpha_i&lt;c\]</span></p></li></ul><h3 id="坐标上升法coordinate-ascent">坐标上升法（Coordinate Ascent）</h3><p>考虑优化问题： <span class="math display">\[\max W(\alpha_1, \alpha_2, ..., \alpha_m)\]</span> 不考虑约束条件，</p><p>重复 {</p><p>​ For i = 1 to m: <span class="math display">\[\alpha_i := \arg \max_{\hat{\alpha}_i} W(\alpha_1,\ldots,{\hat{\alpha}}_i,\ldots,\alpha_m)\]</span> } 直到收敛；</p><p>这个算法，可以认为是执行了以下这个过程（以m=2为例）：</p><figure><img src="http://s3.sinaimg.cn/middle/b09d46024e1a2f5cf49c2&amp;690" alt="坐标上升法，不断沿着坐标轴方向前进"><figcaption>坐标上升法，不断沿着坐标轴方向前进</figcaption></figure><h3 id="顺序最小优化算法sequential-minimal-optimization-smo">顺序最小优化算法（Sequential minimal optimization, SMO）</h3><p>顺序最小优化算法的基本理念就是在坐标上升法的基础上，改成一次性优化其中两个<span class="math inline">\(\alpha\)</span>，而固定其他的<span class="math inline">\(m-2\)</span>个<span class="math inline">\(\alpha\)</span>。</p><p>假如我们更新<span class="math inline">\(\alpha_1, \alpha_2\)</span>：</p><p>因为在之前我们提到: <span class="math display">\[\sum_i^m \alpha_iy^{(i)}=0\]</span> 于是有： <span class="math display">\[\alpha_1 y^{(1)}+\alpha_2 y^{(2)} = -\sum_{i=3}^m\alpha_iy^{(i)}= \zeta\]</span> 那么 <span class="math display">\[\alpha_1=\frac{\zeta-\alpha_2y^{(2)}}{y^{(1)}}\]</span></p><p><span class="math display">\[W(\alpha_1, \alpha_2, ..., \alpha_m) = W(\frac{\zeta-\alpha_2y^{(2)}}{y^{(1)}}, \alpha_2, ..., \alpha_m)\]</span></p><p>在非线性决策边界优化问题中，其实<span class="math inline">\(W\)</span>是一个关于<span class="math inline">\(\alpha_i\)</span>的二次函数，固定其他<span class="math inline">\(\alpha\)</span>之后，<span class="math inline">\(W\)</span>函数就可以被简化为： <span class="math display">\[a\alpha_2^2+b \alpha_2 + c\]</span> 很容易就能求解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-25/83102162.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;当数据中存在异常点时，比如上述的情况，导致原先可以用直线a分割的数据现在不得不用b来进行，以保证
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="https://jiacheng-pan.github.io/wiki/tags/SVM/"/>
    
      <category term="非线性决策边界" scheme="https://jiacheng-pan.github.io/wiki/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C/"/>
    
      <category term="坐标上升法" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%9D%90%E6%A0%87%E4%B8%8A%E5%8D%87%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>SVM（三）核函数</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/07/22/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/12-SVM%EF%BC%88%E4%B8%89%EF%BC%89%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/07/22/吴恩达·机器学习/12-SVM（三）核函数/</id>
    <published>2018-07-22T13:33:00.000Z</published>
    <updated>2019-01-21T07:29:17.147Z</updated>
    
    <content type="html"><![CDATA[<p>在SVM(二)中，我们看到了如下的表示形式： <span class="math display">\[W(\alpha)=\sum_{i=1}\alpha_i-\frac{1}{2}\sum_{i=1}\sum_{j=1}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j)\]</span> 这里，内积<span class="math inline">\((x_i \cdot x_j)\)</span>就是最简单的核函数的形式。一般核函数会被写成<span class="math inline">\(\langle x^{(i)}, x^{(j)} \rangle\)</span>的形式。</p><p>有时候，我们会将一些特征转换到高维空间上，就像我们在之前的过拟合&amp;局部加权回归中提到的，比如特征<span class="math inline">\(x\)</span>表示的是房屋面积，我们需要预测房子是否会在6个月内被卖出，我们有时候会将这个特征映射成如下的形式： <span class="math display">\[x \rightarrow \begin{bmatrix}x \\\x^2 \\\x^3 \\\x^4\end{bmatrix} = \phi(x)\]</span> 原先的特征的内积形式<span class="math inline">\(\langle x^{(i)}, x^{(j)} \rangle\)</span>会被写成<span class="math inline">\(\langle \phi(x^{(i)}), \phi(x^{(j)}) \rangle\)</span>，而且往往<span class="math inline">\(\phi(x)\)</span>会有很高的维度。因为在很多情况下，计算<span class="math inline">\(\phi(x)\)</span>会有很高的代价，或者表示<span class="math inline">\(\phi(x)\)</span>需要很高的代价，但是光是计算内核则可能代价较小。</p><p>比如：假如有两个输入：<span class="math inline">\(x, z \in \Bbb R^n\)</span>，核函数被定义为： <span class="math display">\[\begin{align}k(x, z) = (x^T z)^2 &amp;= (\sum_{i=1}^nx_iz_i)(\sum_{j=1}^nx_jz_j) \\\&amp;=\sum_{i=1}^n\sum_{j=1}^n(x_ix_j)(z_iz_j) \\\&amp;= \phi(x)^T\phi(z)\end{align}\]</span> 假如需要表示成高维向量，那么<span class="math inline">\(\phi(x)\)</span>是一个<span class="math inline">\(n \times n\)</span>维的向量，如果<span class="math inline">\(n = 3\)</span>： <span class="math display">\[\phi(x) = \begin{bmatrix}x_1x_1 \\\x_1x_2 \\\x_1x_3 \\\x_2x_1 \\\\vdots \\\x_3x_3\end{bmatrix}\]</span> 所以，计算<span class="math inline">\(\phi(x)\)</span>的时间复杂度就达到了<span class="math inline">\(O(n^2)\)</span>，而计算核函数仅仅需要计算<span class="math inline">\(x^Tz\)</span>，复杂度为<span class="math inline">\(O(n)\)</span>。</p><p>接下去我们为这个核函数增加常数项： <span class="math display">\[k(x,z)=(x^Tz+c)^2\]</span> 那么： <span class="math display">\[\phi(x) = \begin{bmatrix}x_1x_1 \\\x_1x_2 \\\x_1x_3 \\\x_2x_1 \\\\vdots \\\x_3x_3 \\\\sqrt{2c}x_1 \\\\sqrt{2c}x_2 \\\\sqrt{2c}x_3 \\\c\end{bmatrix}\]</span> 更一般的： <span class="math display">\[k(x, z)=(x^Tz+c)^d\]</span></p><p>有了核函数，即可替换SVM中的内积<span class="math inline">\(\langle x^{(i)}, x^{(j)} \rangle\)</span>，比如常用的高斯核： <span class="math display">\[k(x,z)=\exp(-\frac{||x-z||^2}{2\sigma^2})\]</span> 有了核函数，相当于把数据从原始空间转换到了高位空间，很多数据，在一维空间往往是线性不可分的，但是到了高维空间会变成可分的：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-23/7489488.jpg"></p><h2 id="核函数的合法性">核函数的合法性</h2><p>如何判断一个核函数是合法的呢？判断依据是：是否存在函数<span class="math inline">\(\phi\)</span>，使得<span class="math inline">\(k(x,z)\)</span>能够被写成<span class="math inline">\(\langle \phi(x), \phi(z) \rangle\)</span>。</p><blockquote><p><strong>定理</strong>：如果核函数合法，那么其对应的核矩阵（kernel matrix）是半正定的。</p></blockquote><p>核矩阵指的是矩阵<span class="math inline">\(K \in \Bbb R^{m\times m}\)</span>，其中<span class="math inline">\(K_{ij}=k(x^{(i)}, x^{(j)})\)</span>。半正定的意思是，对于任意向量<span class="math inline">\(z\)</span>，都存在<span class="math inline">\(z^TKz \geq 0\)</span>，证明如下： <span class="math display">\[\begin{align}z^TKz &amp;= \sum_i\sum_jz_iK_{ij}z_j \\\ &amp;= \sum_i\sum_jz_i\phi_(x^{(i)})^T\phi_(x^{(j)})z_j \\\ &amp;= \sum_i\sum_jz_i\cdot \sum_k\phi_(x^{(i)})_k\underbrace{\phi_(x^{(j)})_k}_{向量第k项} \cdot z_j \\\ &amp;= \sum_k\sum_i\sum_jz_i\cdot \phi_(x^{(i)})_k\phi_(x^{(j)})_k \cdot z_j \\\ &amp;= \sum_k(\sum_iz_i\phi(x^{(i)}))^2 \geq 0\end{align}\]</span> 事实上，上面的定理的逆命题也一样成立，总结起来：</p><blockquote><p><strong>Merce定理</strong>：给定核函数<span class="math inline">\(k(x, z)\)</span>，那么<span class="math inline">\(k(x, z)\)</span>合法（也即<span class="math inline">\(\exists \phi, k(x,z)=\phi(x)^T\phi(z)\)</span>），当且仅当，对所有的<span class="math inline">\(\lbrace x^{(1)}, \ldots, x^{(m)} \rbrace\)</span>，核矩阵<span class="math inline">\(K \in \Bbb R^{m\times m}\)</span>是一个对称的半正定矩阵。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在SVM(二)中，我们看到了如下的表示形式： &lt;span class=&quot;math display&quot;&gt;\[
W(\alpha)=\sum_{i=1}\alpha_i-\frac{1}{2}\sum_{i=1}\sum_{j=1}\alpha_i\alpha_jy_iy_j(x
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="https://jiacheng-pan.github.io/wiki/tags/SVM/"/>
    
      <category term="核函数" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>SVM（二）最优间隔分类器</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/07/03/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/11-SVM%EF%BC%88%E4%BA%8C%EF%BC%89%E6%9C%80%E4%BC%98%E9%97%B4%E9%9A%94%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/07/03/吴恩达·机器学习/11-SVM（二）最优间隔分类器/</id>
    <published>2018-07-03T08:27:00.000Z</published>
    <updated>2019-01-21T07:29:06.672Z</updated>
    
    <content type="html"><![CDATA[<p><strong>最优间隔分类器</strong>（<em>Optimal Margin Classifier</em>）。其目标是使得最小几何间隔最大化（SVM（一）概念）： <span class="math display">\[\text{目标(1):} \\\\max_{w, b} \gamma \\\\text{ s.t. } y^{(i)} \cdot ((\frac{w}{||w||})^T \cdot x^{(i)}+\frac{b}{||w||}) \geq \gamma, i=1,\ldots,n\]</span> 我们知道，<span class="math inline">\(\hat{\gamma} = \frac{\gamma}{||w||}\)</span>，所以上面的目标可以等同于： <span class="math display">\[\text{目标(2):} \\\\max_{w, b} \frac{\hat{\gamma}}{||w||} \\\\text{ s.t. }y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq \hat{\gamma}, i=1,\ldots,n\]</span> 为了最大化上述值，我们有两种策略。</p><ol type="1"><li>增大<span class="math inline">\(\hat{\gamma}\)</span></li><li>减小<span class="math inline">\(||w||\)</span></li></ol><p>针对第一种可能，我们要证明其无效性。假如，我们增大<span class="math inline">\(\hat{\gamma}\)</span>到<span class="math inline">\({\hat{\gamma}}_1 := \lambda {\hat{\gamma}}\)</span>，因为<span class="math inline">\(\hat{\gamma}=y(w^Tx+b)\)</span>，可以视作<span class="math inline">\(w_1:=\lambda w, b_1 = \lambda b\)</span>。所以，此时 <span class="math display">\[\frac{\hat{\gamma_1}}{||w_1||}=\frac{\lambda \hat{\gamma}}{||\lambda w||} = \frac{\hat{\gamma}}{||w||} \\\\]</span> 没有发生任何改变，所以第一条策略不可行。于是，我们可以固定<span class="math inline">\(\hat{\gamma}=1\)</span></p><p>此时，上述目标(2)可以表述成： <span class="math display">\[\text{目标(3):} \\\\min_{w, b} \frac{1}{2}||w||^2 \\\\text{ s.t. }y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq 1, i=1,\ldots,n\]</span></p><p>因为最小化<span class="math inline">\(||w||\)</span>和最小化<span class="math inline">\(\frac{1}{2}||w||^2\)</span>是一致的。</p><h2 id="拉格朗日乘子法lagrange-multiplier">拉格朗日乘子法（Lagrange Multiplier）</h2><p>为了解决上述的<strong>凸优化问题</strong>，我们引入拉格朗日乘子法<em>Lagrange Multiplier</em>来解决这个问题。</p><p>我们首先来看看<strong>凸优化问题</strong>的定义： <span class="math display">\[\min_wf(w) \\\\text{s.t. }g_i(w) \leq 0, h_i(w) =0\]</span> 构建拉格朗日乘子： <span class="math display">\[{\cal L}(w, \alpha, \beta) = f(w)+\sum_i\alpha_ig_i(w)+\sum_i\beta_ih_i(w)\]</span> 定义： <span class="math display">\[\theta_p(w) = \max_{\alpha_i&gt;0, \beta}{\cal L}(w, \alpha, \beta)\]</span> 观察<span class="math inline">\(\theta_p(w)\)</span>：</p><ol type="1"><li>如果<span class="math inline">\(g_i(w)&gt;0\)</span>，那么<span class="math inline">\(\theta_p(w)=+\infty\)</span>（因为<span class="math inline">\(\alpha\)</span>可以取任意大值）。</li><li>如果<span class="math inline">\(h_i(w) \neq 0\)</span>，那么<span class="math inline">\(\theta_p(w)=+\infty\)</span>（因为<span class="math inline">\(\beta\)</span>可以取<span class="math inline">\(+\infty/-\infty\)</span>）。</li></ol><p>所以，在满足约束的情况下，<span class="math inline">\(\theta_p(w)=f(w)\)</span>，<span class="math inline">\(\min_w \theta_p(w)=\min_w f(w)\)</span>，因为使得<span class="math inline">\({\cal L}(w, \alpha, \beta)\)</span>最大的方法，就是其他所有项全是0。那么，可以得出这样的结论： <span class="math display">\[\theta_p(w)=\begin{cases}f(w), &amp;\text{满足约束} \\\\infty, &amp;\text{不满足约束}\end{cases}\]</span> 因此，在满足条件的情况下，<span class="math inline">\(\min_w\theta_p(w)\)</span>等价于<span class="math inline">\(min_wf(w)\)</span>。</p><p>我们将最优间隔分类器的目标重新表示一下： <span class="math display">\[p^\ast =\min_{w, b}\max_\alpha {\cal L(w, \alpha, b)} \\\{\cal L}(w, \alpha, b) = \frac{1}{2}||w||^2+\sum_i\alpha_i(1-y^{(i)}(w^T x^{(i)}+b))\]</span> 其中，直接忽略了<span class="math inline">\(h_i(w)=0\)</span>的约束，而<span class="math inline">\(g_i(w,b)=1-y^{(i)}(w^Tx^{(i)}+b) \leq 0, f(w)=\frac{1}{2}||w||^2\)</span></p><h2 id="对偶问题dual-problem">对偶问题（Dual Problem）</h2><p>一般来说，将原始问题转化成对偶问题来求解。一是因为对偶问题往往比较容易求解，二是因为对偶问题引入了核函数，方便推广到非线性分类的情况。</p><p>我们看到，之前的原始问题，是 <span class="math display">\[p^\ast =\min_{w, b}\max_\alpha {\cal L}(w, \alpha, b)\]</span></p><p>那么，定义其对偶问题：</p><p><span class="math display">\[l^\ast =\max_\alpha\min_{w,b}{\cal L}(w, \alpha, b)\]</span></p><p>接下去，我们求解对偶问题：</p><p>先求解<span class="math inline">\(\min_{w,b}{\cal L}(w, \alpha, b)\)</span>：</p><p>分别求偏导，使其等于0，导出最小值： <span class="math display">\[\begin{align}&amp; \nabla_w{\cal L}(w, \alpha, b) =w-\sum_{i=1}\alpha_iy^{(i)}x^{(i)}=0 \\\&amp; \nabla_b{\cal L}(w, \alpha, b) =\sum_{i=1}\alpha_iy^{(i)}=0\end{align}\]</span></p><p>得到：</p><p><span class="math display">\[w =\sum_{i=1}\alpha_iy^{(i)}x^{(i)} \\\\sum_{i=1}\alpha_iy^{(i)} = 0\]</span></p><p>代入<span class="math inline">\({\cal L}(w, \alpha, b)\)</span>，就可以得到最小值：</p><p><span class="math display">\[\begin{align}{\cal L}(w, \alpha, b) &amp;= \frac{1}{2}||w||^2+\sum_i\alpha_i(1-y^{(i)}(w^T x^{(i)}+b)) \\\\min_{w, b}{\cal L}(w, \alpha, b) &amp;=\underbrace{\sum_{i=1}\alpha_i-\frac{1}{2}\sum_{i=1}\sum_{j=1}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j)}_{W(\alpha)}\end{align}\]</span></p><p>于是，我们的对偶问题简化到了对<span class="math inline">\(W(\alpha)\)</span>最大化： <span class="math display">\[\max_\alpha W(\alpha) \\\\text{s.t. }\alpha_i \geq 0, \sum_iy_i\alpha_i=0\]</span></p><p>假设，我们解得的对偶问题的解为：<span class="math inline">\(\alpha^\ast =[\alpha_1^\ast ,\alpha_2^\ast , \ldots, \alpha_m^\ast ]\)</span>，那么最终原始问题的解可以表示成：</p><p><span class="math display">\[w^\ast =\sum_{i=1}\alpha_i^\ast y^{(i)}x^{(i)}\]</span></p><p>在原始问题中，还有<span class="math inline">\(b\)</span>未得到解决。我们先来观察一下约束项： <span class="math display">\[g_i(w,b)=1-y{(i)}(w^Tx^{(i)}+b) \leq 0\]</span> <img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-4/9336603.jpg"></p><p>我们知道，在数据中，只有少数的几个数据点，他们的函数距离为1（最小），也即<span class="math inline">\(g_i(w,b)=0\)</span>，如图所示。</p><p>在整个数据集中，只有这些数据点对约束超平面起了作用，这些数据点被称为支持向量（<em>support vector</em>），其对应的<span class="math inline">\(\alpha_i^\ast \neq 0\)</span>，而其他不是支持向量的数据点，没有对约束超平面起作用，其<span class="math inline">\(\alpha_i^\ast =0\)</span>。</p><p>此时，我们已经得到了$w<sup><span class="math inline">\(，而\)</span>b</sup><span class="math inline">\(的计算如下，找到一个数据点，其\)</span>_j^0$(也就是支持向量，其函数间隔为1)，我们就能得到：</p><p><span class="math display">\[y^{(j)}(w^{*T}x^{(j)}+b^\ast )=1\Rightarrowb^\ast =y^{(j)}-\sum_{i=1}\alpha_i^\ast y^{(i)}(x^{(i)} \cdot x^{(j)})\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;最优间隔分类器&lt;/strong&gt;（&lt;em&gt;Optimal Margin Classifier&lt;/em&gt;）。其目标是使得最小几何间隔最大化（SVM（一）概念）： &lt;span class=&quot;math display&quot;&gt;\[
\text{目标(1):} \\\
\
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="https://jiacheng-pan.github.io/wiki/tags/SVM/"/>
    
      <category term="最优间隔分类器" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%80%E4%BC%98%E9%97%B4%E9%9A%94%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    
      <category term="拉格朗日乘子法" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>SVM（一）概念</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/07/02/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/10-SVM%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E5%BF%B5/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/07/02/吴恩达·机器学习/10-SVM（一）概念/</id>
    <published>2018-07-02T08:27:00.000Z</published>
    <updated>2019-01-20T14:21:01.715Z</updated>
    
    <content type="html"><![CDATA[<p><em>SVM</em>，指的是支持向量机（<em>support vector machines</em>）。</p><p>支持向量机，假设数据是线性可分的，那么我们就能找到一个超平面，将数据分成两类。但是一旦线性可分，我们就可能找到无数的超平面，都可以将数据分成两类：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-3/83131877.jpg"></p><p>但是很明显，上图中虽然<em>a, c</em>都对数据进行了有效的分割。但很明显，都不如<em>b</em>分割的好。</p><p>我们可以用“间隔”这个概念来定义这个超平面（在二维上是线）对数据的分割优劣。在分类正确的情况下，间隔越大，我们认为对数据的分类越好。</p><p>我们的目标是得到数据的分类：<span class="math inline">\(y \in \lbrace -1, +1 \rbrace\)</span>。</p><p>这个超平面，则可以表示成<span class="math inline">\(w^Tx+b\)</span>，其中<span class="math inline">\(w=[\theta_1, \ldots, \theta_n]^T, b=\theta_0\)</span>。这个超平面可以表达成一个<span class="math inline">\(n+1\)</span>维向量。</p><p>判别函数： <span class="math display">\[g(z)=\begin{cases}+1, &amp; \text{如果$z\geq0$} \\\-1, &amp; \text{otherwise}\end{cases}\]</span> 假设则可以表示成：<span class="math inline">\(h_{w,b}(x)=g(w^Tx+b)\)</span></p><h2 id="间隔">间隔</h2><h3 id="函数间隔functional-margin">函数间隔（functional margin）</h3><p>某个超平面<span class="math inline">\((w,b)\)</span>和训练样本<span class="math inline">\((x^{(i)}, y^{(i)})\)</span>之间的函数间隔被表示成： <span class="math display">\[\hat{\gamma}^{(i)}=y^{(i)}(w^Tx^{(i)}+b)\]</span> 于是，我们可以知道：</p><ol type="1"><li>当<span class="math inline">\(y^{(i)}=1\)</span>，于是我们想获得更大的函数间隔（这是我们的目标），就需要使得<span class="math inline">\(w^Tx^{(i)}+b \gg 0\)</span></li><li>相反，当<span class="math inline">\(y^{(i)}=-1\)</span>，我们想获得更大的函数间隔，就需要使得<span class="math inline">\(w^Tx^{(i)}+b \ll 0\)</span></li></ol><p>并且，很明显，只有当函数间隔<span class="math inline">\(\hat{\gamma}&gt;0\)</span>时，分类结果是正确的。</p><p>最后，超平面与数据集<span class="math inline">\(\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots \rbrace\)</span>之间的函数间隔，被定义为所有函数间隔中的最小值： <span class="math display">\[\hat{\gamma}=\min_i\hat{\gamma}^{(i)}\]</span></p><h3 id="几何间隔geometric-margin">几何间隔（geometric margin）</h3><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-3/34659777.jpg"></p><p>从点<span class="math inline">\((x^{(i)}, y^{(i)})\)</span>出发，对超平面做垂线，得到点D，我们知道他们之间的距离，就是该超平面到数据点<span class="math inline">\((x^{(i)}, y^{(i)})\)</span>的几何间隔。</p><p>经过推导，D的坐标可以表示为： <span class="math display">\[x^{(i)}-\gamma^{(i)}\frac{w}{||w||}\]</span> 又因为，D在超平面<span class="math inline">\(w^Tx+b=0\)</span>上，所以： <span class="math display">\[\begin{align}&amp; w^T(x^{(i)}-\gamma^{(i)}\frac{w}{||w||})+b=0 \\\&amp; \Rightarrow w^Tx^{(i)}+b=\gamma^{(i)} \cdot \frac{w^Tw}{||w||}=\gamma^{(i)} \cdot ||w|| \\\&amp; \Rightarrow \gamma^{(i)}=(\frac{w}{||w||})^T \cdot x^{(i)}+\frac{b}{||w||}\end{align}\]</span> 加上正负分类的判断： <span class="math display">\[\gamma^{(i)}=y^{(i)} \cdot ((\frac{w}{||w||})^T \cdot x^{(i)}+\frac{b}{||w||})\]</span> 我们可以看到，几何间隔跟函数间隔之间存在如下的关系： <span class="math display">\[\hat{\gamma}^{(i)} = \frac{\gamma^{(i)}}{||w||}\]</span></p><p>同样的，超平面与数据集<span class="math inline">\(\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots \rbrace\)</span>之间的几何间隔，被定义为所有几何间隔中的最小值： <span class="math display">\[\gamma=\min_i\gamma^{(i)}\]</span> 最后，我们导出<strong>最优间隔分类器</strong>（<em>Optimal Margin Classifier</em>）问题：选择<span class="math inline">\(w, b\)</span>，最大化<span class="math inline">\(\gamma\)</span>，同时满足<span class="math inline">\(\forall(x^{(i)}, y^{(i)})\)</span>，$ y^{(i)} (()^T x^{(i)}+) $（所有数据点的几何间隔都大于该最小几何间隔）。</p><p>目前为止，已经是SVM问题的一个简化版本。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;SVM&lt;/em&gt;，指的是支持向量机（&lt;em&gt;support vector machines&lt;/em&gt;）。&lt;/p&gt;
&lt;p&gt;支持向量机，假设数据是线性可分的，那么我们就能找到一个超平面，将数据分成两类。但是一旦线性可分，我们就可能找到无数的超平面，都可以将数据分成两类：
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="https://jiacheng-pan.github.io/wiki/tags/SVM/"/>
    
      <category term="函数间隔" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%87%BD%E6%95%B0%E9%97%B4%E9%9A%94/"/>
    
      <category term="几何间隔" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94/"/>
    
  </entry>
  
  <entry>
    <title>生成学习算法的例子</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/06/29/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/09-%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E4%BE%8B%E5%AD%90/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/06/29/吴恩达·机器学习/09-生成学习算法的例子/</id>
    <published>2018-06-29T13:10:00.000Z</published>
    <updated>2019-01-21T07:28:52.424Z</updated>
    
    <content type="html"><![CDATA[<h2 id="例一高斯判别分析和logistic函数">例一：高斯判别分析和logistic函数</h2><p>我们来看一个例子，对于一个高斯判别分析问题，根据贝叶斯： <span class="math display">\[\begin{align}p(y=1|x) &amp;= \frac{p(x|y=1)p(y=1)}{p(x)} \\\&amp;= \frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)}\end{align}\]</span> 在这里，我们提出几个假设：</p><ol type="1"><li><span class="math inline">\(p(y)\)</span>是均匀分布的，也就是<span class="math inline">\(p(y=1)=p(y=0)\)</span></li><li><span class="math inline">\(x\)</span>的条件概率分布（<span class="math inline">\(p(x|y=0)\)</span>和<span class="math inline">\(p(x|y=1)\)</span>）满足高斯分布。</li></ol><p>考虑二维的情况：</p><figure><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/2018-06-30-084350.png" alt="image-20180630164349595"><figcaption>image-20180630164349595</figcaption></figure><p>蓝色数据表达的是<span class="math inline">\(p(x|y=0)\)</span>的分布，红色数据表达的是<span class="math inline">\(p(x|y=1)\)</span>的分布，两条蓝色和红色的曲线分别是它们的概率密度曲线。</p><p>而灰色的曲线则表示了<span class="math inline">\(p(y=1|x)\)</span>的概率密度曲线。</p><p>假设<span class="math inline">\(p(x|y=0) \sim N(\mu_0, \sigma_0)\)</span>，<span class="math inline">\(p(x|y=1) \sim N(\mu_1, \sigma_1)\)</span>，而<span class="math inline">\(p(y)\)</span>均匀分布那么： <span class="math display">\[\begin{align}p(y=1|x) &amp;= \frac{N(\mu_0,\sigma_0)}{N(\mu_0,\sigma_0)+N(\mu_1,\sigma_1)} \\\ &amp;= \cdots \\\&amp;= \frac{1}{1+\frac{\sigma_0}{\sigma_1}exp(2\sigma_1^2(x-\mu_0)^2-2\sigma_0^2(x-\mu_1)^2}\end{align}\]</span> 事实上，这条曲线跟我们之前见过的<em>logistic</em>曲线非常像，特别是当我们假设<span class="math inline">\(\sigma_0=\sigma_1\)</span>的时候，就是一条<em>logistic</em>曲线。</p><p>我们有如下的推广结论： <span class="math display">\[{\begin{cases}p(x|y=1) \sim Exp Family(\eta_1) \\\p(x|y=0) \sim Exp Family(\eta_0)\end{cases}} \Rightarrow p(y=1|x)是logistic函数\]</span> 但这个命题的逆命题并不成立，故而我们知道，<em>logistic</em>所需要的假设更少（无需假设<span class="math inline">\(x\)</span>的条件概率分布），鲁棒性更强。而生成函数因为对数据的分布做出了假设，所以需要的数据量会少于<em>logstic</em>回归，我们需要在两者之间进行权衡。</p><h2 id="例二垃圾邮件分类1">例二：垃圾邮件分类（1）</h2><p>这里我们会用朴素贝叶斯（Naive Bayes）来解决垃圾邮件分类问题（<span class="math inline">\(y\in \lbrace 0, 1 \rbrace\)</span>）。</p><p>首先对邮件进行建模，生成特征向量如下： <span class="math display">\[x=\begin{bmatrix}0 \\\0 \\\0 \\\\vdots \\\1 \\\\vdots\end{bmatrix}\begin{matrix}a \\\advark \\\ausworth \\\\vdots \\\buy \\\\vdots\end{matrix}\]</span> 这是一个类似于词频向量的特征向量，我们有一个50000个词的词典，如果邮件中出现了某个词汇，那么其在向量中对应的位置就会被标记为1，否则为0。</p><p>我们的目标是获取，垃圾邮件和非垃圾邮件的特征分别是怎么样的，也即<span class="math inline">\(p(x|y)\)</span>。<span class="math inline">\(x={\lbrace 0, 1 \rbrace}^n, y \in \lbrace 0, 1 \rbrace\)</span>，这里我们的词典中词汇数量是50000，所以<span class="math inline">\(n=50000\)</span>，特征向量<span class="math inline">\(x\)</span>会有<span class="math inline">\(2^{50000}\)</span>种可能，需要<span class="math inline">\(2^{50000}-1\)</span>个参数。</p><p>我们假设<span class="math inline">\(x_i|y\)</span>之间相互独立(虽然假设各个单词的出现概率相互独立不是很合理，但是即便这样，朴素贝叶斯的效果依旧不错)，根据朴素贝叶斯，我们得到： <span class="math display">\[p(x_1, x_2, \ldots, x_{50000}|y)=p(x_1|y)p(x_2|y) \cdots p(x_{50000}|y)\]</span> 单独观察<span class="math inline">\(p(x_j|y=1)​\)</span>： <span class="math display">\[p(x_j|y=1) = p(x_j=1|y=1)^{x_j}p(x_j=0|y=1)^{1-x_j}\]</span> 给定三个参数： <span class="math display">\[\begin{align}\phi_{j|y=1} &amp;= p(x_j=1|y=1) \\\\phi_{j|y=0} &amp;= p(x_j=1|y=0) \\\\phi_y &amp;= p(y = 1)\end{align}\]</span> 故： <span class="math display">\[\begin{align}p(x_j|y=1) &amp;= \phi_{j|y=1}^{x_j}(\phi_y - \phi_{j|y=1})^{1-x_j}\\\p(x_j|y=0) &amp;= \phi_{j|y=0}^{x_j}(1-\phi_y + \phi_{j|y=0})^{1-x_j} \\\p(x_j|y) &amp;= p(x_j|y=1)^yp(x_j|y=0)^{1-y} \\\p(y) &amp;= \phi_y^y(1-\phi_y)^{1-y}\end{align}\]</span> 按照上个博客生成学习算法的概念中所述，我们会选用联合概率分布的极大似然来导出最优解： <span class="math display">\[l(\phi_y,\phi_{j|y=1},\phi_{j|y=0}=\prod_{i=1}^mp(x^{(i)},y^{(i)})=\prod_{i=1}^mp(x^{(i)}|y^{(i)})p(y^{(i)})\]</span></p><p>可以解得： <span class="math display">\[\begin{align}\phi_{j|y=1} &amp;= \frac{\sum_{i=1}^m1\lbrace x_j{(i)}=1, y^{(i)}=1 \rbrace}{\sum_{i=1}^m1\lbrace y^{(i)}=1 \rbrace}  = \frac{统计所有包含词语j的垃圾邮件的数量}{垃圾邮件的总数}\\\\phi_{j|y=0} &amp;= \frac{\sum_{i=1}^m1\lbrace x_j{(i)}=1, y^{(i)}=0 \rbrace}{\sum_{i=1}^m1\lbrace y^{(i)}=0 \rbrace} = \frac{统计所有包含词语j的非垃圾邮件的数量}{非垃圾邮件的总数} \\\\phi_y &amp;= \frac{\sum_{i=1}^m1\lbrace y^{(i)}=1 \rbrace}{m} = \frac{垃圾邮件的数量}{邮件的总数}\end{align}\]</span> 通过以上的公式，我们已经可以完全推得<span class="math inline">\(p(x_1, x_2, \ldots, x_{50000}|y)\)</span>。</p><h3 id="laplace平滑">Laplace平滑</h3><p>假设，训练集中，我们重来没有碰到过"<em>NIPS</em>"这个词汇，假设我们词典中包含这个词，位置是30000，也就是说： <span class="math display">\[\begin{align}p(x_{30000}=1|y=1) &amp;= 0\\\p(x_{30000}=0|y=1) &amp;= 0\end{align} \\\\Downarrow \\\p(x|y=1) =\prod_{i=1}^{50000}p(x_i|y=1)=0 \\\p(x|y=0) =\prod_{i=1}^{50000}p(x_i|y=0)=0\]</span> 故而在分类垃圾邮件时： <span class="math display">\[\begin{align}p(y=1|x) &amp;= \frac{p(x|y=1)p(y=1)}{p(x)} \\\&amp;=\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)} \\\&amp;= \frac{0}{0+0}\end{align}\]</span> 所以，我们提出<span class="math inline">\(p(x_{30000}=1|y=1) = 0\)</span>这样的假设不够好。</p><p><em>Laplace</em>平滑就是来帮助解决这个问题的。</p><p>举例而言，在计算： <span class="math display">\[\phi_y=p(y=1)=\frac{\text{numof(1)}}{\text{numof(0)}+\text{numof(1)}}\]</span> 其中，<span class="math inline">\(\text{numof(1)}\)</span>表示的是，被分类为1的训练集中数据个数。</p><p>在<em>Laplace</em>平滑中，我们会采取如下策略: <span class="math display">\[\phi_y=p(y=1)=\frac{\text{numof(1)}+1}{\text{numof(0)}+1+\text{numof(1)}+1}\]</span> 比如，A球队在之前的五场比赛里面都输了，我们预测下一场比赛赢的概率： <span class="math display">\[p(y=1)=\frac{0+1}{0+1+5+1}=\frac{1}{7}\]</span> 而不是简单的认为（没有<em>Laplace</em>平滑）是0。</p><p>推广而言，在多分类问题中，<span class="math inline">\(y\in\lbrace1, \ldots, k \rbrace\)</span>，那么： <span class="math display">\[p(y=j) = \frac{\sum_{i=1}^m1\lbrace y^{(i)} = j \rbrace+1}{m+k}\]</span></p><h2 id="例三垃圾邮件分类2">例三：垃圾邮件分类（2）</h2><p>之前的垃圾分类模型里面，我们对邮件提取的特征向量是： <span class="math display">\[x=[1,0,0,\ldots,1,\ldots]^T\]</span> 这种模型，我们称之为多元伯努利事件模型（Multivariate Bernoulli Event Model）。</p><p>现在，我们换一种特征向量提取方式，将邮件的特征向量表示为： <span class="math display">\[x=[x_1,x_2,\ldots,x_j,\ldots]^T\]</span> <span class="math inline">\(x_j\)</span>表示词汇<span class="math inline">\(j\)</span>在邮件中出现的次数。上述的特征向量也就是词频向量了。这种模型，我们称为多项式事件模型（Multinomial Event Model）。</p><p>对联合概率分布<span class="math inline">\(p(x,y)\)</span>进行极大似然估计，得到如下的参数： <span class="math display">\[\begin{align}\phi_{k|y=1} &amp;= p(x_j=k|y=1) = \frac{C_{x=k}+1}{C_{y=1}+n} \\\\phi_{k|y=0} &amp;=p(x_j=k|y=0) = \frac{C_{x=k}+1}{C_{y=0}+n} \\\\phi_{y} &amp;= p(y=1) = \frac{C_{y=1}+1}{C_{y=1}+1+C_{y=0}+1}\end{align}\]</span> 其中：</p><p><span class="math inline">\(n\)</span>表示词典中词汇的数量，也就是特征向量的长度； <span class="math display">\[C_{x=k}=\sum_{i=1}^m(1\lbrace y^{(i)}=1 \rbrace \sum_{j=1}^{n_i}1 \lbrace x_j^{(i)} = k \rbrace)\]</span> 表示在训练集中，所有垃圾邮件中词汇<span class="math inline">\(k\)</span>出现的次数（并不是邮件的次数，而是词汇的次数）； <span class="math display">\[C_{y=1}=\sum_{i=1}^n(1\lbrace y^{(i)} = 1 \rbrace \cdot n_i)\]</span> 表示训练集中垃圾邮件的所有词汇总长； <span class="math display">\[C_{y=0}=\sum_{i=1}^n(1\lbrace y^{(i)} = 0 \rbrace \cdot n_i)\]</span> 表示训练集中非垃圾邮件的所有词汇总长；</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;例一高斯判别分析和logistic函数&quot;&gt;例一：高斯判别分析和logistic函数&lt;/h2&gt;
&lt;p&gt;我们来看一个例子，对于一个高斯判别分析问题，根据贝叶斯： &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
p(y=1|x) 
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成学习算法" scheme="https://jiacheng-pan.github.io/wiki/tags/%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="拉普拉斯平滑" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91/"/>
    
  </entry>
  
  <entry>
    <title>生成学习算法的概念</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/06/29/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/08-%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/06/29/吴恩达·机器学习/08-生成学习算法的概念/</id>
    <published>2018-06-29T04:40:00.000Z</published>
    <updated>2019-01-21T07:28:45.415Z</updated>
    
    <content type="html"><![CDATA[<p>生成学习算法，英文为<em>Generative Learning Algorithm</em>。</p><p>我们之前看到的都是判别学习算法（<em>Discriminative Learning Algorithm</em>）。判别学习算法可以分成两种：</p><ol type="1"><li>学得<span class="math inline">\(p(y|x)\)</span>，比如之前的线性模型</li><li>学得一个假设<span class="math inline">\(h_\theta (x) = \lbrace 0, 1 \rbrace\)</span>，比如二分类问题</li></ol><p>上面都是根据特征<span class="math inline">\(x\)</span>，来对输出<span class="math inline">\(y\)</span>进行建模，也许<span class="math inline">\(y\)</span>是一个连续的值，也可能是离散的，比如类别。</p><p>那么，生成学习算法（<em>Generative Learning Algorithm</em>）刚好跟判别学习算法（<em>Discriminative Learning Algorithm</em>）相反，其用输出<span class="math inline">\(y\)</span>对<span class="math inline">\(x\)</span>进行建模，也就是：<strong>学得<span class="math inline">\(p(x|y)\)</span></strong>。</p><p><strong>详细解释</strong>：</p><p>对于一个二分类或多分类问题，生成学习算法，在给定样本所述的类别的条件下，会对其样本特征建立一个概率模型。举例而言，在给定癌症是良性或者是恶性的条件下，生成模型会对该癌症的特征的概率分布进行建模。</p><p>根据朴素贝叶斯， <span class="math display">\[p(y=1|x)=\frac{p(x|y=1) p(y=1)}{p(x)}\]</span> 因为给定了<span class="math inline">\(x\)</span>，所以<span class="math inline">\(p(x)\)</span>可以视作1，也即 <span class="math display">\[p(y=1|x)=p(x|y=1) p(y=1)\]</span></p><h2 id="高斯判别算法">高斯判别算法</h2><p><em>Gaussian Discriminant Algorithm</em></p><p>对特征满足高斯分布的二分类问题进行建模。</p><p>假设<span class="math inline">\(x \in \Bbb R^n\)</span>，且<span class="math inline">\(p(x|y)\)</span>满足高斯分布，因为<span class="math inline">\(x\)</span>往往是一个特征向量，故而这里的高斯分布是一个多元高斯分布（<em>multivariate Gaussian）</em>。</p><blockquote><p>多元高斯分布，<span class="math inline">\(z \sim N(\mu, \Sigma)\)</span>。 <span class="math display">\[p(z)=\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}exp(-\frac{1}{2}(z-\mu)^T\Sigma^{-1}(z-\mu))\]</span> <span class="math inline">\(z\)</span>是一个<span class="math inline">\(n\)</span>维向量，<span class="math inline">\(\mu\)</span>则是均值向量，<span class="math inline">\(\Sigma\)</span>是协方差矩阵：<span class="math inline">\(E[(z-\mu)(z-\mu)^T]\)</span>。</p></blockquote><p>对于一个二分类判别问题，正则响应函数为<span class="math inline">\(\phi\)</span>，那么 <span class="math display">\[p(y)=\phi^y(1-\phi)^{1-y}\]</span> 且 <span class="math display">\[p(x|y=0) \sim N(\mu_0, \Sigma) \\\p(x|y=1) \sim N(\mu_1, \Sigma)\]</span> (这里假设了，对于<span class="math inline">\(y=0\)</span>和<span class="math inline">\(y=1\)</span>，是两个不同均值，但协方差矩阵相同的多元高斯分布，所以这也是该模型的限制之一)</p><p>我们写出对数似然函数： <span class="math display">\[\begin{align}l(\phi, \mu_0, \mu_1, \Sigma) &amp;= log\prod_{i=1}^mp(x^{(i)}, (y^{(i)})) \\\&amp;= log\prod_{i=1}^mp(x^{(i)}| (y^{(i)}))p(y^{(i)})\end{align}\]</span></p><p>对比一下之前二分类问题中的对数似然估计函数： <span class="math display">\[l(\theta) = \sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\theta))\]</span> 这里，我们使用了联合概率（<em>joint likelihood</em>）: <span class="math inline">\(p(x^{(i)}, (y^{(i)}))\)</span>，而在之前的判别模型里，我们用的是条件概率（<em>conditional likelihood</em>）: <span class="math inline">\(P(y^{(i)}|x^{(i)};\theta)\)</span>。因为，在判别模型中，特征<span class="math inline">\(x\)</span>是给定的，所以用条件概率来进行极大似然估计；而在生成模型中，特征<span class="math inline">\(x\)</span>不给定，所以需要用联合概率来进行极大似然估计。</p><p>然后，我们进行极大似然估计，得到： <span class="math display">\[\phi=\frac{1}{m}\sum_{i=1}^my^{(i)}=\frac{1}{m}\sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace\]</span> 也就是分类标签为1的训练样本的比例。</p><p>再看<span class="math inline">\(\mu_0, \mu_1\)</span>： <span class="math display">\[\mu_0=\underbrace{\sum_{i=1}^m 1\lbrace y^{(i)} = 0 \rbrace \cdot x^{(i)}}_{(1)} /\underbrace{ \sum_{i=1}^m 1\lbrace y^{(i)} = 0 \rbrace}_{(2)} \\\\mu_1=\sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace \cdot x^{(i)} / \sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace\]</span> 式(1)表达了分类为0的样本中，特征<span class="math inline">\(x\)</span>的和；式(2)表达了分类为0的样本个数；故而<span class="math inline">\(\mu_0\)</span>表达了样本中，分类为0的样本的特征的均值。同理得到<span class="math inline">\(\mu_1\)</span>。</p><p>我们再次回顾上面的高斯判别算法，事实上，高斯判别算法，假设了特征<span class="math inline">\(x\)</span>满足了多元高斯分布，利用极大似然估计，对不同类别的特征<span class="math inline">\(x\)</span>的分布进行了建模。</p><p>下节课我们将会探讨其他的关于生成学习算法的案例。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;生成学习算法，英文为&lt;em&gt;Generative Learning Algorithm&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;我们之前看到的都是判别学习算法（&lt;em&gt;Discriminative Learning Algorithm&lt;/em&gt;）。判别学习算法可以分成两种：&lt;/p&gt;
&lt;o
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成学习算法" scheme="https://jiacheng-pan.github.io/wiki/tags/%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="高斯判别算法" scheme="https://jiacheng-pan.github.io/wiki/tags/%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>广义线性模型</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/06/09/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/07-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/06/09/吴恩达·机器学习/07-广义线性模型/</id>
    <published>2018-06-09T08:29:00.000Z</published>
    <updated>2019-01-20T14:20:52.669Z</updated>
    
    <content type="html"><![CDATA[<p>广义线性模型，英文名为<strong>Generalized Linear Model</strong>，简称GLM。</p><p>之前，涉及到两种的两种模型： 1. 线性拟合模型，假设了<span class="math inline">\(P(y|x;\theta)\)</span>是高斯分布 2. 二分类问题，假设了<span class="math inline">\(P(y|x;\theta)\)</span>满足伯努利分布</p><p>但以上两者知识一种更广泛的，被称为『指数分布族』（The Exponential Family）的特例。</p><h2 id="指数分布族">指数分布族</h2><p><span class="math display">\[P(y;\eta)=b(y)exp(\eta^TT(y)-a(\eta))\]</span></p><p>可以被表示为以上形式的分布，都是指数分布族的某个特定分布，给定<span class="math inline">\(a, b, T\)</span>，就可以定义一个概率分布的集合，以<span class="math inline">\(\eta\)</span>为参数，就可以得到不同的概率分布。</p><p>在广义线性模型中，会假设<span class="math inline">\(\eta=\theta^Tx\)</span>，也就是<span class="math inline">\(\eta\)</span>和特征<span class="math inline">\(x\)</span>线性相关。</p><h2 id="伯努利分布">伯努利分布</h2><p>首先，我们给出<span class="math inline">\(y=1\)</span>的概率： <span class="math display">\[P(y=1;\phi)=\phi\]</span> 于是： <span class="math display">\[\begin{align}P(y;\phi)    &amp;= \phi^y(1-\phi)^T\\\    &amp;= exp(log(\phi^T(1-\phi^T)))\\\    &amp;= exp(ylog(\phi)+(1-y)log(1-\phi))\\\    &amp;= exp(log\frac{\phi}{1-\phi} \cdot y + log(1-\phi))\end{align}\]</span> 比较我们上面的概率形式和指数分布族的标准形式，可以得到： <span class="math display">\[\begin{cases}\eta &amp;= log\frac{\phi}{1-\phi}, \text{于是} \phi=\frac{1}{1+e^{-\eta}}\\\a(\eta) &amp;= -log(1-\phi)=log(1+e^\eta)\\\T(y) &amp;= y\\\b(y) &amp;= 1\end{cases}\]</span></p><p>这里的<span class="math inline">\(\phi\)</span>一般会被称为正则响应函数（<em>canonic response function</em>）： <span class="math display">\[g(\eta) = E[y|\eta]=\frac{1}{1+e^{-\eta}}\]</span> 相对的，正则关联函数（<em>canonic link function</em>）则是<span class="math inline">\(g^{-1}\)</span>。</p><h2 id="高斯分布">高斯分布</h2><p><span class="math display">\[N(\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}(y-\mu)^2)\]</span></p><p>这里，出于简洁考虑，假设<span class="math inline">\(\sigma=1\)</span>，经过一系列化简后，可以表示成： <span class="math display">\[\frac{1}{\sqrt{2\pi}} \cdot exp(-\frac{1}{2}y^2) \cdot exp(\mu y-\frac{1}{2}\mu^2)\]</span> 那么， <span class="math display">\[\begin{cases}\eta &amp;= \mu\\\a(\eta) &amp;= \frac{1}{2}\mu^2=\frac{1}{2}\eta^2\\\T(y) &amp;= y\\\b(y) &amp;= \frac{1}{\sqrt{2\pi}} \cdot exp(-\frac{1}{2}y^2)\end{cases}\]</span></p><h2 id="多项式分布">多项式分布</h2><h4 id="建模">建模</h4><p>在二项分布中，<span class="math inline">\(y\in \lbrace 1, 2 \rbrace\)</span></p><p>而多项式分布，<span class="math inline">\(y \in \lbrace 1,\cdots, k \rbrace\)</span></p><p>一般会被用来进行邮件分类或者进行病情分类等等</p><p>我们假设 <span class="math display">\[P(y=i)=\phi_i\]</span> 也即，邮件属于<span class="math inline">\(i\)</span>类的概率是<span class="math inline">\(\phi_i\)</span>，是关于特征<span class="math inline">\(x\)</span>的一个函数。</p><p>那么，可以用<span class="math inline">\(k\)</span>个参数来建模多项式分布 <span class="math display">\[P(y)=\prod_{i=1}^k\phi_i^{1\lbrace y=i \rbrace}\]</span></p><p>其中，<span class="math inline">\(1 \lbrace \cdots \rbrace\)</span>的含义为，检验<span class="math inline">\(\cdots\)</span>是否为真命题，若为真命题，则取1，否则取0。</p><p>因为所有概率和为1，所以最后一个参数 <span class="math display">\[\begin{align}\phi_k &amp;= 1-\sum_{j=1}^{k-1}\phi_j \\\1 \lbrace y=k \rbrace &amp;=1-\sum_{j=1}^{k-1}1 \lbrace y=j \rbrace\end{align}\]</span> 经过化简，也可以表示成： <span class="math display">\[P(y)=exp[\sum_{i=1}^{k-1}(log(\frac{\phi_i}{\phi_k}) \cdot 1\lbrace y=i \rbrace )] + log(\phi_k)\]</span> 故而 <span class="math display">\[\eta = \begin{bmatrix}log(\frac{\phi_1}{\phi_k}) \\\\vdots \\\log(\frac{\phi_{k-1}}{\phi_k})\end{bmatrix} \in \Bbb R^{k-1}\]</span> <span class="math display">\[a(\eta) = -log(\phi_k)\]</span> <span class="math display">\[T(y)= \begin{bmatrix}1 \lbrace y=1 \rbrace \\\\vdots \\\1 \lbrace y=k-1 \rbrace\end{bmatrix} \in (0, 1)^{k-1}\]</span> <span class="math display">\[b(y) = 1\]</span></p><p>根据<span class="math inline">\(\eta\)</span>可得： <span class="math display">\[\phi_i = e^{\eta_i} \cdot \phi_k\]</span> 又因为： <span class="math display">\[\sum_{i=1}^{k}\phi_i=\sum_{i=1}^k\phi_ke^{\eta_i}=1\]</span> 故而： <span class="math display">\[\phi_k = \frac{1}{\sum_{i=1}^ke^{\eta_i}}=\frac{1}{e^{\eta_k}+\sum_{i=1}^{k-1}e^{\eta_i}} = \frac{1}{1+\sum_{i=1}^{k-1}e^{\eta_i}}\]</span> 所以： <span class="math display">\[\begin{align}\phi_i &amp;= e^{\eta_i} \cdot \phi_k \\\&amp;= \frac{e^{\eta_i}}{1 + \sum_{j=1}^{k-1}e^{\eta_j}} \\\&amp;= \frac{e^{\theta_i^Tx_i}}{1 + \sum_{j=1}^{k-1}e^{\theta_j^Tx_j}}\end{align}\]</span> 上述函数，被称为『softmax』函数，这个函数的作用经常用于进行归一化。</p><p>经过上述步骤，假设函数可以被写成如下形式： <span class="math display">\[h_\theta(x)=\left[\begin{array}{c}1\lbrace y=1 \rbrace  \\\\vdots \\\1\lbrace y=k-1 \rbrace\end{array} | x;\theta\right]=\begin{bmatrix}\phi_1\\\\vdots\\\\phi_{k-1}\end{bmatrix}\]</span></p><h4 id="回归">回归</h4><p>在经过上述推导，当我们有一堆训练集（<span class="math inline">\((x^{(1)}, y^{(1)}), \cdots, (x^{(m)}, y^{(m)})\)</span>）用于训练的时候，则可以进行极大似然估计： <span class="math display">\[L(\theta) = \prod_{i=1}^mP(y^{(i)} | x^{(i)};\theta) = \prod_{i=1}^m\prod_{j=1}^k\phi_j^{1\lbrace y^{(i)}=j \rbrace }\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;广义线性模型，英文名为&lt;strong&gt;Generalized Linear Model&lt;/strong&gt;，简称GLM。&lt;/p&gt;
&lt;p&gt;之前，涉及到两种的两种模型： 1. 线性拟合模型，假设了&lt;span class=&quot;math inline&quot;&gt;\(P(y|x;\theta)\
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="softmax" scheme="https://jiacheng-pan.github.io/wiki/tags/softmax/"/>
    
      <category term="指数分布族" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83%E6%97%8F/"/>
    
  </entry>
  
  <entry>
    <title>牛顿法</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/06/07/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/06-%E7%89%9B%E9%A1%BF%E6%B3%95/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/06/07/吴恩达·机器学习/06-牛顿法/</id>
    <published>2018-06-07T12:40:00.000Z</published>
    <updated>2019-01-20T14:20:52.652Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>牛顿法</strong>（英语：Newton's method）又称为<strong>牛顿-拉弗森方法</strong>（英语：Newton-Raphson method），它是一种在实数域和复数域上近似求解方程的方法。方法使用函数<span class="math inline">\(\displaystyle f(x)\)</span>的<a href="https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0" target="_blank" rel="noopener">泰勒级数</a>的前面几项来寻找方程<span class="math inline">\(\displaystyle f(y)=0\)</span>的根。</p><p>——维基百科</p></blockquote><p>牛顿法可以通过迭代逼近的方法，求得函数<span class="math inline">\(f(x)=0\)</span>的解。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-7/69557176.jpg"></p><ol type="1"><li>先初始化某个点<span class="math inline">\(x_0\)</span>，对该点求导数<span class="math inline">\(f&#39;(x_0)\)</span>，可以得到一条切线；</li><li>切线会和横轴再有一个交点<span class="math inline">\(x_1\)</span>，然后再重复第一步；</li><li>直到<span class="math inline">\(f(x_n)=0\)</span></li></ol><p>通过一系列推导，我们可以得知： <span class="math display">\[x_{i+1}-x_{i}=\frac{f(x^{(i)})}{f&#39;(x^{(i)})}\]</span> 于是，我们可以将牛顿法用于极大似然估计，也就是求<span class="math inline">\(l(\theta)\)</span>的最大值，可以看做是求<span class="math inline">\(l&#39;(\theta)=0\)</span>的解。</p><p>那么，每次迭代就可以写成： <span class="math display">\[\theta^{(t+1)}=\theta^{(t)}-\frac{l&#39;(\theta^{(t)})}{l&#39;&#39;(\theta^{(t)}}\]</span> 更一般地，可以写成： <span class="math display">\[\theta^{(t+1)}=\theta^{(t)}-H^{-1}\nabla_\theta l\]</span> 其中，<span class="math inline">\(H\)</span>是<span class="math inline">\(l(\theta)\)</span>的Hessian矩阵： <span class="math display">\[H_{ij}=\frac{\partial^2l}{\partial\theta_i\partial\theta_j}\]</span> 但这个方法有个缺点，每次迭代的时候，都需要重新计算<span class="math inline">\(H^{-1}\)</span>，虽然牛顿法对函数<span class="math inline">\(f\)</span>有很多要求和限制，但对于logistic函数而言，足够有效。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;牛顿法&lt;/strong&gt;（英语：Newton&#39;s method）又称为&lt;strong&gt;牛顿-拉弗森方法&lt;/strong&gt;（英语：Newton-Raphson method），它是一种在实数域和复数域上近似求解方程的方法。方法使用函数
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="牛顿法" scheme="https://jiacheng-pan.github.io/wiki/tags/%E7%89%9B%E9%A1%BF%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>二分类问题</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/06/05/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/05-%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/06/05/吴恩达·机器学习/05-二分类问题/</id>
    <published>2018-06-05T03:03:00.000Z</published>
    <updated>2019-01-20T14:20:52.653Z</updated>
    
    <content type="html"><![CDATA[<p>在二分类问题中，输出<span class="math inline">\(y\in \{0, 1\}\)</span>。同样的，我们也可以用线性拟合来尝试解决二分类问题（如下图左），但数据点比较异常时，容易出现下图右这样的情况：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-5/41455819.jpg"></p><p>一般，在二分类问题中，我们会选用『<em>logistic</em>函数』来拟合（因为形状像<em>S</em>，又称为『<em>sigmoid</em>函数』）： <span class="math display">\[h_\theta (x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\]</span> <em>logistic</em>函数<span class="math inline">\(g(z)=1/(1+e^{-z})​\)</span>的形状如下： <img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-5/53166407.jpg"> 可以定义 <span class="math display">\[\begin{align}P(y=1|x;\theta)&amp; =h_\theta (x) \\\P(y=0|x;\theta)&amp; =1-h_\theta(x)\end{align}\]</span> 于是： <span class="math display">\[P(y|x;\theta)=h_\theta(x)^y(1-h_\theta(x))^{(1-y)}\]</span> 进行极大似然估计： <span class="math display">\[L(\theta)=P(y|x;\theta)=\prod_{i=1}^mP(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^mh_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{(1-y^{(i)})}\]</span> 为了计算方便，定义 <span class="math display">\[\begin{align}l(\theta)&amp;=log(L(\theta))\\\&amp;=\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\theta))\\\&amp;=\sum_{i=1}^m(y^{(i)}\cdot log(h_\theta(x^{(i)}))+(1-y^{(i)})\cdot log(1-h_\theta(x^{(i)})))\end{align}\]</span> 利用梯度上升进行求解： <span class="math display">\[\theta := \theta + \alpha \nabla_\theta l(\theta)\]</span> 其中 <span class="math display">\[\nabla_{\theta_j} l(\theta)=\frac{\partial}{\partial\theta_j}l(\theta)=\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))\cdot x_j^{(i)}\\\\theta_j:=\theta_j+\alpha \cdot \sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))\cdot x_j^{(i)}\]</span> 最终的梯度上升结果几乎与线性拟合中的梯度下降结果一样。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在二分类问题中，输出&lt;span class=&quot;math inline&quot;&gt;\(y\in \{0, 1\}\)&lt;/span&gt;。同样的，我们也可以用线性拟合来尝试解决二分类问题（如下图左），但数据点比较异常时，容易出现下图右这样的情况：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="二分类问题" scheme="https://jiacheng-pan.github.io/wiki/tags/%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>线性模型的概率解释</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/06/05/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/04-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A6%82%E7%8E%87%E8%A7%A3%E9%87%8A/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/06/05/吴恩达·机器学习/04-线性模型的概率解释/</id>
    <published>2018-06-05T03:02:00.000Z</published>
    <updated>2019-01-20T14:20:01.708Z</updated>
    
    <content type="html"><![CDATA[<p>关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？</p><p>我们更新一下假设函数，使之变为： <span class="math display">\[y^{(i)} = \theta^Tx^{(i)} + \varepsilon^{(i)}\]</span> 其中，<span class="math inline">\(\varepsilon^{(i)}\)</span>是误差项，表示未捕获的特征（unmodeled effects），比如房子存在壁炉也影响价格，或者其他的一些随机噪音（random noise）。</p><p>一般，会假设误差项<span class="math inline">\(\varepsilon^{(i)} \sim N(0, \sigma^2)\)</span>（满足正态分布），也就是： <span class="math display">\[P(\varepsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})\]</span> 关于为什么假设正态分布的解释：</p><ol type="1"><li>便于数学运算；</li><li>很多独立分布的变量之间相互叠加后会趋向于正态分布（中心极限定理），在大多数情况下能成立</li></ol><p>所以，<span class="math inline">\(y^{(i)}\)</span>的后验分布： <span class="math display">\[P(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \sim N(\theta^Tx^{(i)}, \sigma^2)\]</span></p><p>之后，进行极大似然估计（maximum likelihood estimation）：<span class="math inline">\(max L(\theta)\)</span>，即选择合适的<span class="math inline">\(\theta\)</span>，使得<span class="math inline">\(y^{(i)}\)</span>对于<span class="math inline">\(x^{(i)}\)</span>出现的概率最高（有一些存在即合理的感觉），其中<span class="math inline">\(L(\theta)\)</span>的定义如下： <span class="math display">\[L(\theta)=P(y|x;\theta)=\prod_{i=1}^mP(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\]</span> 那么，为了计算方便，我们定义： <span class="math display">\[l(\theta) = log(L(\theta))=\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\theta))=m\cdot log(\frac{1}{\sqrt{2\pi}\sigma})-\sum_{i=1}^m\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\]</span> 于是，极大似然估计变为最小化： <span class="math display">\[\sum_{i=1}^m\frac{(y{(i)}-\theta^Tx{(i)})2}{2\sigma2}\]</span> 也即之前线性回归所需进行最小二乘拟合的<span class="math inline">\(J(\theta)\)</span>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？&lt;/p&gt;
&lt;p&gt;我们更新一下假设函数，使之变为： &lt;span class=&quot;math display&quot;&gt;\[
y^{(i)} = \theta^Tx^{(i)}
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="https://jiacheng-pan.github.io/wiki/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>过拟合&amp;局部加权回归</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/06/04/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E8%BF%87%E6%8B%9F%E5%90%88&amp;%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/06/04/吴恩达·机器学习/03-过拟合&amp;局部加权回归/</id>
    <published>2018-06-04T10:36:00.000Z</published>
    <updated>2019-01-20T14:20:52.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="欠拟合和过拟合">欠拟合和过拟合</h2><p>对于之前房价的例子，假设只有一个特征size。</p><p>假如，我们只用简单的线性拟合（<span class="math inline">\(\theta_0+\theta_1x_1\)</span>，<span class="math inline">\(x_1\)</span>表示size），最终拟合结果会变一条直线，就可能产生下图最左边的结果，我们称之为『欠拟合』。</p><p>当我们尝试用二次曲线来拟合（<span class="math inline">\(\theta_0+\theta_1x_1+\theta_2x_1^2\)</span>，可以假设<span class="math inline">\(x_2=x_1^2\)</span>，再进行线性拟合），就可能产生中间的结果。</p><p>但如果再继续增加曲线的复杂度，对于下图这种五个样本的例子，假如我们用一个五次曲线来拟合它（<span class="math inline">\(\theta_0+\theta_1x1+\theta_2x1^2+\cdots+\theta_5x_1^5\)</span>）就会精确拟合所有数据，产生右图的结果，我们称之为『过拟合』。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-4/71073333.jpg"></p><h2 id="局部加权回归locally-weighted-regression">局部加权回归（Locally Weighted Regression）</h2><p>局部加权回归，是一种特定的非参数学习方法。</p><p>什么叫非参数学习方法，首先，简单了解一下『参数化学习方法』(parametric learning algorithm)，是一种参数固定的学习方法，如上所示。而『非参数化学习方法』（non-parametric learning algorithm）则不固定参数，参数的个数会随着训练集数量而增长。</p><p>我们回顾一下，线性拟合中，我们的目标是找到合适的参数<span class="math inline">\(\theta\)</span>，使得最小化<span class="math inline">\(\sum_i(Y^{(i)} - \theta^TX^{(i)})^2\)</span>。</p><p>而『局部线性拟合』，则是在某个局部区域A进行线性拟合，目标是最小化<span class="math inline">\(\sum_iw^{(i)}(Y^{(i)} - \theta^TX^{(i)})^2\)</span>，其中权重<span class="math display">\[w^{(i)} = exp(-\frac{(x^{(i))}-x)^2}{2})\]</span>，当然，权重公式是可替换的。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-4/30285296.jpg"></p><p>我们观察一下<span class="math inline">\(w^{(i)}\)</span>的形状，当数据<span class="math inline">\(x^{(i)}\)</span>靠近<span class="math inline">\(x\)</span>时，其权重就会较大，那么对目标函数的贡献就会大一些；而数据远离<span class="math inline">\(x\)</span>的时候，权重就会较小，贡献就会较小。这样做，目标函数就会更关注<span class="math inline">\(x\)</span>附近的数据点，从而达到局部的目的。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-4/78196882.jpg"></p><p>当然，可以调整权重函数，常用的另一个权重函数：<span class="math display">\[w^{(i)} = exp(-\frac{(x^{(i))}-x)^2}{2 \tau^2 })\]</span>（波长函数），<span class="math inline">\(\tau\)</span>越大，波形越平缓，局部性越差。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-4/54967961.jpg"></p><p>但问题在于，当训练数据较大时，该方法的代价会很高。每要预测一个值，就需要重新进行一次局部线性拟合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;欠拟合和过拟合&quot;&gt;欠拟合和过拟合&lt;/h2&gt;
&lt;p&gt;对于之前房价的例子，假设只有一个特征size。&lt;/p&gt;
&lt;p&gt;假如，我们只用简单的线性拟合（&lt;span class=&quot;math inline&quot;&gt;\(\theta_0+\theta_1x_1\)&lt;/span&gt;，&lt;sp
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="局部加权回归" scheme="https://jiacheng-pan.github.io/wiki/tags/%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>线性回归</title>
    <link href="https://jiacheng-pan.github.io/wiki/2018/06/03/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>https://jiacheng-pan.github.io/wiki/2018/06/03/吴恩达·机器学习/02-线性回归/</id>
    <published>2018-06-03T12:04:00.000Z</published>
    <updated>2019-01-20T14:20:52.652Z</updated>
    
    <content type="html"><![CDATA[<p>首先引入一些后面会用到的定理：</p><p><strong>定义1</strong>：定义函数<span class="math inline">\(f: \Bbb R^{m \times n} \mapsto \Bbb R\)</span>，<span class="math inline">\(A \in \Bbb R^{m \times n}\)</span>，定义 <span class="math display">\[\nabla_Af(A)=    \begin{bmatrix}    \frac{\partial f}{\partial A_{11}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{1n}}\\\    \vdots &amp; \ddots &amp; \vdots \\\    \frac{\partial f}{\partial A_{m1}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{mn}}    \end{bmatrix}\]</span> <strong>定义2</strong>：矩阵的迹（Trace）：如果<span class="math inline">\(A \in R^{n\times n}\)</span>方阵，那么<span class="math inline">\(A\)</span>的迹，是<span class="math inline">\(A\)</span>对角线元素之和 <span class="math display">\[tr A = \sum_{i=1}^nA_{ii}\]</span> <strong>定理1</strong>：<span class="math inline">\(tr AB = tr BA\)</span></p><p><strong>定理2</strong>：<span class="math inline">\(tr ABC = tr CAB = tr BCA\)</span></p><p><strong>定理3</strong>：<span class="math inline">\(f(A)=tr AB \Rightarrow \nabla_Af(A)=B^T\)</span></p><p><strong>定理4</strong>：<span class="math inline">\(trA = tr A^T\)</span></p><p><strong>定理5</strong>：<span class="math inline">\(a \in R \Rightarrow tr a=a\)</span></p><p><strong>定理6</strong>：<span class="math inline">\(\nabla_AtrABA^TC=CAB+C^TAB^T\)</span></p><h2 id="线性回归">线性回归</h2><h4 id="一些符号的改写">一些符号的改写</h4><p><a href="http://jackieanxis.coding.me/2018/06/03/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/" target="_blank" rel="noopener">上一篇博客</a>提到，梯度下降的每一步，对某个参数<span class="math inline">\(\theta_i\)</span>，执行： <span class="math display">\[\displaystyle \theta_i:=\theta_i - \alpha\frac{\partial}{\partial \theta_i}J(\theta)\]</span> 那么，<span class="math inline">\(h_\theta(x)\)</span>的所有参数<span class="math inline">\(\theta\)</span>可以表示成一列向量： <span class="math display">\[\theta = \left[    \begin{array}{c}    \theta_0\\\    \theta_1\\\    \vdots\\\    \theta_n    \end{array}\right] \in R^{n+1}\]</span></p><p>我们可以定义： <span class="math display">\[\nabla_\theta J = \left[    \begin{array}{c}    \frac{\partial}{\partial \theta_0}J\\\    \frac{\partial}{\partial \theta_1}J\\\    \vdots\\\    \frac{\partial}{\partial \theta_n}J    \end{array}\right] \in R^{n+1}\]</span></p><p>梯度下降过程可以表示成： <span class="math display">\[\theta:=\theta - \alpha\nabla_\theta J\]</span> 其中，<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\nabla_\theta J\)</span>都说是n+1维向量。</p><p>对于训练集中所有的输入<span class="math inline">\({x^{(1)}},x^{(2)},…,x^{(m)}\)</span>，其中 <span class="math display">\[x^{(i)} = \left[    \begin{array}{c}    1\\\    x_1^{(i)}\\\    \vdots\\\    x_n^{(i)}\\\    \end{array}\right] \in R^{n+1}\]</span></p><p><span class="math inline">\(h(x)=h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\)</span>，可以表示成向量： <span class="math display">\[\left[    \begin{array}{c}    h_\theta(x^{(1)})\\\    h_\theta(x^{(2)})\\\    \vdots\\\    h_\theta(x^{(m)})\\\    \end{array}\right] = \left[    \begin{array}{c}    (x^{(1)})^T\theta\\\    (x^{(2)})^T\theta\\\    \vdots\\\    (x^{(m)})^T\theta\\\    \end{array}\right] = \left[    \begin{array}{c}    (x^{(1)})^T\\\    (x^{(2)})^T\\\    \vdots\\\    (x^{(m)})^T    \end{array}\right] \cdot \theta = X \cdot \theta\]</span></p><p>而 <span class="math display">\[Y = \left[    \begin{array}{c}    y^{(1)}\\\    y^{(2)}\\\    \vdots\\\    y^{(m)}    \end{array}\right]\]</span> 于是， <span class="math display">\[J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(h(x^{(i)} - y^{(i)})^2)=\frac{1}{2}(X \cdot \theta - Y)^T(X \cdot \theta - Y)\]</span></p><h4 id="推导过程">推导过程</h4><p>关于梯度下降法，可以直接简化为求梯度为0的位置，即求<span class="math inline">\(\nabla_\theta J(\theta) = \vec{0}\)</span></p><p>首先，简化： <span class="math display">\[\begin{align}\nabla_\theta J(\theta) &amp; = \nabla_\theta\frac{1}{2}(X \cdot \theta - Y)^T(X \cdot \theta - Y)\\\&amp; =\frac{1}{2}\nabla_\theta tr(\theta^TX^TX\theta - \theta^TX^TY - Y^TX\theta + Y^TY)\\\&amp; =\frac{1}{2}[\nabla_\theta tr(\theta\theta^TX^TX) - \nabla_\theta tr(Y^TX\theta) - \nabla tr(Y^TX\theta)]\end{align}\]</span> 其中，第一项： <span class="math display">\[\begin{align}\nabla_\theta tr(\theta\theta^TX^TX) &amp; = \nabla_\theta tr(\theta I \theta^TX^TX) &amp;\text{定理6, set: $\theta =^{set} A, I = B, X^TX=C$}\\\&amp; = X^TX\theta I + X^TX\theta I &amp; \text{$CAB+C^TAB^T$}\\\&amp; = X^TX\theta + X^TX\theta\end{align}\]</span> 第二项和第三项： <span class="math display">\[\nabla_\theta tr(Y^TX\theta) = X^TY\\\(定理3，set:Y^TX = B, \theta = A)\]</span> 所以： <span class="math display">\[\nabla_\theta J(\theta) = X^TX\theta - X^TY = 0\\\\Rightarrow X^TX\theta = X^TY\\\\]</span> 最后解得： <span class="math display">\[\theta = (X^TX)^{(-1)}X^TY\]</span> 当然，以上的解是有限制的，只有当<span class="math inline">\(X^TX\)</span>满秩时，才能够求逆。</p><p>如果非满秩，说明方程数量不够，也就是当需要n个参数时，却不够n个输入样本。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先引入一些后面会用到的定理：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义1&lt;/strong&gt;：定义函数&lt;span class=&quot;math inline&quot;&gt;\(f: \Bbb R^{m \times n} \mapsto \Bbb R\)&lt;/span&gt;，&lt;span class=&quot;m
      
    
    </summary>
    
      <category term="吴恩达·机器学习" scheme="https://jiacheng-pan.github.io/wiki/categories/%E5%90%B4%E6%81%A9%E8%BE%BE%C2%B7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://jiacheng-pan.github.io/wiki/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="https://jiacheng-pan.github.io/wiki/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
